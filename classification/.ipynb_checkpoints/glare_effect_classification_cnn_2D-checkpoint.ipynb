{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncard code:\\n- 7 * 40\\n- 7 * 40\\n--> 14 * 40\\n\\n#Card left in game\\n--> ändern zu: pairs left in game ? um weniger platz zu verschwenden\\n- statt 15 * 40 -> 8 * 40\\n\\n#Never revealed cards\\n- 14 * 40 (eigentlich nur 13 nötig..mal gucken)\\n\\nmaximum number revealing the same card \\n- theoretisch 20 * 40 ? weil man in je zwei zügen maximal einmal die selbe karte nehmen kann -> 40/2 = 20\\n\\n#rounds since game done \\nman braucht mindestens 14 Züge um alle 7 paare umzudrehen\\n(gucken ob im daten round 2 schritte sind oder einer)\\n--> 40 - 14 = 26 man kann maximal  24 haben : 0-24 sind 25 werte\\n- 25 * 40\\n\\n\\n-> 14 + 14 + 13 + 20 + 25 = 86\\n\\nInsgesamt Bildgröße:\\n40 breit und 86  hoch \\n'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 statistival features + card code \n",
    "# so wenig wie möglich platz aber so viel wie nötig \n",
    "'''\n",
    "card code:\n",
    "- 7 * 40\n",
    "- 7 * 40\n",
    "--> 14 * 40\n",
    "\n",
    "#Card left in game\n",
    "--> ändern zu: pairs left in game ? um weniger platz zu verschwenden\n",
    "- statt 15 * 40 -> 8 * 40\n",
    "\n",
    "#Never revealed cards\n",
    "- 14 * 40 (eigentlich nur 13 nötig..mal gucken)\n",
    "\n",
    "maximum number revealing the same card \n",
    "- theoretisch 20 * 40 ? weil man in je zwei zügen maximal einmal die selbe karte nehmen kann -> 40/2 = 20\n",
    "\n",
    "#rounds since game done \n",
    "man braucht mindestens 14 Züge um alle 7 paare umzudrehen\n",
    "(gucken ob im daten round 2 schritte sind oder einer)\n",
    "--> 40 - 14 = 26 man kann maximal  24 haben : 0-24 sind 25 werte\n",
    "- 25 * 40\n",
    "\n",
    "\n",
    "-> 14 + 14 + 13 + 20 + 25 = 86\n",
    "\n",
    "Insgesamt Bildgröße:\n",
    "40 breit und 86  hoch \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import keras\n",
    "from keras.layers import Conv2D, Dense, Dropout, Input, Concatenate, GlobalMaxPooling1D, MaxPooling2D, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_splits(base_path='C:\\\\Users\\\\dylin\\\\Documents\\\\BA_Glare_Effect\\\\classification_data\\\\features\\\\', splits=20):\n",
    "    real_data_splits_train = []\n",
    "    real_data_splits_test = []\n",
    "    simulated_data_splits_train = []\n",
    "    for split in range(1, splits + 1):\n",
    "        # Real data for training\n",
    "        X_realData_train = np.load(base_path + 'real\\\\Split%s\\\\for_simulation\\\\X_realData_train.npy' %str(split))\n",
    "        y_realData_train = np.load(base_path + 'real\\\\Split%s\\\\for_simulation\\\\y_realData_train.npy' %str(split))\n",
    "        real_data_splits_train.append((X_realData_train, y_realData_train))\n",
    "        \n",
    "        # Real data for testing\n",
    "        X_realData_test = np.load(base_path + 'real\\\\Split%s\\\\for_testing\\\\X_realData_test.npy' %str(split))\n",
    "        y_realData_test = np.load(base_path + 'real\\\\Split%s\\\\for_testing\\\\y_realData_test.npy' %str(split))\n",
    "        real_data_splits_test.append((X_realData_test, y_realData_test))\n",
    "    \n",
    "        # Simulated data for training\n",
    "        X_simulatedData_train = np.load(base_path + 'simulated\\\\Split%s\\\\X_simulatedData_train.npy' %str(split))\n",
    "        y_simulatedData_train = np.load(base_path + 'simulated\\\\Split%s\\\\y_simulatedData_train.npy' %str(split))\n",
    "        simulated_data_splits_train.append((X_simulatedData_train, y_simulatedData_train))\n",
    "    return real_data_splits_train, real_data_splits_test, simulated_data_splits_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_splits_train, real_data_splits_test, simulated_data_splits_train = load_data_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "       [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "       [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "       [ 4.1, 14. , 10. ,  1. ,  0. ],\n",
       "       [ 2.2, 14. ,  9. ,  1. ,  0. ],\n",
       "       [ 2.1, 12. ,  9. ,  2. ,  0. ],\n",
       "       [ 5.1, 12. ,  8. ,  2. ,  0. ],\n",
       "       [ 6.1, 12. ,  7. ,  2. ,  0. ],\n",
       "       [ 5.2, 12. ,  6. ,  2. ,  0. ],\n",
       "       [ 5.1, 10. ,  6. ,  2. ,  0. ],\n",
       "       [ 3.2, 10. ,  5. ,  2. ,  0. ],\n",
       "       [ 3.1,  8. ,  5. ,  2. ,  0. ],\n",
       "       [ 7.1,  8. ,  4. ,  2. ,  0. ],\n",
       "       [ 6.1,  8. ,  4. ,  2. ,  0. ],\n",
       "       [ 1.2,  8. ,  3. ,  2. ,  0. ],\n",
       "       [ 1.1,  6. ,  3. ,  2. ,  0. ],\n",
       "       [ 7.2,  6. ,  2. ,  2. ,  0. ],\n",
       "       [ 7.1,  4. ,  2. ,  2. ,  0. ],\n",
       "       [ 6.2,  4. ,  1. ,  2. ,  0. ],\n",
       "       [ 6.1,  2. ,  1. ,  3. ,  0. ],\n",
       "       [ 4.2,  2. ,  0. ,  3. ,  0. ],\n",
       "       [ 4.1,  0. ,  0. ,  3. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. ,  1. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. ,  2. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. ,  3. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. ,  4. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. ,  5. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. ,  6. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. ,  7. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. ,  8. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. ,  9. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. , 10. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. , 11. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. , 12. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. , 13. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. , 14. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. , 15. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. , 16. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. , 17. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. , 18. ]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = real_data_splits_train[0][0][0]\n",
    "game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 5)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image(game):\n",
    "    card_codes = np.zeros((7, 40))\n",
    "    cards_left = np.zeros((8, 40))\n",
    "    never_revealed_cards = np.zeros((14, 40))\n",
    "    max_same_card_reveals = np.zeros((21, 40))\n",
    "    rounds_since_done = np.zeros((25, 40))\n",
    "    \n",
    "    x_position = 0\n",
    "    \n",
    "    for step in game:\n",
    "        card_code = math.floor(step[0])\n",
    "        first_or_second = int(round((step[0] % 1) * 10))\n",
    "        \n",
    "        if card_code != 0:\n",
    "            card_codes[card_code - 1][x_position] = first_or_second\n",
    "            \n",
    "        cards_left[int(step[1] / 2)][x_position] = 1\n",
    "        never_revealed_cards[int(step[2])][x_position] = 1\n",
    "        max_same_card_reveals[int(step[3])][x_position] = 1\n",
    "        rounds_since_done[int(step[4])][x_position] = 1\n",
    "        \n",
    "        x_position += 1\n",
    "    #switched order of statistival features so that they have some space between them.\n",
    "    return np.vstack((card_codes, max_same_card_reveals, rounds_since_done, cards_left, never_revealed_cards))#[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 40)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = create_image(game)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x168ffeb8ac8>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Showing image reverted. More natural for humans, because higher values are higher. \n",
    "plt.imshow(image, origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_participants_per_split = 19 # 20 but one is removed in each split for testing\n",
    "simulations_per_participant = 1000\n",
    "n_added_simulations_per_participant = 20\n",
    "n_runs = 20\n",
    "n_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_simulated_data(X_train, y_train, simulated_train_set):\n",
    "    for n in range(n_added_simulations_per_participant):\n",
    "        for i in range(n_participants_per_split):\n",
    "\n",
    "            X_train_simulated_1 = simulated_train_set[0][(i * simulations_per_participant) + n]\n",
    "            y_train_simulated_1 = simulated_train_set[1][(i * simulations_per_participant) + n]\n",
    "            X_train_simulated_2 = simulated_train_set[0][(simulations_per_participant * n_participants_per_split) \\\n",
    "                                                         + (i * simulations_per_participant) + n]\n",
    "            y_train_simulated_2 = simulated_train_set[1][(simulations_per_participant * n_participants_per_split) \\\n",
    "                                                         + (i * simulations_per_participant) + n]\n",
    "            \n",
    "            X_train_simulated = np.concatenate((X_train_simulated_1[np.newaxis, :, :], \\\n",
    "                                               X_train_simulated_2[np.newaxis, :, :]), axis=0)\n",
    "            y_train_simulated = np.concatenate((y_train_simulated_1[np.newaxis, :], \\\n",
    "                                               y_train_simulated_2[np.newaxis, :]), axis=0)\n",
    "\n",
    "            X_train = np.concatenate((X_train, X_train_simulated), axis=0)\n",
    "            y_train = np.concatenate((y_train, y_train_simulated), axis=0)\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_score_of_run(histories, epochs):\n",
    "    mean_val_losses = []\n",
    "    mean_val_accuracies = []\n",
    "    mean_losses = []\n",
    "    mean_accuracies = []\n",
    "    for i in range(epochs):\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        for l in range(len(histories)):\n",
    "            history = histories[l]\n",
    "            val_losses.append(history.history['val_loss'][i])\n",
    "            val_accuracies.append(history.history['val_accuracy'][i])\n",
    "            losses.append(history.history['loss'][i])\n",
    "            accuracies.append(history.history['accuracy'][i])\n",
    "        mean_val_losses.append(np.mean(val_losses))\n",
    "        mean_val_accuracies.append(np.mean(val_accuracies))\n",
    "        mean_losses.append(np.mean(losses))\n",
    "        mean_accuracies.append(np.mean(accuracies))\n",
    "    return mean_val_losses, mean_val_accuracies, mean_losses, mean_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_score_over_all_runs(mean_run_scores, n_runs):\n",
    "    val_losses = np.asarray(mean_run_scores[0][0])\n",
    "    val_accuracies = np.asarray(mean_run_scores[0][1])\n",
    "    losses = np.asarray(mean_run_scores[0][2])\n",
    "    accuracies = np.asarray(mean_run_scores[0][3])\n",
    "                            \n",
    "    for i in range(1, n_runs):\n",
    "        val_losses += np.asarray(mean_run_scores[i][0])\n",
    "        val_accuracies += np.asarray(mean_run_scores[i][1])\n",
    "        losses += np.asarray(mean_run_scores[i][2])\n",
    "        accuracies += np.asarray(mean_run_scores[i][3])\n",
    "                                 \n",
    "    val_losses /= n_runs\n",
    "    val_accuracies /= n_runs\n",
    "    losses /= n_runs\n",
    "    accuracies /= n_runs\n",
    "    \n",
    "    return val_losses, val_accuracies, losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train(X_train, y_train, X_test, y_test, n_epochs=n_epochs):\n",
    "    input_shape = \n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (5, 5), input_shape=input_shape, activation='relu'))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(10, (2, 2), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose, \n",
    "                        shuffle=True, validation_data=(X_test, y_test))\n",
    "    histories.append(history)\n",
    "    \n",
    "    '''\n",
    "    verbose, epochs, batch_size = 0, n_epochs, 32 \n",
    "    n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=32, kernel_size=5, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    #print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose, \n",
    "                        shuffle=True, validation_data=(X_test, y_test))\n",
    "    histories.append(history)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_run_scores = []\n",
    "for i in trange(n_runs, desc='Runs'): \n",
    "    histories = []\n",
    "    for train_set, test_set, simulated_train_set in tqdm(zip(real_data_splits_train, \\\n",
    "                                    real_data_splits_test, simulated_data_splits_train), total=20, desc='Folds'):\n",
    "        X_train = train_set[0]\n",
    "        y_train = train_set[1]\n",
    "        X_test = test_set[0]\n",
    "        y_test = test_set[1]\n",
    "        \n",
    "        # Adding simulated data. \n",
    "        X_train, y_train = add_simulated_data(X_train, y_train, simulated_train_set)\n",
    "        \n",
    "        #print(X_train.shape)\n",
    "        #print(y_train.shape)\n",
    "\n",
    "        # Shuffling training data\n",
    "        temp_train = list(zip(X_train.tolist(), y_train.tolist()))\n",
    "        random.shuffle(temp_train)\n",
    "        X_train, y_train = zip(*temp_train)\n",
    "        \n",
    "        create_and_train(np.asarray(X_train), np.asarray(y_train), X_test, y_test)\n",
    "        \n",
    "    mean_run_score = mean_score_of_run(histories=histories, epochs=n_epochs)\n",
    "    mean_run_scores.append(mean_run_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_val_losses, mean_val_accuracies, mean_losses, mean_accuracies = mean_score_over_all_runs(mean_run_scores, n_runs)\n",
    "mean_val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('glareObs_noAdapt: Best validation accuracy %s%% (at epoch %s)' \\\n",
    "          %(round(mean_val_accuracies.max() * 100, 2), np.argmax(mean_val_accuracies) + 1))\n",
    "plt.plot(mean_accuracies)\n",
    "plt.plot(mean_val_accuracies)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train(RD_SD%sx)' %n_added_simulations_per_participant, 'validation(RD)'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('glareObs_noAdapt: Lowest validation loss %s (at epoch %s)' \\\n",
    "          %(round(mean_val_losses.min(), 4), np.argmin(mean_val_losses) + 1))\n",
    "plt.plot(mean_losses)\n",
    "plt.plot(mean_val_losses)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train(RD_SD%sx)' %n_added_simulations_per_participant, 'validation(RD)'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
