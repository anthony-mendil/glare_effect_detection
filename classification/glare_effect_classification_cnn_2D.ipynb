{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from keras.layers import Conv2D, Dense, Dropout, Input, Concatenate, GlobalMaxPooling1D, MaxPooling2D, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "#K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_splits(base_path='C:\\\\Users\\\\dylin\\\\Documents\\\\BA_Glare_Effect\\\\classification_data_initial\\\\features\\\\', splits=20):\n",
    "    real_data_splits_train = []\n",
    "    real_data_splits_test = []\n",
    "    simulated_data_splits_train = []\n",
    "    for split in range(1, splits + 1):\n",
    "        # Real data for training\n",
    "        X_realData_train = np.load(base_path + 'real\\\\Split%s\\\\for_simulation\\\\X_realData_train.npy' %str(split))\n",
    "        y_realData_train = np.load(base_path + 'real\\\\Split%s\\\\for_simulation\\\\y_realData_train.npy' %str(split))\n",
    "        real_data_splits_train.append((X_realData_train, y_realData_train))\n",
    "        \n",
    "        # Real data for testing\n",
    "        X_realData_test = np.load(base_path + 'real\\\\Split%s\\\\for_testing\\\\X_realData_test.npy' %str(split))\n",
    "        y_realData_test = np.load(base_path + 'real\\\\Split%s\\\\for_testing\\\\y_realData_test.npy' %str(split))\n",
    "        real_data_splits_test.append((X_realData_test, y_realData_test))\n",
    "    \n",
    "        # Simulated data for training\n",
    "        X_simulatedData_train = np.load(base_path + 'simulated\\\\Split%s\\\\X_simulatedData_train.npy' %str(split))\n",
    "        y_simulatedData_train = np.load(base_path + 'simulated\\\\Split%s\\\\y_simulatedData_train.npy' %str(split))\n",
    "        simulated_data_splits_train.append((X_simulatedData_train, y_simulatedData_train))\n",
    "    return real_data_splits_train, real_data_splits_test, simulated_data_splits_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_splits_train, real_data_splits_test, simulated_data_splits_train = load_data_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "       [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "       [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "       [ 4.1, 14. , 10. ,  1. ,  0. ],\n",
       "       [ 2.2, 14. ,  9. ,  1. ,  0. ],\n",
       "       [ 2.1, 12. ,  9. ,  2. ,  0. ],\n",
       "       [ 5.1, 12. ,  8. ,  2. ,  0. ],\n",
       "       [ 6.1, 12. ,  7. ,  2. ,  0. ],\n",
       "       [ 5.2, 12. ,  6. ,  2. ,  0. ],\n",
       "       [ 5.1, 10. ,  6. ,  2. ,  0. ],\n",
       "       [ 3.2, 10. ,  5. ,  2. ,  0. ],\n",
       "       [ 3.1,  8. ,  5. ,  2. ,  0. ],\n",
       "       [ 7.1,  8. ,  4. ,  2. ,  0. ],\n",
       "       [ 6.1,  8. ,  4. ,  2. ,  0. ],\n",
       "       [ 1.2,  8. ,  3. ,  2. ,  0. ],\n",
       "       [ 1.1,  6. ,  3. ,  2. ,  0. ],\n",
       "       [ 7.2,  6. ,  2. ,  2. ,  0. ],\n",
       "       [ 7.1,  4. ,  2. ,  2. ,  0. ],\n",
       "       [ 6.2,  4. ,  1. ,  2. ,  0. ],\n",
       "       [ 6.1,  2. ,  1. ,  3. ,  0. ],\n",
       "       [ 4.2,  2. ,  0. ,  3. ,  0. ],\n",
       "       [ 4.1,  0. ,  0. ,  3. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. ,  1. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. ,  2. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. ,  3. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. ,  4. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. ,  5. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. ,  6. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. ,  7. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. ,  8. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. ,  9. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. , 10. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. , 11. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. , 12. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. , 13. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. , 14. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. , 15. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. , 16. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. , 17. ],\n",
       "       [ 0. ,  0. ,  0. ,  3. , 18. ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = real_data_splits_train[0][0][0]\n",
    "game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image(game, components=[True, True, True, True, True]):\n",
    "    card_codes = np.zeros((7, 40))\n",
    "    cards_left = np.zeros((8, 40))\n",
    "    never_revealed_cards = np.zeros((14, 40))\n",
    "    max_same_card_reveals = np.zeros((21, 40))\n",
    "    rounds_since_done = np.zeros((25, 40))\n",
    "    \n",
    "    x_position = 0\n",
    "    \n",
    "    for step in game:\n",
    "        card_code = math.floor(step[0])\n",
    "        first_or_second = int(round((step[0] % 1) * 10))\n",
    "        \n",
    "        if card_code != 0:\n",
    "            card_codes[card_code - 1][x_position] = first_or_second\n",
    "            \n",
    "        cards_left[int(step[1] / 2)][x_position] = 1\n",
    "        never_revealed_cards[int(step[2])][x_position] = 1\n",
    "        max_same_card_reveals[int(step[3])][x_position] = 1\n",
    "        rounds_since_done[int(step[4])][x_position] = 1\n",
    "        \n",
    "        x_position += 1\n",
    "        \n",
    "    # Try leaving out some features and compare results!\n",
    "    image = np.zeros((0, 40))\n",
    "    if components[0]:   # Good visual feature for cnn.\n",
    "        image = np.vstack((image, card_codes))\n",
    "    if components[1]:\n",
    "        image = np.vstack((image, max_same_card_reveals))\n",
    "    if components[2]:   # I think good visual feature for cnn. \n",
    "        image = np.vstack((image, rounds_since_done))\n",
    "    if components[3]:\n",
    "        image = np.vstack((image, cards_left))\n",
    "    if components[4]:   # I think this feature is not very usefull for the cnn. \n",
    "                        # No big visual difference between being blinded an not. \n",
    "        image = np.vstack((image, never_revealed_cards))\n",
    "    #switched order of statistival features so that they have some space between them.\n",
    "        \n",
    "    return image#[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = create_image(game, components=[True, True, True, True, True])\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 40)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1867f3f2d88>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Showing image reverted. More natural for humans, because higher values are higher. \n",
    "plt.imshow(image)#, origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_images_for_split(data, components):\n",
    "    data_images = []\n",
    "    for split in trange(len(data)):\n",
    "        X = data[split][0]\n",
    "        y = data[split][1]\n",
    "        images = []\n",
    "        for game in range(len(X)):\n",
    "            image = create_image(X[game], components)\n",
    "            images.append(image)\n",
    "        split_data = ((images, y))\n",
    "        data_images.append(split_data)\n",
    "    return data_images\n",
    "    \n",
    "def create_images(components=[True, True, True, True, True]):\n",
    "    real_data_splits_train_images = create_images_for_split(real_data_splits_train, components)\n",
    "    real_data_splits_test_images = create_images_for_split(real_data_splits_test, components)\n",
    "    simulated_data_splits_train_images = create_images_for_split(simulated_data_splits_train, components)\n",
    "    return real_data_splits_train_images, real_data_splits_test_images, simulated_data_splits_train_images \n",
    "    #return real_data_splits_train_images[0][0][24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'real_data_splits_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1866371469bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mreal_data_splits_train_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_data_splits_test_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimulated_data_splits_train_images\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mcreate_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-8f2fb504ae8c>\u001b[0m in \u001b[0;36mcreate_images\u001b[1;34m(components)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mreal_data_splits_train_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_images_for_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_data_splits_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mreal_data_splits_test_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_images_for_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_data_splits_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0msimulated_data_splits_train_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_images_for_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimulated_data_splits_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'real_data_splits_train' is not defined"
     ]
    }
   ],
   "source": [
    "real_data_splits_train_images, real_data_splits_test_images, simulated_data_splits_train_images = \\\n",
    "    create_images(components=[True, True, True, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unclear noObst indices: 8?, 12\n",
    "# Unclear glare indices: 23, 26?, 27, 32, 36, \n",
    "# Not cortrect validated: glare in split 1, \n",
    "# Plan: remove and train best conigs and see if results get better \n",
    "\n",
    "plt.imshow(real_data_splits_train_images[0][0][19], origin='lower') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_splits_train[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_splits_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_participants_per_split = 19 # 20 but one is removed in each split for testing\n",
    "simulations_per_participant = 1000\n",
    "n_added_simulations_per_participant = 20\n",
    "n_runs = 20\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_simulated_data(X_train, y_train, simulated_train_set):\n",
    "    for n in range(n_added_simulations_per_participant):\n",
    "        for i in range(n_participants_per_split):\n",
    "\n",
    "            X_train_simulated_1 = simulated_train_set[0][(i * simulations_per_participant) + n]\n",
    "            y_train_simulated_1 = simulated_train_set[1][(i * simulations_per_participant) + n]\n",
    "            X_train_simulated_2 = simulated_train_set[0][(simulations_per_participant * n_participants_per_split) \\\n",
    "                                                         + (i * simulations_per_participant) + n]\n",
    "            y_train_simulated_2 = simulated_train_set[1][(simulations_per_participant * n_participants_per_split) \\\n",
    "                                                         + (i * simulations_per_participant) + n]\n",
    "            \n",
    "            X_train_simulated = np.concatenate((X_train_simulated_1[np.newaxis, :, :], \\\n",
    "                                               X_train_simulated_2[np.newaxis, :, :]), axis=0)\n",
    "            y_train_simulated = np.concatenate((y_train_simulated_1[np.newaxis, :], \\\n",
    "                                               y_train_simulated_2[np.newaxis, :]), axis=0)\n",
    "\n",
    "            X_train = np.concatenate((X_train, X_train_simulated), axis=0)\n",
    "            y_train = np.concatenate((y_train, y_train_simulated), axis=0)\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_score_of_run(histories, epochs):\n",
    "    mean_val_losses = []\n",
    "    mean_val_accuracies = []\n",
    "    mean_losses = []\n",
    "    mean_accuracies = []\n",
    "    for i in range(epochs):\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        for l in range(len(histories)):\n",
    "            history = histories[l]\n",
    "            val_losses.append(history.history['val_loss'][i])\n",
    "            val_accuracies.append(history.history['val_accuracy'][i])\n",
    "            losses.append(history.history['loss'][i])\n",
    "            accuracies.append(history.history['accuracy'][i])\n",
    "        mean_val_losses.append(np.mean(val_losses))\n",
    "        mean_val_accuracies.append(np.mean(val_accuracies))\n",
    "        mean_losses.append(np.mean(losses))\n",
    "        mean_accuracies.append(np.mean(accuracies))\n",
    "    return mean_val_losses, mean_val_accuracies, mean_losses, mean_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_score_over_all_runs(mean_run_scores, n_runs):\n",
    "    val_losses = np.asarray(mean_run_scores[0][0])\n",
    "    val_accuracies = np.asarray(mean_run_scores[0][1])\n",
    "    losses = np.asarray(mean_run_scores[0][2])\n",
    "    accuracies = np.asarray(mean_run_scores[0][3])\n",
    "                            \n",
    "    for i in range(1, n_runs):\n",
    "        val_losses += np.asarray(mean_run_scores[i][0])\n",
    "        val_accuracies += np.asarray(mean_run_scores[i][1])\n",
    "        losses += np.asarray(mean_run_scores[i][2])\n",
    "        accuracies += np.asarray(mean_run_scores[i][3])\n",
    "                                 \n",
    "    val_losses /= n_runs\n",
    "    val_accuracies /= n_runs\n",
    "    losses /= n_runs\n",
    "    accuracies /= n_runs\n",
    "    \n",
    "    return val_losses, val_accuracies, losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train(X_train, y_train, X_test, y_test):\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "    cnn_input_shape = X_train[0].shape\n",
    "    #epochs = n_epochs\n",
    "    cnn_batch_size = 32 #1000\n",
    "    #verbose = 0\n",
    "    \n",
    "    #print(cnn_input_shape)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (5, 5), input_shape=cnn_input_shape, activation='relu'))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    #model.add(Conv2D(10, (2, 2), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train, y_train, epochs=n_epochs, batch_size=cnn_batch_size, verbose=0, \n",
    "                        shuffle=True, validation_data=(X_test, y_test))\n",
    "    \n",
    "    #print(history.history['val_accuracy'])\n",
    "    \n",
    "    histories.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_run_scores = []\n",
    "for i in trange(n_runs, desc='Runs'): \n",
    "    histories = []\n",
    "    for train_set, test_set, simulated_train_set in tqdm(zip(real_data_splits_train_images, \\\n",
    "                                    real_data_splits_test_images, simulated_data_splits_train_images), total=20, desc='Folds'):\n",
    "        \n",
    "        X_train = train_set[0]\n",
    "        y_train = train_set[1]\n",
    "        X_test = test_set[0]\n",
    "        y_test = test_set[1]\n",
    "        \n",
    "        #print(X_test[0])\n",
    "        \n",
    "        #plt.imshow(X_test[1], origin='lower') \n",
    "        \n",
    "        # Adding simulated data. \n",
    "        X_train, y_train = add_simulated_data(X_train, y_train, simulated_train_set)   \n",
    "        \n",
    "        print(X_train.shape)\n",
    "        print(y_train.shape) \n",
    "        exit()\n",
    "\n",
    "        # Shuffling training data\n",
    "        temp_train = list(zip(X_train, y_train.tolist()))\n",
    "        random.shuffle(temp_train)\n",
    "        X_train, y_train = zip(*temp_train)\n",
    "        \n",
    "        create_and_train(np.asarray(X_train), np.asarray(y_train), np.asarray(X_test), y_test)\n",
    "        \n",
    "    mean_run_score = mean_score_of_run(histories=histories, epochs=n_epochs)\n",
    "    mean_run_scores.append(mean_run_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_val_losses, mean_val_accuracies, mean_losses, mean_accuracies = mean_score_over_all_runs(mean_run_scores, n_runs)\n",
    "mean_val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('glareObs_noAdapt: Best validation accuracy %s%% (at epoch %s)' \\\n",
    "          %(round(mean_val_accuracies.max() * 100, 2), np.argmax(mean_val_accuracies) + 1))\n",
    "plt.plot(mean_accuracies)\n",
    "plt.plot(mean_val_accuracies)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train(RD_SD%sx)' %n_added_simulations_per_participant, 'validation(RD)'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('glareObs_noAdapt: Lowest validation loss %s (at epoch %s)' \\\n",
    "          %(round(mean_val_losses.min(), 4), np.argmin(mean_val_losses) + 1))\n",
    "plt.plot(mean_losses)\n",
    "plt.plot(mean_val_losses)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train(RD_SD%sx)' %n_added_simulations_per_participant, 'validation(RD)'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
