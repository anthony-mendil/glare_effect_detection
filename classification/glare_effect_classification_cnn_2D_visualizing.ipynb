{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from keras.layers import Conv2D, Dense, Dropout, Input, Concatenate, GlobalMaxPooling1D, MaxPooling2D, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "#K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_data_splits(base_path='C:\\\\Users\\\\dylin\\\\Documents\\\\BA_Glare_Effect\\\\classification_data_initial\\\\features\\\\', splits=20):\n",
    "    real_data_splits_train = []\n",
    "    real_data_splits_test = []\n",
    "    simulated_data_splits_train = []\n",
    "    for split in range(1, splits + 1):\n",
    "        # Real data for training\n",
    "        X_realData_train = np.load(base_path + 'real\\\\Split%s\\\\for_simulation\\\\X_realData_train.npy' %str(split))\n",
    "        y_realData_train = np.load(base_path + 'real\\\\Split%s\\\\for_simulation\\\\y_realData_train.npy' %str(split))\n",
    "        real_data_splits_train.append((X_realData_train, y_realData_train))\n",
    "        \n",
    "        # Real data for testing\n",
    "        X_realData_test = np.load(base_path + 'real\\\\Split%s\\\\for_testing\\\\X_realData_test.npy' %str(split))\n",
    "        y_realData_test = np.load(base_path + 'real\\\\Split%s\\\\for_testing\\\\y_realData_test.npy' %str(split))\n",
    "        real_data_splits_test.append((X_realData_test, y_realData_test))\n",
    "    \n",
    "        # Simulated data for training\n",
    "        X_simulatedData_train = np.load(base_path + 'simulated\\\\Split%s\\\\X_simulatedData_train.npy' %str(split))\n",
    "        y_simulatedData_train = np.load(base_path + 'simulated\\\\Split%s\\\\y_simulatedData_train.npy' %str(split))\n",
    "        simulated_data_splits_train.append((X_simulatedData_train, y_simulatedData_train))\n",
    "    return real_data_splits_train, real_data_splits_test, simulated_data_splits_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "real_data_splits_train, real_data_splits_test, simulated_data_splits_train = load_data_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "real_data_split_train, real_data_split_test, simulated_data_split_train = \\\n",
    "    real_data_splits_train[0], real_data_splits_test[0], simulated_data_splits_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_data_split_train[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_image(game, components=[True, True, True, True, True]):\n",
    "    card_codes = np.zeros((7, 40))\n",
    "    cards_left = np.zeros((8, 40))\n",
    "    never_revealed_cards = np.zeros((14, 40))\n",
    "    max_same_card_reveals = np.zeros((20, 40))\n",
    "    rounds_since_done = np.zeros((27, 40))\n",
    "    \n",
    "    x_position = 0\n",
    "    \n",
    "    for step in game:\n",
    "        card_code = math.floor(step[0])\n",
    "        first_or_second = int(round((step[0] % 1) * 10))\n",
    "        \n",
    "        if card_code != 0:\n",
    "            card_codes[card_code - 1][x_position] = first_or_second\n",
    "            \n",
    "        cards_left[int(step[1] / 2)][x_position] = 1\n",
    "        never_revealed_cards[int(step[2])][x_position] = 1\n",
    "        max_same_card_reveals[int(step[3])][x_position] = 1\n",
    "        rounds_since_done[int(step[4])][x_position] = 1\n",
    "        \n",
    "        x_position += 1\n",
    "        \n",
    "    # Try leaving out some features and compare results!\n",
    "    image = np.zeros((0, 40))\n",
    "    if components[0]:   # Good visual feature for cnn.\n",
    "        image = np.vstack((image, card_codes))\n",
    "    if components[1]:\n",
    "        image = np.vstack((image, max_same_card_reveals))\n",
    "    if components[2]:   # I think good visual feature for cnn. \n",
    "        image = np.vstack((image, rounds_since_done))\n",
    "    if components[3]:\n",
    "        image = np.vstack((image, cards_left))\n",
    "    if components[4]:   # I think this feature is not very usefull for the cnn. \n",
    "                        # No big visual difference between being blinded an not. \n",
    "        image = np.vstack((image, never_revealed_cards))\n",
    "    #switched order of statistival features so that they have some space between them.\n",
    "        \n",
    "    return image#[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_images_for_split(data, components):\n",
    "    data_images = []\n",
    "    for split in trange(len(data)):\n",
    "        X = data[split][0]\n",
    "        y = data[split][1]\n",
    "        images = []\n",
    "        for game in range(len(X)):\n",
    "            image = create_image(X[game], components)\n",
    "            images.append(image)\n",
    "        split_data = ((images, y))\n",
    "        data_images.append(split_data)\n",
    "    return data_images\n",
    "    \n",
    "def create_images(components=[True, True, True, True, True]):\n",
    "    real_data_splits_train_images = create_images_for_split(real_data_splits_train, components)\n",
    "    real_data_splits_test_images = create_images_for_split(real_data_splits_test, components)\n",
    "    simulated_data_splits_train_images = create_images_for_split(simulated_data_splits_train, components)\n",
    "    return real_data_splits_train_images, real_data_splits_test_images, simulated_data_splits_train_images \n",
    "    #return real_data_splits_train_images[0][0][24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949ddaf95d514ea981c3735f3dc8594e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97071007793e4a9b8ff91acbd20eb1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74dfe5146a9743bab3bbb334071f1313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "real_data_splits_train_images, real_data_splits_test_images, simulated_data_splits_train_images = \\\n",
    "    create_images(components=[True, True, True, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c32fe84848>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Unclear noObst indices: 8?, 12\n",
    "# Unclear glare indices: 23, 26?, 27, 32, 36, \n",
    "# Not cortrect validated: glare in split 1, \n",
    "# Plan: remove and train best conigs and see if results get better \n",
    "\n",
    "plt.imshow(real_data_splits_train_images[0][0][19], origin='lower') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_data_splits_train[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 1.2, 12. , 12. ,  1. ,  0. ],\n",
       "          [ 2.1, 12. , 11. ,  1. ,  0. ],\n",
       "          [ 3.1, 12. , 10. ,  1. ,  0. ],\n",
       "          [ 4.1, 12. ,  9. ,  1. ,  0. ],\n",
       "          [ 5.1, 12. ,  8. ,  1. ,  0. ],\n",
       "          [ 5.2, 12. ,  7. ,  1. ,  0. ],\n",
       "          [ 5.1, 10. ,  7. ,  2. ,  0. ],\n",
       "          [ 6.1, 10. ,  6. ,  2. ,  0. ],\n",
       "          [ 3.2, 10. ,  5. ,  2. ,  0. ],\n",
       "          [ 3.1, 10. ,  5. ,  2. ,  0. ],\n",
       "          [ 3.2,  8. ,  5. ,  2. ,  0. ],\n",
       "          [ 6.2,  8. ,  4. ,  2. ,  0. ],\n",
       "          [ 4.1,  8. ,  4. ,  2. ,  0. ],\n",
       "          [ 2.2,  8. ,  3. ,  2. ,  0. ],\n",
       "          [ 2.1,  6. ,  3. ,  2. ,  0. ],\n",
       "          [ 7.1,  6. ,  2. ,  2. ,  0. ],\n",
       "          [ 7.2,  4. ,  1. ,  2. ,  0. ],\n",
       "          [ 4.2,  4. ,  0. ,  2. ,  0. ],\n",
       "          [ 6.1,  4. ,  0. ,  2. ,  0. ],\n",
       "          [ 4.1,  4. ,  0. ,  3. ,  0. ],\n",
       "          [ 4.2,  2. ,  0. ,  3. ,  0. ],\n",
       "          [ 6.2,  2. ,  0. ,  3. ,  0. ],\n",
       "          [ 6.1,  0. ,  0. ,  3. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  4. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  5. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  6. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  7. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  8. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  9. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 10. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 11. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 12. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 13. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 14. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 15. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 16. ]],\n",
       "  \n",
       "         [[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 3.2, 12. , 10. ,  1. ,  0. ],\n",
       "          [ 4.1, 12. ,  9. ,  1. ,  0. ],\n",
       "          [ 5.1, 12. ,  8. ,  1. ,  0. ],\n",
       "          [ 4.2, 12. ,  7. ,  1. ,  0. ],\n",
       "          [ 4.1, 10. ,  7. ,  2. ,  0. ],\n",
       "          [ 6.1, 10. ,  6. ,  2. ,  0. ],\n",
       "          [ 6.2,  8. ,  5. ,  2. ,  0. ],\n",
       "          [ 5.2,  8. ,  4. ,  2. ,  0. ],\n",
       "          [ 2.1,  8. ,  4. ,  2. ,  0. ],\n",
       "          [ 2.2,  8. ,  3. ,  2. ,  0. ],\n",
       "          [ 5.2,  8. ,  3. ,  2. ,  0. ],\n",
       "          [ 1.1,  8. ,  3. ,  2. ,  0. ],\n",
       "          [ 1.2,  6. ,  2. ,  2. ,  0. ],\n",
       "          [ 7.1,  6. ,  1. ,  2. ,  0. ],\n",
       "          [ 2.2,  6. ,  1. ,  2. ,  0. ],\n",
       "          [ 5.2,  6. ,  1. ,  3. ,  0. ],\n",
       "          [ 7.1,  6. ,  1. ,  3. ,  0. ],\n",
       "          [ 5.1,  6. ,  1. ,  3. ,  0. ],\n",
       "          [ 5.2,  4. ,  1. ,  4. ,  0. ],\n",
       "          [ 7.2,  4. ,  0. ,  4. ,  0. ],\n",
       "          [ 2.1,  4. ,  0. ,  4. ,  0. ],\n",
       "          [ 2.2,  4. ,  0. ,  4. ,  0. ],\n",
       "          [ 2.1,  2. ,  0. ,  4. ,  0. ],\n",
       "          [ 7.1,  2. ,  0. ,  4. ,  0. ],\n",
       "          [ 7.2,  0. ,  0. ,  4. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  4. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  5. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  6. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  7. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  8. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  9. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. , 10. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. , 11. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. , 12. ]]]),\n",
       "  array([[1., 0.],\n",
       "         [0., 1.]], dtype=float32)),\n",
       " (array([[[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  1. ,  0. ],\n",
       "          [ 2.2, 14. ,  9. ,  1. ,  0. ],\n",
       "          [ 2.1, 12. ,  9. ,  2. ,  0. ],\n",
       "          [ 5.1, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 6.1, 12. ,  7. ,  2. ,  0. ],\n",
       "          [ 5.2, 12. ,  6. ,  2. ,  0. ],\n",
       "          [ 5.1, 10. ,  6. ,  2. ,  0. ],\n",
       "          [ 3.2, 10. ,  5. ,  2. ,  0. ],\n",
       "          [ 3.1,  8. ,  5. ,  2. ,  0. ],\n",
       "          [ 7.1,  8. ,  4. ,  2. ,  0. ],\n",
       "          [ 6.1,  8. ,  4. ,  2. ,  0. ],\n",
       "          [ 1.2,  8. ,  3. ,  2. ,  0. ],\n",
       "          [ 1.1,  6. ,  3. ,  2. ,  0. ],\n",
       "          [ 7.2,  6. ,  2. ,  2. ,  0. ],\n",
       "          [ 7.1,  4. ,  2. ,  2. ,  0. ],\n",
       "          [ 6.2,  4. ,  1. ,  2. ,  0. ],\n",
       "          [ 6.1,  2. ,  1. ,  3. ,  0. ],\n",
       "          [ 4.2,  2. ,  0. ,  3. ,  0. ],\n",
       "          [ 4.1,  0. ,  0. ,  3. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  4. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  5. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  6. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  7. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  8. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  9. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 10. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 11. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 12. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 13. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 14. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 15. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 16. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 17. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 18. ]],\n",
       "  \n",
       "         [[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 1.1, 14. , 11. ,  2. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  2. ,  0. ],\n",
       "          [ 1.1, 14. , 10. ,  3. ,  0. ],\n",
       "          [ 3.2, 14. ,  9. ,  3. ,  0. ],\n",
       "          [ 5.1, 14. ,  8. ,  3. ,  0. ],\n",
       "          [ 3.2, 14. ,  8. ,  3. ,  0. ],\n",
       "          [ 1.1, 14. ,  8. ,  4. ,  0. ],\n",
       "          [ 5.2, 14. ,  7. ,  4. ,  0. ],\n",
       "          [ 5.1, 12. ,  7. ,  4. ,  0. ],\n",
       "          [ 6.1, 12. ,  6. ,  4. ,  0. ],\n",
       "          [ 1.1, 12. ,  6. ,  5. ,  0. ],\n",
       "          [ 2.2, 12. ,  5. ,  5. ,  0. ],\n",
       "          [ 6.1, 12. ,  5. ,  5. ,  0. ],\n",
       "          [ 6.1, 12. ,  5. ,  5. ,  0. ],\n",
       "          [ 3.2, 12. ,  5. ,  5. ,  0. ],\n",
       "          [ 6.2, 12. ,  4. ,  5. ,  0. ],\n",
       "          [ 1.1, 12. ,  4. ,  6. ,  0. ],\n",
       "          [ 2.2, 12. ,  4. ,  6. ,  0. ],\n",
       "          [ 6.1, 12. ,  4. ,  6. ,  0. ],\n",
       "          [ 1.2, 12. ,  3. ,  6. ,  0. ],\n",
       "          [ 6.1, 12. ,  3. ,  6. ,  0. ],\n",
       "          [ 4.2, 12. ,  2. ,  6. ,  0. ],\n",
       "          [ 6.1, 12. ,  2. ,  6. ,  0. ],\n",
       "          [ 6.2, 12. ,  2. ,  6. ,  0. ],\n",
       "          [ 7.1, 12. ,  1. ,  6. ,  0. ],\n",
       "          [ 4.2, 12. ,  1. ,  6. ,  0. ],\n",
       "          [ 7.2, 12. ,  0. ,  6. ,  0. ],\n",
       "          [ 4.2, 12. ,  0. ,  6. ,  0. ],\n",
       "          [ 1.2, 12. ,  0. ,  6. ,  0. ],\n",
       "          [ 4.2, 12. ,  0. ,  6. ,  0. ],\n",
       "          [ 3.1, 12. ,  0. ,  6. ,  0. ],\n",
       "          [ 4.2, 12. ,  0. ,  6. ,  0. ],\n",
       "          [ 2.1, 12. ,  0. ,  6. ,  0. ],\n",
       "          [ 4.2, 12. ,  0. ,  6. ,  0. ],\n",
       "          [ 4.1, 10. ,  0. ,  6. ,  0. ],\n",
       "          [ 3.2, 10. ,  0. ,  6. ,  0. ],\n",
       "          [ 1.2, 10. ,  0. ,  6. ,  0. ]]]),\n",
       "  array([[1., 0.],\n",
       "         [0., 1.]], dtype=float32)),\n",
       " (array([[[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  1. ,  0. ],\n",
       "          [ 3.2, 14. ,  9. ,  1. ,  0. ],\n",
       "          [ 3.1, 12. ,  9. ,  2. ,  0. ],\n",
       "          [ 5.1, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 5.2, 10. ,  7. ,  2. ,  0. ],\n",
       "          [ 6.1, 10. ,  6. ,  2. ,  0. ],\n",
       "          [ 2.1, 10. ,  6. ,  2. ,  0. ],\n",
       "          [ 6.2, 10. ,  5. ,  2. ,  0. ],\n",
       "          [ 6.1,  8. ,  5. ,  2. ,  0. ],\n",
       "          [ 4.2,  8. ,  4. ,  2. ,  0. ],\n",
       "          [ 7.1,  8. ,  3. ,  2. ,  0. ],\n",
       "          [ 1.2,  8. ,  2. ,  2. ,  0. ],\n",
       "          [ 4.2,  8. ,  2. ,  2. ,  0. ],\n",
       "          [ 2.2,  8. ,  1. ,  2. ,  0. ],\n",
       "          [ 2.1,  6. ,  1. ,  3. ,  0. ],\n",
       "          [ 7.2,  6. ,  0. ,  3. ,  0. ],\n",
       "          [ 1.2,  6. ,  0. ,  3. ,  0. ],\n",
       "          [ 4.2,  6. ,  0. ,  3. ,  0. ],\n",
       "          [ 4.1,  4. ,  0. ,  3. ,  0. ],\n",
       "          [ 7.1,  4. ,  0. ,  3. ,  0. ],\n",
       "          [ 7.2,  2. ,  0. ,  3. ,  0. ],\n",
       "          [ 1.2,  2. ,  0. ,  3. ,  0. ],\n",
       "          [ 1.1,  0. ,  0. ,  3. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  4. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  5. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  6. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  7. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  8. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  9. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 10. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 11. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 12. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 13. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 14. ]],\n",
       "  \n",
       "         [[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 10. ,  2. ,  0. ],\n",
       "          [ 5.1, 14. ,  9. ,  2. ,  0. ],\n",
       "          [ 6.1, 14. ,  8. ,  2. ,  0. ],\n",
       "          [ 7.1, 14. ,  7. ,  2. ,  0. ],\n",
       "          [ 7.2, 14. ,  6. ,  2. ,  0. ],\n",
       "          [ 7.1, 12. ,  6. ,  2. ,  0. ],\n",
       "          [ 1.2, 12. ,  5. ,  2. ,  0. ],\n",
       "          [ 1.1, 10. ,  5. ,  2. ,  0. ],\n",
       "          [ 3.2, 10. ,  4. ,  2. ,  0. ],\n",
       "          [ 4.1, 10. ,  4. ,  2. ,  0. ],\n",
       "          [ 3.1, 10. ,  4. ,  3. ,  0. ],\n",
       "          [ 4.2, 10. ,  3. ,  3. ,  0. ],\n",
       "          [ 4.1, 10. ,  3. ,  3. ,  0. ],\n",
       "          [ 4.2,  8. ,  3. ,  3. ,  0. ],\n",
       "          [ 6.2,  8. ,  2. ,  3. ,  0. ],\n",
       "          [ 5.2,  8. ,  1. ,  3. ,  0. ],\n",
       "          [ 3.1,  8. ,  1. ,  4. ,  0. ],\n",
       "          [ 6.2,  8. ,  1. ,  4. ,  0. ],\n",
       "          [ 5.1,  8. ,  1. ,  4. ,  0. ],\n",
       "          [ 3.2,  8. ,  1. ,  4. ,  0. ],\n",
       "          [ 6.1,  8. ,  1. ,  4. ,  0. ],\n",
       "          [ 5.1,  8. ,  1. ,  4. ,  0. ],\n",
       "          [ 3.2,  8. ,  1. ,  4. ,  0. ],\n",
       "          [ 5.1,  8. ,  1. ,  4. ,  0. ],\n",
       "          [ 3.2,  8. ,  1. ,  4. ,  0. ],\n",
       "          [ 6.1,  8. ,  1. ,  4. ,  0. ],\n",
       "          [ 2.2,  8. ,  0. ,  4. ,  0. ],\n",
       "          [ 6.1,  8. ,  0. ,  4. ,  0. ],\n",
       "          [ 2.2,  8. ,  0. ,  4. ,  0. ],\n",
       "          [ 5.1,  8. ,  0. ,  5. ,  0. ],\n",
       "          [ 2.2,  8. ,  0. ,  5. ,  0. ],\n",
       "          [ 6.2,  8. ,  0. ,  5. ,  0. ],\n",
       "          [ 5.2,  8. ,  0. ,  5. ,  0. ],\n",
       "          [ 2.2,  8. ,  0. ,  5. ,  0. ],\n",
       "          [ 2.1,  8. ,  0. ,  5. ,  0. ],\n",
       "          [ 2.2,  6. ,  0. ,  5. ,  0. ]]]),\n",
       "  array([[1., 0.],\n",
       "         [0., 1.]], dtype=float32)),\n",
       " (array([[[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  1. ,  0. ],\n",
       "          [ 4.2, 14. ,  9. ,  1. ,  0. ],\n",
       "          [ 4.1, 12. ,  9. ,  2. ,  0. ],\n",
       "          [ 1.2, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 1.1, 10. ,  8. ,  2. ,  0. ],\n",
       "          [ 5.1, 10. ,  7. ,  2. ,  0. ],\n",
       "          [ 2.1, 10. ,  7. ,  2. ,  0. ],\n",
       "          [ 3.1, 10. ,  7. ,  2. ,  0. ],\n",
       "          [ 6.1, 10. ,  6. ,  2. ,  0. ],\n",
       "          [ 5.2, 10. ,  5. ,  2. ,  0. ],\n",
       "          [ 5.1,  8. ,  5. ,  2. ,  0. ],\n",
       "          [ 6.2,  8. ,  4. ,  2. ,  0. ],\n",
       "          [ 6.1,  6. ,  4. ,  2. ,  0. ],\n",
       "          [ 7.1,  6. ,  3. ,  2. ,  0. ],\n",
       "          [ 3.2,  6. ,  2. ,  2. ,  0. ],\n",
       "          [ 2.1,  6. ,  2. ,  3. ,  0. ],\n",
       "          [ 3.1,  6. ,  2. ,  3. ,  0. ],\n",
       "          [ 3.1,  6. ,  2. ,  4. ,  0. ],\n",
       "          [ 3.2,  4. ,  2. ,  4. ,  0. ],\n",
       "          [ 7.2,  4. ,  1. ,  4. ,  0. ],\n",
       "          [ 7.1,  2. ,  1. ,  4. ,  0. ],\n",
       "          [ 2.2,  2. ,  0. ,  4. ,  0. ],\n",
       "          [ 2.1,  0. ,  0. ,  4. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  4. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  5. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  6. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  7. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  8. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  9. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. , 10. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. , 11. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. , 12. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. , 13. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. , 14. ]],\n",
       "  \n",
       "         [[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 1.2, 14. , 10. ,  1. ,  0. ],\n",
       "          [ 1.1, 14. , 10. ,  2. ,  0. ],\n",
       "          [ 1.2, 12. , 10. ,  2. ,  0. ],\n",
       "          [ 4.1, 12. ,  9. ,  2. ,  0. ],\n",
       "          [ 3.1, 12. ,  9. ,  2. ,  0. ],\n",
       "          [ 3.2, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 4.1, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 3.2, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 3.1, 10. ,  8. ,  3. ,  0. ],\n",
       "          [ 5.1, 10. ,  7. ,  3. ,  0. ],\n",
       "          [ 6.1, 10. ,  6. ,  3. ,  0. ],\n",
       "          [ 7.1, 10. ,  5. ,  3. ,  0. ],\n",
       "          [ 4.1, 10. ,  5. ,  3. ,  0. ],\n",
       "          [ 2.1, 10. ,  5. ,  3. ,  0. ],\n",
       "          [ 7.1, 10. ,  5. ,  3. ,  0. ],\n",
       "          [ 4.1, 10. ,  5. ,  4. ,  0. ],\n",
       "          [ 2.1, 10. ,  5. ,  4. ,  0. ],\n",
       "          [ 4.2, 10. ,  4. ,  4. ,  0. ],\n",
       "          [ 7.2, 10. ,  3. ,  4. ,  0. ],\n",
       "          [ 7.1, 10. ,  3. ,  4. ,  0. ],\n",
       "          [ 7.2,  8. ,  3. ,  4. ,  0. ],\n",
       "          [ 6.2,  8. ,  2. ,  4. ,  0. ],\n",
       "          [ 5.2,  8. ,  1. ,  4. ,  0. ],\n",
       "          [ 6.1,  8. ,  1. ,  4. ,  0. ],\n",
       "          [ 5.2,  8. ,  1. ,  4. ,  0. ],\n",
       "          [ 6.2,  8. ,  1. ,  4. ,  0. ],\n",
       "          [ 6.1,  6. ,  1. ,  4. ,  0. ],\n",
       "          [ 4.2,  6. ,  1. ,  4. ,  0. ],\n",
       "          [ 5.2,  6. ,  1. ,  4. ,  0. ],\n",
       "          [ 2.2,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.2,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.1,  6. ,  0. ,  5. ,  0. ],\n",
       "          [ 4.2,  4. ,  0. ,  5. ,  0. ],\n",
       "          [ 5.2,  4. ,  0. ,  5. ,  0. ],\n",
       "          [ 2.1,  4. ,  0. ,  5. ,  0. ],\n",
       "          [ 5.1,  4. ,  0. ,  5. ,  0. ],\n",
       "          [ 5.2,  2. ,  0. ,  5. ,  0. ]]]),\n",
       "  array([[1., 0.],\n",
       "         [0., 1.]], dtype=float32)),\n",
       " (array([[[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  1. ,  0. ],\n",
       "          [ 5.1, 14. ,  9. ,  1. ,  0. ],\n",
       "          [ 6.1, 14. ,  8. ,  1. ,  0. ],\n",
       "          [ 1.1, 14. ,  8. ,  2. ,  0. ],\n",
       "          [ 7.1, 14. ,  7. ,  2. ,  0. ],\n",
       "          [ 3.2, 14. ,  6. ,  2. ,  0. ],\n",
       "          [ 5.1, 14. ,  6. ,  2. ,  0. ],\n",
       "          [ 3.2, 14. ,  6. ,  2. ,  0. ],\n",
       "          [ 3.1, 12. ,  6. ,  2. ,  0. ],\n",
       "          [ 7.2, 12. ,  5. ,  2. ,  0. ],\n",
       "          [ 7.1, 10. ,  5. ,  2. ,  0. ],\n",
       "          [ 6.2, 10. ,  4. ,  2. ,  0. ],\n",
       "          [ 6.1,  8. ,  4. ,  2. ,  0. ],\n",
       "          [ 5.2,  8. ,  3. ,  2. ,  0. ],\n",
       "          [ 4.2,  8. ,  2. ,  2. ,  0. ],\n",
       "          [ 5.1,  8. ,  2. ,  3. ,  0. ],\n",
       "          [ 5.2,  6. ,  2. ,  3. ,  0. ],\n",
       "          [ 1.2,  6. ,  1. ,  3. ,  0. ],\n",
       "          [ 1.1,  4. ,  1. ,  3. ,  0. ],\n",
       "          [ 2.2,  4. ,  0. ,  3. ,  0. ],\n",
       "          [ 4.1,  4. ,  0. ,  3. ,  0. ],\n",
       "          [ 4.2,  4. ,  0. ,  3. ,  0. ],\n",
       "          [ 4.1,  2. ,  0. ,  3. ,  0. ],\n",
       "          [ 2.2,  2. ,  0. ,  3. ,  0. ],\n",
       "          [ 2.1,  0. ,  0. ,  3. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  4. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  5. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  6. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  7. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  8. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  9. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 10. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 11. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 12. ]],\n",
       "  \n",
       "         [[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 1.2, 12. , 12. ,  1. ,  0. ],\n",
       "          [ 2.1, 12. , 11. ,  1. ,  0. ],\n",
       "          [ 3.1, 12. , 10. ,  1. ,  0. ],\n",
       "          [ 4.1, 12. ,  9. ,  1. ,  0. ],\n",
       "          [ 5.1, 12. ,  8. ,  1. ,  0. ],\n",
       "          [ 3.1, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 5.1, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 6.1, 12. ,  7. ,  2. ,  0. ],\n",
       "          [ 3.2, 12. ,  6. ,  2. ,  0. ],\n",
       "          [ 7.1, 12. ,  5. ,  2. ,  0. ],\n",
       "          [ 2.1, 12. ,  5. ,  2. ,  0. ],\n",
       "          [ 5.2, 12. ,  4. ,  2. ,  0. ],\n",
       "          [ 2.1, 12. ,  4. ,  3. ,  0. ],\n",
       "          [ 4.1, 12. ,  4. ,  3. ,  0. ],\n",
       "          [ 2.2, 12. ,  3. ,  3. ,  0. ],\n",
       "          [ 4.2, 12. ,  2. ,  3. ,  0. ],\n",
       "          [ 6.1, 12. ,  2. ,  3. ,  0. ],\n",
       "          [ 6.2, 12. ,  1. ,  3. ,  0. ],\n",
       "          [ 3.1, 12. ,  1. ,  3. ,  0. ],\n",
       "          [ 5.2, 12. ,  1. ,  3. ,  0. ],\n",
       "          [ 3.1, 12. ,  1. ,  4. ,  0. ],\n",
       "          [ 5.1, 12. ,  1. ,  4. ,  0. ],\n",
       "          [ 5.2, 10. ,  1. ,  4. ,  0. ],\n",
       "          [ 7.2, 10. ,  0. ,  4. ,  0. ],\n",
       "          [ 2.2, 10. ,  0. ,  4. ,  0. ],\n",
       "          [ 6.1, 10. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.2, 10. ,  0. ,  4. ,  0. ],\n",
       "          [ 7.2, 10. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.2, 10. ,  0. ,  4. ,  0. ],\n",
       "          [ 2.2, 10. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.2, 10. ,  0. ,  4. ,  0. ],\n",
       "          [ 7.1, 10. ,  0. ,  4. ,  0. ],\n",
       "          [ 7.2,  8. ,  0. ,  4. ,  0. ],\n",
       "          [ 3.2,  8. ,  0. ,  4. ,  0. ],\n",
       "          [ 6.2,  8. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.1,  8. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.2,  6. ,  0. ,  5. ,  0. ],\n",
       "          [ 2.2,  6. ,  0. ,  5. ,  0. ],\n",
       "          [ 2.1,  4. ,  0. ,  5. ,  0. ]]]),\n",
       "  array([[1., 0.],\n",
       "         [0., 1.]], dtype=float32)),\n",
       " (array([[[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 3.2, 12. , 10. ,  1. ,  0. ],\n",
       "          [ 4.1, 12. ,  9. ,  1. ,  0. ],\n",
       "          [ 2.2, 12. ,  8. ,  1. ,  0. ],\n",
       "          [ 1.1, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 2.2, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 2.1, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 2.2, 10. ,  8. ,  3. ,  0. ],\n",
       "          [ 4.2, 10. ,  7. ,  3. ,  0. ],\n",
       "          [ 4.1,  8. ,  7. ,  3. ,  0. ],\n",
       "          [ 5.1,  8. ,  6. ,  3. ,  0. ],\n",
       "          [ 6.1,  8. ,  5. ,  3. ,  0. ],\n",
       "          [ 7.1,  8. ,  4. ,  3. ,  0. ],\n",
       "          [ 7.2,  6. ,  3. ,  3. ,  0. ],\n",
       "          [ 1.2,  6. ,  2. ,  3. ,  0. ],\n",
       "          [ 1.1,  4. ,  2. ,  3. ,  0. ],\n",
       "          [ 6.2,  4. ,  1. ,  3. ,  0. ],\n",
       "          [ 6.1,  2. ,  1. ,  3. ,  0. ],\n",
       "          [ 5.1,  2. ,  1. ,  3. ,  0. ],\n",
       "          [ 5.2,  0. ,  0. ,  3. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  4. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  5. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  6. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  7. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  8. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  9. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 10. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 11. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 12. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 13. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 14. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 15. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 16. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 17. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 18. ]],\n",
       "  \n",
       "         [[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  1. ,  0. ],\n",
       "          [ 5.1, 14. ,  9. ,  1. ,  0. ],\n",
       "          [ 6.1, 14. ,  8. ,  1. ,  0. ],\n",
       "          [ 1.2, 14. ,  7. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. ,  7. ,  2. ,  0. ],\n",
       "          [ 2.2, 14. ,  6. ,  2. ,  0. ],\n",
       "          [ 2.1, 12. ,  6. ,  3. ,  0. ],\n",
       "          [ 1.1, 12. ,  6. ,  3. ,  0. ],\n",
       "          [ 1.2, 10. ,  6. ,  3. ,  0. ],\n",
       "          [ 7.1, 10. ,  5. ,  3. ,  0. ],\n",
       "          [ 4.2, 10. ,  4. ,  3. ,  0. ],\n",
       "          [ 6.2, 10. ,  3. ,  3. ,  0. ],\n",
       "          [ 3.2, 10. ,  2. ,  3. ,  0. ],\n",
       "          [ 5.2, 10. ,  1. ,  3. ,  0. ],\n",
       "          [ 7.2, 10. ,  0. ,  3. ,  0. ],\n",
       "          [ 4.1, 10. ,  0. ,  3. ,  0. ],\n",
       "          [ 6.1, 10. ,  0. ,  3. ,  0. ],\n",
       "          [ 5.1, 10. ,  0. ,  3. ,  0. ],\n",
       "          [ 4.1, 10. ,  0. ,  3. ,  0. ],\n",
       "          [ 3.1, 10. ,  0. ,  3. ,  0. ],\n",
       "          [ 3.2,  8. ,  0. ,  3. ,  0. ],\n",
       "          [ 5.2,  8. ,  0. ,  3. ,  0. ],\n",
       "          [ 5.1,  6. ,  0. ,  3. ,  0. ],\n",
       "          [ 4.1,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 6.1,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 6.2,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 6.1,  4. ,  0. ,  4. ,  0. ],\n",
       "          [ 7.2,  4. ,  0. ,  4. ,  0. ],\n",
       "          [ 7.1,  2. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.2,  2. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.1,  0. ,  0. ,  5. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  5. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  5. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  5. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  5. ,  4. ],\n",
       "          [ 0. ,  0. ,  0. ,  5. ,  5. ],\n",
       "          [ 0. ,  0. ,  0. ,  5. ,  6. ]]]),\n",
       "  array([[1., 0.],\n",
       "         [0., 1.]], dtype=float32)),\n",
       " (array([[[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  1. ,  0. ],\n",
       "          [ 2.2, 14. ,  9. ,  1. ,  0. ],\n",
       "          [ 2.1, 12. ,  9. ,  2. ,  0. ],\n",
       "          [ 5.1, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 3.2, 12. ,  7. ,  2. ,  0. ],\n",
       "          [ 1.2, 12. ,  6. ,  2. ,  0. ],\n",
       "          [ 5.1, 12. ,  6. ,  2. ,  0. ],\n",
       "          [ 6.1, 12. ,  5. ,  2. ,  0. ],\n",
       "          [ 1.1, 12. ,  5. ,  2. ,  0. ],\n",
       "          [ 5.2, 12. ,  4. ,  2. ,  0. ],\n",
       "          [ 1.1, 12. ,  4. ,  3. ,  0. ],\n",
       "          [ 5.1, 12. ,  4. ,  3. ,  0. ],\n",
       "          [ 5.2, 10. ,  4. ,  3. ,  0. ],\n",
       "          [ 1.2, 10. ,  4. ,  3. ,  0. ],\n",
       "          [ 1.1,  8. ,  4. ,  4. ,  0. ],\n",
       "          [ 3.2,  8. ,  4. ,  4. ,  0. ],\n",
       "          [ 6.1,  8. ,  4. ,  4. ,  0. ],\n",
       "          [ 7.1,  8. ,  3. ,  4. ,  0. ],\n",
       "          [ 3.2,  8. ,  3. ,  4. ,  0. ],\n",
       "          [ 3.1,  8. ,  3. ,  4. ,  0. ],\n",
       "          [ 3.2,  6. ,  3. ,  4. ,  0. ],\n",
       "          [ 4.1,  6. ,  3. ,  4. ,  0. ],\n",
       "          [ 6.1,  6. ,  3. ,  4. ,  0. ],\n",
       "          [ 7.1,  6. ,  3. ,  4. ,  0. ],\n",
       "          [ 4.2,  6. ,  2. ,  4. ,  0. ],\n",
       "          [ 4.1,  6. ,  2. ,  4. ,  0. ],\n",
       "          [ 4.2,  4. ,  2. ,  4. ,  0. ],\n",
       "          [ 6.2,  4. ,  1. ,  4. ,  0. ],\n",
       "          [ 6.1,  2. ,  1. ,  4. ,  0. ],\n",
       "          [ 7.1,  2. ,  1. ,  4. ,  0. ],\n",
       "          [ 7.2,  0. ,  0. ,  4. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  4. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  5. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  6. ]],\n",
       "  \n",
       "         [[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 2.2, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 1.1, 14. , 11. ,  2. ,  0. ],\n",
       "          [ 2.1, 14. , 11. ,  2. ,  0. ],\n",
       "          [ 2.2, 12. , 11. ,  2. ,  0. ],\n",
       "          [ 3.1, 12. , 10. ,  2. ,  0. ],\n",
       "          [ 1.2, 12. ,  9. ,  2. ,  0. ],\n",
       "          [ 4.1, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 5.1, 12. ,  7. ,  2. ,  0. ],\n",
       "          [ 5.1, 12. ,  7. ,  2. ,  0. ],\n",
       "          [ 1.1, 12. ,  7. ,  3. ,  0. ],\n",
       "          [ 4.2, 12. ,  6. ,  3. ,  0. ],\n",
       "          [ 4.1, 10. ,  6. ,  3. ,  0. ],\n",
       "          [ 6.1, 10. ,  5. ,  3. ,  0. ],\n",
       "          [ 1.2, 10. ,  5. ,  3. ,  0. ],\n",
       "          [ 3.1, 10. ,  5. ,  3. ,  0. ],\n",
       "          [ 6.1, 10. ,  5. ,  3. ,  0. ],\n",
       "          [ 7.1, 10. ,  4. ,  3. ,  0. ],\n",
       "          [ 1.1, 10. ,  4. ,  4. ,  0. ],\n",
       "          [ 3.2, 10. ,  3. ,  4. ,  0. ],\n",
       "          [ 3.1,  8. ,  3. ,  4. ,  0. ],\n",
       "          [ 7.2,  8. ,  2. ,  4. ,  0. ],\n",
       "          [ 6.2,  8. ,  1. ,  4. ,  0. ],\n",
       "          [ 5.2,  8. ,  0. ,  4. ,  0. ],\n",
       "          [ 6.2,  8. ,  0. ,  4. ,  0. ],\n",
       "          [ 6.2,  8. ,  0. ,  4. ,  0. ],\n",
       "          [ 7.1,  8. ,  0. ,  4. ,  0. ],\n",
       "          [ 5.1,  8. ,  0. ,  4. ,  0. ],\n",
       "          [ 6.2,  8. ,  0. ,  4. ,  0. ],\n",
       "          [ 6.2,  8. ,  0. ,  5. ,  0. ],\n",
       "          [ 1.2,  8. ,  0. ,  5. ,  0. ],\n",
       "          [ 7.2,  8. ,  0. ,  5. ,  0. ],\n",
       "          [ 6.2,  8. ,  0. ,  6. ,  0. ],\n",
       "          [ 7.2,  8. ,  0. ,  6. ,  0. ],\n",
       "          [ 5.2,  8. ,  0. ,  6. ,  0. ],\n",
       "          [ 1.1,  8. ,  0. ,  6. ,  0. ],\n",
       "          [ 6.2,  8. ,  0. ,  7. ,  0. ],\n",
       "          [ 1.2,  8. ,  0. ,  7. ,  0. ],\n",
       "          [ 6.2,  8. ,  0. ,  8. ,  0. ]]]),\n",
       "  array([[1., 0.],\n",
       "         [0., 1.]], dtype=float32)),\n",
       " (array([[[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 1.2, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 1.1, 12. , 11. ,  2. ,  0. ],\n",
       "          [ 3.1, 12. , 10. ,  2. ,  0. ],\n",
       "          [ 4.1, 12. ,  9. ,  2. ,  0. ],\n",
       "          [ 5.1, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 2.2, 12. ,  7. ,  2. ,  0. ],\n",
       "          [ 2.1, 12. ,  7. ,  2. ,  0. ],\n",
       "          [ 2.2, 10. ,  7. ,  2. ,  0. ],\n",
       "          [ 6.1, 10. ,  6. ,  2. ,  0. ],\n",
       "          [ 5.2, 10. ,  5. ,  2. ,  0. ],\n",
       "          [ 5.1, 10. ,  5. ,  2. ,  0. ],\n",
       "          [ 5.2,  8. ,  5. ,  2. ,  0. ],\n",
       "          [ 3.2,  8. ,  4. ,  2. ,  0. ],\n",
       "          [ 3.1,  6. ,  4. ,  2. ,  0. ],\n",
       "          [ 7.1,  6. ,  3. ,  2. ,  0. ],\n",
       "          [ 4.2,  6. ,  2. ,  2. ,  0. ],\n",
       "          [ 6.1,  6. ,  2. ,  2. ,  0. ],\n",
       "          [ 4.2,  6. ,  2. ,  2. ,  0. ],\n",
       "          [ 4.1,  6. ,  2. ,  2. ,  0. ],\n",
       "          [ 4.2,  4. ,  2. ,  3. ,  0. ],\n",
       "          [ 7.2,  4. ,  1. ,  3. ,  0. ],\n",
       "          [ 7.1,  2. ,  1. ,  3. ,  0. ],\n",
       "          [ 6.1,  2. ,  1. ,  3. ,  0. ],\n",
       "          [ 6.2,  0. ,  0. ,  3. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  4. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  5. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  6. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  7. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  8. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  9. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 10. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 11. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 12. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 13. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 14. ]],\n",
       "  \n",
       "         [[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 1.1, 14. , 11. ,  2. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  2. ,  0. ],\n",
       "          [ 1.1, 14. , 10. ,  3. ,  0. ],\n",
       "          [ 3.1, 14. , 10. ,  3. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  3. ,  0. ],\n",
       "          [ 2.1, 14. , 10. ,  3. ,  0. ],\n",
       "          [ 5.1, 14. ,  9. ,  3. ,  0. ],\n",
       "          [ 4.1, 14. ,  9. ,  3. ,  0. ],\n",
       "          [ 5.1, 14. ,  9. ,  3. ,  0. ],\n",
       "          [ 3.1, 14. ,  9. ,  3. ,  0. ],\n",
       "          [ 5.1, 14. ,  9. ,  3. ,  0. ],\n",
       "          [ 1.1, 14. ,  9. ,  4. ,  0. ],\n",
       "          [ 4.1, 14. ,  9. ,  4. ,  0. ],\n",
       "          [ 2.2, 14. ,  8. ,  4. ,  0. ],\n",
       "          [ 6.1, 14. ,  7. ,  4. ,  0. ],\n",
       "          [ 7.1, 14. ,  6. ,  4. ,  0. ],\n",
       "          [ 6.1, 14. ,  6. ,  4. ,  0. ],\n",
       "          [ 7.1, 14. ,  6. ,  4. ,  0. ],\n",
       "          [ 2.2, 14. ,  6. ,  4. ,  0. ],\n",
       "          [ 5.1, 14. ,  6. ,  4. ,  0. ],\n",
       "          [ 6.2, 14. ,  5. ,  4. ,  0. ],\n",
       "          [ 7.1, 14. ,  5. ,  4. ,  0. ],\n",
       "          [ 6.2, 14. ,  5. ,  4. ,  0. ],\n",
       "          [ 5.2, 14. ,  4. ,  4. ,  0. ],\n",
       "          [ 4.1, 14. ,  4. ,  5. ,  0. ],\n",
       "          [ 2.2, 14. ,  4. ,  5. ,  0. ],\n",
       "          [ 5.2, 14. ,  4. ,  5. ,  0. ],\n",
       "          [ 3.2, 14. ,  3. ,  5. ,  0. ],\n",
       "          [ 1.1, 14. ,  3. ,  5. ,  0. ],\n",
       "          [ 4.2, 14. ,  2. ,  5. ,  0. ],\n",
       "          [ 5.2, 14. ,  2. ,  5. ,  0. ],\n",
       "          [ 1.2, 14. ,  1. ,  5. ,  0. ],\n",
       "          [ 1.1, 12. ,  1. ,  6. ,  0. ],\n",
       "          [ 7.2, 12. ,  0. ,  6. ,  0. ],\n",
       "          [ 5.2, 12. ,  0. ,  6. ,  0. ],\n",
       "          [ 4.2, 12. ,  0. ,  6. ,  0. ],\n",
       "          [ 7.1, 12. ,  0. ,  6. ,  0. ]]]),\n",
       "  array([[1., 0.],\n",
       "         [0., 1.]], dtype=float32)),\n",
       " (array([[[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 2.2, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 2.1, 12. , 11. ,  2. ,  0. ],\n",
       "          [ 3.1, 12. , 10. ,  2. ,  0. ],\n",
       "          [ 1.2, 12. ,  9. ,  2. ,  0. ],\n",
       "          [ 1.1, 12. ,  9. ,  2. ,  0. ],\n",
       "          [ 1.2, 10. ,  9. ,  2. ,  0. ],\n",
       "          [ 4.1, 10. ,  8. ,  2. ,  0. ],\n",
       "          [ 5.1, 10. ,  7. ,  2. ,  0. ],\n",
       "          [ 6.1, 10. ,  6. ,  2. ,  0. ],\n",
       "          [ 3.1, 10. ,  6. ,  2. ,  0. ],\n",
       "          [ 7.1, 10. ,  5. ,  2. ,  0. ],\n",
       "          [ 4.2, 10. ,  4. ,  2. ,  0. ],\n",
       "          [ 7.2, 10. ,  3. ,  2. ,  0. ],\n",
       "          [ 4.1, 10. ,  3. ,  2. ,  0. ],\n",
       "          [ 6.1, 10. ,  3. ,  2. ,  0. ],\n",
       "          [ 5.1, 10. ,  3. ,  2. ,  0. ],\n",
       "          [ 7.1, 10. ,  3. ,  2. ,  0. ],\n",
       "          [ 7.2,  8. ,  3. ,  2. ,  0. ],\n",
       "          [ 6.2,  8. ,  2. ,  2. ,  0. ],\n",
       "          [ 6.1,  6. ,  2. ,  3. ,  0. ],\n",
       "          [ 5.2,  6. ,  1. ,  3. ,  0. ],\n",
       "          [ 4.1,  6. ,  1. ,  3. ,  0. ],\n",
       "          [ 3.2,  6. ,  0. ,  3. ,  0. ],\n",
       "          [ 3.1,  4. ,  0. ,  3. ,  0. ],\n",
       "          [ 5.1,  4. ,  0. ,  3. ,  0. ],\n",
       "          [ 5.2,  2. ,  0. ,  3. ,  0. ],\n",
       "          [ 4.1,  2. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.2,  0. ,  0. ,  4. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  4. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  5. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  6. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  7. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  8. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  9. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. , 10. ]],\n",
       "  \n",
       "         [[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 11. ,  2. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  2. ,  0. ],\n",
       "          [ 2.2, 14. ,  9. ,  2. ,  0. ],\n",
       "          [ 1.2, 14. ,  8. ,  2. ,  0. ],\n",
       "          [ 1.1, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 5.1, 12. ,  7. ,  2. ,  0. ],\n",
       "          [ 6.1, 12. ,  6. ,  2. ,  0. ],\n",
       "          [ 7.1, 12. ,  5. ,  2. ,  0. ],\n",
       "          [ 3.1, 12. ,  5. ,  2. ,  0. ],\n",
       "          [ 4.1, 12. ,  5. ,  2. ,  0. ],\n",
       "          [ 7.1, 12. ,  5. ,  2. ,  0. ],\n",
       "          [ 5.2, 12. ,  4. ,  2. ,  0. ],\n",
       "          [ 2.2, 12. ,  4. ,  2. ,  0. ],\n",
       "          [ 2.1, 12. ,  4. ,  3. ,  0. ],\n",
       "          [ 5.2, 12. ,  4. ,  3. ,  0. ],\n",
       "          [ 7.1, 12. ,  4. ,  3. ,  0. ],\n",
       "          [ 2.1, 12. ,  4. ,  4. ,  0. ],\n",
       "          [ 6.1, 12. ,  4. ,  4. ,  0. ],\n",
       "          [ 6.2, 10. ,  3. ,  4. ,  0. ],\n",
       "          [ 7.2, 10. ,  2. ,  4. ,  0. ],\n",
       "          [ 5.1, 10. ,  2. ,  4. ,  0. ],\n",
       "          [ 5.2, 10. ,  2. ,  4. ,  0. ],\n",
       "          [ 5.1,  8. ,  2. ,  4. ,  0. ],\n",
       "          [ 7.1,  8. ,  2. ,  4. ,  0. ],\n",
       "          [ 7.2,  6. ,  2. ,  4. ,  0. ],\n",
       "          [ 3.2,  6. ,  1. ,  4. ,  0. ],\n",
       "          [ 4.2,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 2.1,  6. ,  0. ,  5. ,  0. ],\n",
       "          [ 2.2,  4. ,  0. ,  5. ,  0. ],\n",
       "          [ 3.1,  4. ,  0. ,  5. ,  0. ],\n",
       "          [ 3.2,  2. ,  0. ,  5. ,  0. ],\n",
       "          [ 4.1,  2. ,  0. ,  5. ,  0. ],\n",
       "          [ 4.2,  0. ,  0. ,  5. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  5. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  5. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  5. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  5. ,  4. ]]]),\n",
       "  array([[1., 0.],\n",
       "         [0., 1.]], dtype=float32)),\n",
       " (array([[[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 1.2, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 1.1, 12. , 11. ,  2. ,  0. ],\n",
       "          [ 3.1, 12. , 10. ,  2. ,  0. ],\n",
       "          [ 4.1, 12. ,  9. ,  2. ,  0. ],\n",
       "          [ 5.1, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 4.2, 12. ,  7. ,  2. ,  0. ],\n",
       "          [ 4.1, 12. ,  7. ,  2. ,  0. ],\n",
       "          [ 4.2, 10. ,  7. ,  2. ,  0. ],\n",
       "          [ 2.2, 10. ,  6. ,  2. ,  0. ],\n",
       "          [ 6.1, 10. ,  5. ,  2. ,  0. ],\n",
       "          [ 7.1, 10. ,  4. ,  2. ,  0. ],\n",
       "          [ 7.2,  8. ,  3. ,  2. ,  0. ],\n",
       "          [ 5.2,  8. ,  2. ,  2. ,  0. ],\n",
       "          [ 2.2,  8. ,  2. ,  2. ,  0. ],\n",
       "          [ 6.2,  8. ,  1. ,  2. ,  0. ],\n",
       "          [ 2.1,  8. ,  1. ,  2. ,  0. ],\n",
       "          [ 3.2,  8. ,  0. ,  2. ,  0. ],\n",
       "          [ 6.1,  8. ,  0. ,  2. ,  0. ],\n",
       "          [ 2.2,  8. ,  0. ,  3. ,  0. ],\n",
       "          [ 5.1,  8. ,  0. ,  3. ,  0. ],\n",
       "          [ 3.1,  8. ,  0. ,  3. ,  0. ],\n",
       "          [ 5.2,  8. ,  0. ,  3. ,  0. ],\n",
       "          [ 6.2,  8. ,  0. ,  3. ,  0. ],\n",
       "          [ 2.1,  8. ,  0. ,  3. ,  0. ],\n",
       "          [ 3.1,  8. ,  0. ,  3. ,  0. ],\n",
       "          [ 5.1,  8. ,  0. ,  3. ,  0. ],\n",
       "          [ 2.2,  8. ,  0. ,  4. ,  0. ],\n",
       "          [ 2.1,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 3.2,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 3.1,  4. ,  0. ,  4. ,  0. ],\n",
       "          [ 6.2,  4. ,  0. ,  4. ,  0. ],\n",
       "          [ 6.1,  2. ,  0. ,  4. ,  0. ],\n",
       "          [ 5.1,  2. ,  0. ,  4. ,  0. ],\n",
       "          [ 5.2,  0. ,  0. ,  4. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  4. ]],\n",
       "  \n",
       "         [[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  1. ,  0. ],\n",
       "          [ 1.1, 14. , 10. ,  2. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  2. ,  0. ],\n",
       "          [ 4.2, 14. ,  9. ,  2. ,  0. ],\n",
       "          [ 1.1, 14. ,  9. ,  3. ,  0. ],\n",
       "          [ 4.1, 14. ,  9. ,  3. ,  0. ],\n",
       "          [ 4.2, 12. ,  9. ,  3. ,  0. ],\n",
       "          [ 3.2, 12. ,  8. ,  3. ,  0. ],\n",
       "          [ 3.1, 10. ,  8. ,  3. ,  0. ],\n",
       "          [ 5.1, 10. ,  7. ,  3. ,  0. ],\n",
       "          [ 6.1, 10. ,  6. ,  3. ,  0. ],\n",
       "          [ 1.2, 10. ,  5. ,  3. ,  0. ],\n",
       "          [ 7.1, 10. ,  4. ,  3. ,  0. ],\n",
       "          [ 5.2, 10. ,  3. ,  3. ,  0. ],\n",
       "          [ 6.1, 10. ,  3. ,  3. ,  0. ],\n",
       "          [ 6.2, 10. ,  2. ,  3. ,  0. ],\n",
       "          [ 6.1,  8. ,  2. ,  3. ,  0. ],\n",
       "          [ 7.2,  8. ,  1. ,  3. ,  0. ],\n",
       "          [ 7.1,  6. ,  1. ,  3. ,  0. ],\n",
       "          [ 2.2,  6. ,  0. ,  3. ,  0. ],\n",
       "          [ 2.1,  4. ,  0. ,  3. ,  0. ],\n",
       "          [ 1.1,  4. ,  0. ,  4. ,  0. ],\n",
       "          [ 1.2,  2. ,  0. ,  4. ,  0. ],\n",
       "          [ 5.1,  2. ,  0. ,  4. ,  0. ],\n",
       "          [ 5.2,  0. ,  0. ,  4. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  4. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  5. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  6. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  7. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  8. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  9. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. , 10. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. , 11. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. , 12. ]]]),\n",
       "  array([[1., 0.],\n",
       "         [0., 1.]], dtype=float32)),\n",
       " (array([[[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 2.2, 14. , 10. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 10. ,  2. ,  0. ],\n",
       "          [ 2.2, 12. , 10. ,  2. ,  0. ],\n",
       "          [ 4.1, 12. ,  9. ,  2. ,  0. ],\n",
       "          [ 5.1, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 4.2, 12. ,  7. ,  2. ,  0. ],\n",
       "          [ 4.1, 10. ,  7. ,  2. ,  0. ],\n",
       "          [ 6.1, 10. ,  6. ,  2. ,  0. ],\n",
       "          [ 5.2, 10. ,  5. ,  2. ,  0. ],\n",
       "          [ 5.2, 10. ,  5. ,  2. ,  0. ],\n",
       "          [ 3.1, 10. ,  5. ,  2. ,  0. ],\n",
       "          [ 1.1, 10. ,  5. ,  2. ,  0. ],\n",
       "          [ 5.1, 10. ,  5. ,  2. ,  0. ],\n",
       "          [ 5.1, 10. ,  5. ,  3. ,  0. ],\n",
       "          [ 5.2,  8. ,  5. ,  3. ,  0. ],\n",
       "          [ 7.1,  8. ,  4. ,  3. ,  0. ],\n",
       "          [ 6.2,  8. ,  3. ,  3. ,  0. ],\n",
       "          [ 6.1,  8. ,  3. ,  3. ,  0. ],\n",
       "          [ 6.2,  6. ,  3. ,  3. ,  0. ],\n",
       "          [ 7.2,  6. ,  2. ,  3. ,  0. ],\n",
       "          [ 7.1,  4. ,  2. ,  3. ,  0. ],\n",
       "          [ 3.2,  4. ,  1. ,  3. ,  0. ],\n",
       "          [ 3.1,  2. ,  1. ,  3. ,  0. ],\n",
       "          [ 1.2,  2. ,  0. ,  3. ,  0. ],\n",
       "          [ 1.1,  0. ,  0. ,  3. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  4. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  5. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  6. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  7. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  8. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  9. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 10. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 11. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 12. ]],\n",
       "  \n",
       "         [[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 1.2, 12. , 12. ,  1. ,  0. ],\n",
       "          [ 2.1, 12. , 11. ,  1. ,  0. ],\n",
       "          [ 3.1, 12. , 10. ,  1. ,  0. ],\n",
       "          [ 3.2, 12. ,  9. ,  1. ,  0. ],\n",
       "          [ 3.1, 10. ,  9. ,  2. ,  0. ],\n",
       "          [ 4.1, 10. ,  8. ,  2. ,  0. ],\n",
       "          [ 2.1, 10. ,  8. ,  2. ,  0. ],\n",
       "          [ 5.1, 10. ,  7. ,  2. ,  0. ],\n",
       "          [ 4.1, 10. ,  7. ,  2. ,  0. ],\n",
       "          [ 2.1, 10. ,  7. ,  3. ,  0. ],\n",
       "          [ 5.1, 10. ,  7. ,  3. ,  0. ],\n",
       "          [ 4.2, 10. ,  6. ,  3. ,  0. ],\n",
       "          [ 4.1,  8. ,  6. ,  3. ,  0. ],\n",
       "          [ 2.2,  8. ,  5. ,  3. ,  0. ],\n",
       "          [ 6.1,  8. ,  4. ,  3. ,  0. ],\n",
       "          [ 2.2,  8. ,  4. ,  3. ,  0. ],\n",
       "          [ 5.1,  8. ,  4. ,  3. ,  0. ],\n",
       "          [ 6.2,  8. ,  3. ,  3. ,  0. ],\n",
       "          [ 2.1,  8. ,  3. ,  4. ,  0. ],\n",
       "          [ 5.2,  8. ,  2. ,  4. ,  0. ],\n",
       "          [ 7.1,  8. ,  1. ,  4. ,  0. ],\n",
       "          [ 7.2,  8. ,  0. ,  4. ,  0. ],\n",
       "          [ 2.1,  8. ,  0. ,  5. ,  0. ],\n",
       "          [ 2.2,  8. ,  0. ,  5. ,  0. ],\n",
       "          [ 6.1,  8. ,  0. ,  5. ,  0. ],\n",
       "          [ 2.2,  8. ,  0. ,  5. ,  0. ],\n",
       "          [ 2.1,  6. ,  0. ,  6. ,  0. ],\n",
       "          [ 5.1,  6. ,  0. ,  6. ,  0. ],\n",
       "          [ 6.2,  6. ,  0. ,  6. ,  0. ],\n",
       "          [ 5.2,  6. ,  0. ,  6. ,  0. ],\n",
       "          [ 7.1,  6. ,  0. ,  6. ,  0. ],\n",
       "          [ 6.1,  6. ,  0. ,  6. ,  0. ],\n",
       "          [ 5.2,  6. ,  0. ,  6. ,  0. ],\n",
       "          [ 5.1,  6. ,  0. ,  6. ,  0. ],\n",
       "          [ 5.2,  4. ,  0. ,  6. ,  0. ],\n",
       "          [ 7.1,  4. ,  0. ,  6. ,  0. ],\n",
       "          [ 6.2,  4. ,  0. ,  6. ,  0. ],\n",
       "          [ 6.1,  4. ,  0. ,  6. ,  0. ],\n",
       "          [ 7.1,  4. ,  0. ,  6. ,  0. ]]]),\n",
       "  array([[1., 0.],\n",
       "         [0., 1.]], dtype=float32)),\n",
       " (array([[[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  1. ,  0. ],\n",
       "          [ 2.2, 14. ,  9. ,  1. ,  0. ],\n",
       "          [ 2.1, 12. ,  9. ,  2. ,  0. ],\n",
       "          [ 3.2, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 3.1, 10. ,  8. ,  2. ,  0. ],\n",
       "          [ 5.1, 10. ,  7. ,  2. ,  0. ],\n",
       "          [ 6.1, 10. ,  6. ,  2. ,  0. ],\n",
       "          [ 7.1, 10. ,  5. ,  2. ,  0. ],\n",
       "          [ 6.2, 10. ,  4. ,  2. ,  0. ],\n",
       "          [ 6.1, 10. ,  4. ,  2. ,  0. ],\n",
       "          [ 6.2,  8. ,  4. ,  2. ,  0. ],\n",
       "          [ 5.2,  8. ,  3. ,  2. ,  0. ],\n",
       "          [ 4.2,  8. ,  2. ,  2. ,  0. ],\n",
       "          [ 4.1,  8. ,  2. ,  2. ,  0. ],\n",
       "          [ 4.2,  6. ,  2. ,  2. ,  0. ],\n",
       "          [ 1.1,  6. ,  2. ,  2. ,  0. ],\n",
       "          [ 1.2,  4. ,  1. ,  2. ,  0. ],\n",
       "          [ 7.2,  4. ,  0. ,  2. ,  0. ],\n",
       "          [ 7.1,  2. ,  0. ,  2. ,  0. ],\n",
       "          [ 5.2,  2. ,  0. ,  2. ,  0. ],\n",
       "          [ 5.1,  0. ,  0. ,  2. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  4. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  5. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  6. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  7. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  8. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  9. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. , 10. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. , 11. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. , 12. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. , 13. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. , 14. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. , 15. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. , 16. ]],\n",
       "  \n",
       "         [[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 1.2, 12. , 12. ,  1. ,  0. ],\n",
       "          [ 2.1, 12. , 11. ,  1. ,  0. ],\n",
       "          [ 3.1, 12. , 10. ,  1. ,  0. ],\n",
       "          [ 4.1, 12. ,  9. ,  1. ,  0. ],\n",
       "          [ 5.1, 12. ,  8. ,  1. ,  0. ],\n",
       "          [ 6.1, 12. ,  7. ,  1. ,  0. ],\n",
       "          [ 3.2, 12. ,  6. ,  1. ,  0. ],\n",
       "          [ 6.2, 12. ,  5. ,  1. ,  0. ],\n",
       "          [ 3.1, 12. ,  5. ,  2. ,  0. ],\n",
       "          [ 2.2, 12. ,  4. ,  2. ,  0. ],\n",
       "          [ 4.2, 12. ,  3. ,  2. ,  0. ],\n",
       "          [ 5.1, 12. ,  3. ,  2. ,  0. ],\n",
       "          [ 7.1, 12. ,  2. ,  2. ,  0. ],\n",
       "          [ 5.2, 12. ,  1. ,  2. ,  0. ],\n",
       "          [ 5.1, 10. ,  1. ,  3. ,  0. ],\n",
       "          [ 7.2, 10. ,  0. ,  3. ,  0. ],\n",
       "          [ 7.1,  8. ,  0. ,  3. ,  0. ],\n",
       "          [ 6.2,  8. ,  0. ,  3. ,  0. ],\n",
       "          [ 4.2,  8. ,  0. ,  3. ,  0. ],\n",
       "          [ 2.2,  8. ,  0. ,  3. ,  0. ],\n",
       "          [ 6.2,  8. ,  0. ,  3. ,  0. ],\n",
       "          [ 4.1,  8. ,  0. ,  3. ,  0. ],\n",
       "          [ 2.2,  8. ,  0. ,  3. ,  0. ],\n",
       "          [ 3.2,  8. ,  0. ,  3. ,  0. ],\n",
       "          [ 2.1,  8. ,  0. ,  3. ,  0. ],\n",
       "          [ 6.1,  8. ,  0. ,  3. ,  0. ],\n",
       "          [ 3.1,  8. ,  0. ,  3. ,  0. ],\n",
       "          [ 2.2,  8. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.1,  8. ,  0. ,  4. ,  0. ],\n",
       "          [ 6.2,  8. ,  0. ,  4. ,  0. ],\n",
       "          [ 6.1,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.2,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 2.1,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.1,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 2.1,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 3.1,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 2.2,  6. ,  0. ,  5. ,  0. ],\n",
       "          [ 3.2,  6. ,  0. ,  5. ,  0. ],\n",
       "          [ 4.1,  6. ,  0. ,  5. ,  0. ]]]),\n",
       "  array([[1., 0.],\n",
       "         [0., 1.]], dtype=float32)),\n",
       " (array([[[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 2.2, 14. , 10. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 10. ,  2. ,  0. ],\n",
       "          [ 2.2, 12. , 10. ,  2. ,  0. ],\n",
       "          [ 3.2, 12. ,  9. ,  2. ,  0. ],\n",
       "          [ 3.1, 10. ,  9. ,  2. ,  0. ],\n",
       "          [ 4.1, 10. ,  8. ,  2. ,  0. ],\n",
       "          [ 1.2, 10. ,  7. ,  2. ,  0. ],\n",
       "          [ 1.1, 10. ,  7. ,  2. ,  0. ],\n",
       "          [ 1.2,  8. ,  7. ,  2. ,  0. ],\n",
       "          [ 4.2,  8. ,  6. ,  2. ,  0. ],\n",
       "          [ 5.1,  8. ,  5. ,  2. ,  0. ],\n",
       "          [ 6.1,  8. ,  4. ,  2. ,  0. ],\n",
       "          [ 6.2,  6. ,  3. ,  2. ,  0. ],\n",
       "          [ 7.1,  6. ,  2. ,  2. ,  0. ],\n",
       "          [ 7.2,  4. ,  1. ,  2. ,  0. ],\n",
       "          [ 4.2,  4. ,  1. ,  2. ,  0. ],\n",
       "          [ 5.2,  4. ,  0. ,  2. ,  0. ],\n",
       "          [ 5.1,  4. ,  0. ,  2. ,  0. ],\n",
       "          [ 5.2,  2. ,  0. ,  2. ,  0. ],\n",
       "          [ 4.1,  2. ,  0. ,  2. ,  0. ],\n",
       "          [ 4.2,  0. ,  0. ,  3. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  4. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  5. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  6. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  7. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  8. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  9. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 10. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 11. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 12. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 13. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 14. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 15. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 16. ]],\n",
       "  \n",
       "         [[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 2.2, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 10. ,  1. ,  0. ],\n",
       "          [ 4.1, 14. ,  9. ,  1. ,  0. ],\n",
       "          [ 5.1, 14. ,  8. ,  1. ,  0. ],\n",
       "          [ 5.2, 14. ,  7. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. ,  7. ,  2. ,  0. ],\n",
       "          [ 1.2, 14. ,  6. ,  2. ,  0. ],\n",
       "          [ 2.2, 14. ,  6. ,  2. ,  0. ],\n",
       "          [ 2.2, 14. ,  6. ,  3. ,  0. ],\n",
       "          [ 6.1, 14. ,  5. ,  3. ,  0. ],\n",
       "          [ 4.2, 14. ,  4. ,  3. ,  0. ],\n",
       "          [ 2.2, 14. ,  4. ,  4. ,  0. ],\n",
       "          [ 6.1, 14. ,  4. ,  4. ,  0. ],\n",
       "          [ 4.2, 14. ,  4. ,  4. ,  0. ],\n",
       "          [ 1.2, 14. ,  4. ,  4. ,  0. ],\n",
       "          [ 6.1, 14. ,  4. ,  4. ,  0. ],\n",
       "          [ 6.2, 14. ,  3. ,  4. ,  0. ],\n",
       "          [ 3.1, 14. ,  3. ,  4. ,  0. ],\n",
       "          [ 5.2, 14. ,  3. ,  4. ,  0. ],\n",
       "          [ 2.1, 14. ,  3. ,  4. ,  0. ],\n",
       "          [ 7.1, 14. ,  2. ,  4. ,  0. ],\n",
       "          [ 5.2, 14. ,  2. ,  4. ,  0. ],\n",
       "          [ 3.1, 14. ,  2. ,  4. ,  0. ],\n",
       "          [ 7.1, 14. ,  2. ,  4. ,  0. ],\n",
       "          [ 7.2, 14. ,  1. ,  4. ,  0. ],\n",
       "          [ 6.1, 14. ,  1. ,  4. ,  0. ],\n",
       "          [ 7.2, 14. ,  1. ,  4. ,  0. ],\n",
       "          [ 3.1, 14. ,  1. ,  5. ,  0. ],\n",
       "          [ 4.1, 14. ,  1. ,  5. ,  0. ],\n",
       "          [ 6.1, 14. ,  1. ,  5. ,  0. ],\n",
       "          [ 6.2, 14. ,  1. ,  5. ,  0. ],\n",
       "          [ 2.2, 14. ,  1. ,  5. ,  0. ],\n",
       "          [ 7.1, 14. ,  1. ,  5. ,  0. ],\n",
       "          [ 3.2, 14. ,  0. ,  5. ,  0. ],\n",
       "          [ 7.1, 14. ,  0. ,  5. ,  0. ],\n",
       "          [ 1.2, 14. ,  0. ,  5. ,  0. ],\n",
       "          [ 1.1, 14. ,  0. ,  5. ,  0. ],\n",
       "          [ 6.1, 14. ,  0. ,  6. ,  0. ]]]),\n",
       "  array([[1., 0.],\n",
       "         [0., 1.]], dtype=float32)),\n",
       " (array([[[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  1. ,  0. ],\n",
       "          [ 5.1, 14. ,  9. ,  1. ,  0. ],\n",
       "          [ 3.2, 14. ,  8. ,  1. ,  0. ],\n",
       "          [ 1.1, 14. ,  8. ,  2. ,  0. ],\n",
       "          [ 6.1, 14. ,  7. ,  2. ,  0. ],\n",
       "          [ 4.1, 14. ,  7. ,  2. ,  0. ],\n",
       "          [ 6.2, 14. ,  6. ,  2. ,  0. ],\n",
       "          [ 3.2, 14. ,  6. ,  2. ,  0. ],\n",
       "          [ 7.1, 14. ,  5. ,  2. ,  0. ],\n",
       "          [ 7.2, 14. ,  4. ,  2. ,  0. ],\n",
       "          [ 7.1, 12. ,  4. ,  2. ,  0. ],\n",
       "          [ 5.1, 12. ,  4. ,  2. ,  0. ],\n",
       "          [ 1.2, 12. ,  3. ,  2. ,  0. ],\n",
       "          [ 3.2, 12. ,  3. ,  3. ,  0. ],\n",
       "          [ 5.2, 12. ,  2. ,  3. ,  0. ],\n",
       "          [ 3.1, 12. ,  2. ,  3. ,  0. ],\n",
       "          [ 3.2, 10. ,  2. ,  4. ,  0. ],\n",
       "          [ 5.1, 10. ,  2. ,  4. ,  0. ],\n",
       "          [ 5.2,  8. ,  2. ,  4. ,  0. ],\n",
       "          [ 4.2,  8. ,  1. ,  4. ,  0. ],\n",
       "          [ 6.1,  8. ,  1. ,  4. ,  0. ],\n",
       "          [ 6.2,  8. ,  1. ,  4. ,  0. ],\n",
       "          [ 6.1,  6. ,  1. ,  4. ,  0. ],\n",
       "          [ 2.2,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.1,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 1.1,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 2.1,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.1,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 1.2,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 2.2,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 2.1,  4. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.2,  4. ,  0. ,  4. ,  0. ],\n",
       "          [ 1.1,  4. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.1,  4. ,  0. ,  5. ,  0. ],\n",
       "          [ 4.2,  2. ,  0. ,  5. ,  0. ],\n",
       "          [ 1.1,  2. ,  0. ,  5. ,  0. ],\n",
       "          [ 1.2,  0. ,  0. ,  5. ,  0. ]],\n",
       "  \n",
       "         [[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  1. ,  0. ],\n",
       "          [ 1.1, 14. , 10. ,  2. ,  0. ],\n",
       "          [ 4.2, 14. ,  9. ,  2. ,  0. ],\n",
       "          [ 3.2, 14. ,  8. ,  2. ,  0. ],\n",
       "          [ 5.1, 14. ,  7. ,  2. ,  0. ],\n",
       "          [ 3.1, 14. ,  7. ,  2. ,  0. ],\n",
       "          [ 5.1, 14. ,  7. ,  2. ,  0. ],\n",
       "          [ 4.1, 14. ,  7. ,  2. ,  0. ],\n",
       "          [ 6.1, 14. ,  6. ,  2. ,  0. ],\n",
       "          [ 5.2, 14. ,  5. ,  2. ,  0. ],\n",
       "          [ 6.2, 14. ,  4. ,  2. ,  0. ],\n",
       "          [ 2.1, 14. ,  4. ,  2. ,  0. ],\n",
       "          [ 3.2, 14. ,  4. ,  2. ,  0. ],\n",
       "          [ 4.2, 14. ,  4. ,  2. ,  0. ],\n",
       "          [ 1.2, 14. ,  3. ,  2. ,  0. ],\n",
       "          [ 1.2, 14. ,  3. ,  2. ,  0. ],\n",
       "          [ 5.2, 14. ,  3. ,  2. ,  0. ],\n",
       "          [ 7.1, 14. ,  2. ,  2. ,  0. ],\n",
       "          [ 7.2, 12. ,  1. ,  2. ,  0. ],\n",
       "          [ 3.2, 12. ,  1. ,  3. ,  0. ],\n",
       "          [ 1.1, 12. ,  1. ,  3. ,  0. ],\n",
       "          [ 2.1, 12. ,  1. ,  3. ,  0. ],\n",
       "          [ 3.2, 12. ,  1. ,  4. ,  0. ],\n",
       "          [ 3.1, 12. ,  1. ,  4. ,  0. ],\n",
       "          [ 3.2, 10. ,  1. ,  5. ,  0. ],\n",
       "          [ 6.2, 10. ,  1. ,  5. ,  0. ],\n",
       "          [ 1.2, 10. ,  1. ,  5. ,  0. ],\n",
       "          [ 2.2, 10. ,  0. ,  5. ,  0. ],\n",
       "          [ 5.2, 10. ,  0. ,  5. ,  0. ],\n",
       "          [ 6.2, 10. ,  0. ,  5. ,  0. ],\n",
       "          [ 5.2, 10. ,  0. ,  5. ,  0. ],\n",
       "          [ 6.1, 10. ,  0. ,  5. ,  0. ],\n",
       "          [ 5.1, 10. ,  0. ,  5. ,  0. ],\n",
       "          [ 4.1, 10. ,  0. ,  5. ,  0. ],\n",
       "          [ 6.1, 10. ,  0. ,  5. ,  0. ],\n",
       "          [ 2.1, 10. ,  0. ,  5. ,  0. ],\n",
       "          [ 2.2,  8. ,  0. ,  5. ,  0. ]]]),\n",
       "  array([[1., 0.],\n",
       "         [0., 1.]], dtype=float32)),\n",
       " (array([[[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  1. ,  0. ],\n",
       "          [ 3.2, 14. ,  9. ,  1. ,  0. ],\n",
       "          [ 3.1, 12. ,  9. ,  2. ,  0. ],\n",
       "          [ 1.2, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 1.1, 10. ,  8. ,  2. ,  0. ],\n",
       "          [ 5.1, 10. ,  7. ,  2. ,  0. ],\n",
       "          [ 2.2, 10. ,  6. ,  2. ,  0. ],\n",
       "          [ 2.1, 10. ,  6. ,  2. ,  0. ],\n",
       "          [ 2.2,  8. ,  6. ,  2. ,  0. ],\n",
       "          [ 5.2,  8. ,  5. ,  2. ,  0. ],\n",
       "          [ 5.1,  6. ,  5. ,  2. ,  0. ],\n",
       "          [ 4.2,  6. ,  4. ,  2. ,  0. ],\n",
       "          [ 4.1,  4. ,  4. ,  2. ,  0. ],\n",
       "          [ 6.1,  4. ,  3. ,  2. ,  0. ],\n",
       "          [ 7.1,  4. ,  2. ,  2. ,  0. ],\n",
       "          [ 6.2,  4. ,  1. ,  2. ,  0. ],\n",
       "          [ 6.1,  2. ,  1. ,  2. ,  0. ],\n",
       "          [ 7.2,  2. ,  0. ,  2. ,  0. ],\n",
       "          [ 7.1,  0. ,  0. ,  2. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  4. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  5. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  6. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  7. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  8. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  9. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. , 10. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. , 11. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. , 12. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. , 13. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. , 14. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. , 15. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. , 16. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. , 17. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. , 18. ]],\n",
       "  \n",
       "         [[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 1.2, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 1.1, 12. , 11. ,  2. ,  0. ],\n",
       "          [ 3.1, 12. , 10. ,  2. ,  0. ],\n",
       "          [ 2.2, 12. ,  9. ,  2. ,  0. ],\n",
       "          [ 2.1, 12. ,  9. ,  2. ,  0. ],\n",
       "          [ 2.2, 10. ,  9. ,  2. ,  0. ],\n",
       "          [ 4.1, 10. ,  8. ,  2. ,  0. ],\n",
       "          [ 3.2, 10. ,  7. ,  2. ,  0. ],\n",
       "          [ 3.1, 10. ,  7. ,  2. ,  0. ],\n",
       "          [ 3.2,  8. ,  7. ,  2. ,  0. ],\n",
       "          [ 4.2,  8. ,  6. ,  2. ,  0. ],\n",
       "          [ 4.1,  6. ,  6. ,  2. ,  0. ],\n",
       "          [ 5.1,  6. ,  5. ,  2. ,  0. ],\n",
       "          [ 6.1,  6. ,  4. ,  2. ,  0. ],\n",
       "          [ 7.1,  6. ,  3. ,  2. ,  0. ],\n",
       "          [ 6.2,  6. ,  2. ,  2. ,  0. ],\n",
       "          [ 6.1,  6. ,  2. ,  2. ,  0. ],\n",
       "          [ 6.2,  4. ,  2. ,  2. ,  0. ],\n",
       "          [ 5.2,  4. ,  1. ,  2. ,  0. ],\n",
       "          [ 5.1,  2. ,  1. ,  2. ,  0. ],\n",
       "          [ 7.2,  2. ,  0. ,  2. ,  0. ],\n",
       "          [ 7.1,  0. ,  0. ,  2. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  4. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  5. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  6. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  7. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  8. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. ,  9. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. , 10. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. , 11. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. , 12. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. , 13. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. , 14. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. , 15. ],\n",
       "          [ 0. ,  0. ,  0. ,  2. , 16. ]]]),\n",
       "  array([[1., 0.],\n",
       "         [0., 1.]], dtype=float32)),\n",
       " (array([[[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 1.2, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 1.1, 12. , 11. ,  2. ,  0. ],\n",
       "          [ 2.1, 12. , 11. ,  2. ,  0. ],\n",
       "          [ 3.1, 12. , 10. ,  2. ,  0. ],\n",
       "          [ 4.1, 12. ,  9. ,  2. ,  0. ],\n",
       "          [ 3.2, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 3.1, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 3.2, 10. ,  8. ,  2. ,  0. ],\n",
       "          [ 5.1, 10. ,  7. ,  2. ,  0. ],\n",
       "          [ 4.2, 10. ,  6. ,  2. ,  0. ],\n",
       "          [ 4.2, 10. ,  6. ,  2. ,  0. ],\n",
       "          [ 4.1,  8. ,  6. ,  2. ,  0. ],\n",
       "          [ 6.1,  8. ,  5. ,  2. ,  0. ],\n",
       "          [ 7.1,  8. ,  4. ,  2. ,  0. ],\n",
       "          [ 5.2,  8. ,  3. ,  2. ,  0. ],\n",
       "          [ 5.1,  6. ,  3. ,  2. ,  0. ],\n",
       "          [ 2.1,  6. ,  3. ,  3. ,  0. ],\n",
       "          [ 6.2,  6. ,  2. ,  3. ,  0. ],\n",
       "          [ 6.2,  6. ,  2. ,  3. ,  0. ],\n",
       "          [ 7.1,  6. ,  2. ,  3. ,  0. ],\n",
       "          [ 2.2,  6. ,  1. ,  3. ,  0. ],\n",
       "          [ 7.2,  6. ,  0. ,  3. ,  0. ],\n",
       "          [ 7.1,  6. ,  0. ,  3. ,  0. ],\n",
       "          [ 7.2,  4. ,  0. ,  3. ,  0. ],\n",
       "          [ 6.1,  4. ,  0. ,  3. ,  0. ],\n",
       "          [ 6.2,  2. ,  0. ,  3. ,  0. ],\n",
       "          [ 2.1,  2. ,  0. ,  4. ,  0. ],\n",
       "          [ 2.2,  0. ,  0. ,  4. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  4. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  5. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  6. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  7. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  8. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  9. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. , 10. ]],\n",
       "  \n",
       "         [[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 1.1, 14. , 11. ,  2. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  2. ,  0. ],\n",
       "          [ 1.1, 14. , 10. ,  3. ,  0. ],\n",
       "          [ 5.1, 14. ,  9. ,  3. ,  0. ],\n",
       "          [ 2.1, 14. ,  9. ,  3. ,  0. ],\n",
       "          [ 3.2, 14. ,  8. ,  3. ,  0. ],\n",
       "          [ 1.1, 14. ,  8. ,  4. ,  0. ],\n",
       "          [ 3.1, 14. ,  8. ,  4. ,  0. ],\n",
       "          [ 3.2, 12. ,  8. ,  4. ,  0. ],\n",
       "          [ 4.2, 12. ,  7. ,  4. ,  0. ],\n",
       "          [ 1.1, 12. ,  7. ,  5. ,  0. ],\n",
       "          [ 2.2, 12. ,  6. ,  5. ,  0. ],\n",
       "          [ 2.1, 10. ,  6. ,  5. ,  0. ],\n",
       "          [ 5.1, 10. ,  6. ,  5. ,  0. ],\n",
       "          [ 5.2,  8. ,  5. ,  5. ,  0. ],\n",
       "          [ 6.1,  8. ,  4. ,  5. ,  0. ],\n",
       "          [ 1.1,  8. ,  4. ,  6. ,  0. ],\n",
       "          [ 7.1,  8. ,  3. ,  6. ,  0. ],\n",
       "          [ 6.1,  8. ,  3. ,  6. ,  0. ],\n",
       "          [ 4.1,  8. ,  3. ,  6. ,  0. ],\n",
       "          [ 7.1,  8. ,  3. ,  6. ,  0. ],\n",
       "          [ 6.2,  8. ,  2. ,  6. ,  0. ],\n",
       "          [ 1.1,  8. ,  2. ,  7. ,  0. ],\n",
       "          [ 1.2,  8. ,  1. ,  7. ,  0. ],\n",
       "          [ 1.1,  6. ,  1. ,  8. ,  0. ],\n",
       "          [ 4.2,  6. ,  1. ,  8. ,  0. ],\n",
       "          [ 6.1,  6. ,  1. ,  8. ,  0. ],\n",
       "          [ 7.2,  6. ,  0. ,  8. ,  0. ],\n",
       "          [ 4.2,  6. ,  0. ,  8. ,  0. ],\n",
       "          [ 6.1,  6. ,  0. ,  8. ,  0. ],\n",
       "          [ 7.1,  6. ,  0. ,  8. ,  0. ],\n",
       "          [ 6.1,  6. ,  0. ,  8. ,  0. ],\n",
       "          [ 4.1,  6. ,  0. ,  8. ,  0. ],\n",
       "          [ 6.2,  6. ,  0. ,  8. ,  0. ],\n",
       "          [ 7.1,  6. ,  0. ,  8. ,  0. ],\n",
       "          [ 6.2,  6. ,  0. ,  8. ,  0. ],\n",
       "          [ 4.2,  6. ,  0. ,  8. ,  0. ]]]),\n",
       "  array([[1., 0.],\n",
       "         [0., 1.]], dtype=float32)),\n",
       " (array([[[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 10. ,  2. ,  0. ],\n",
       "          [ 1.2, 14. ,  9. ,  2. ,  0. ],\n",
       "          [ 5.1, 14. ,  8. ,  2. ,  0. ],\n",
       "          [ 3.1, 14. ,  8. ,  2. ,  0. ],\n",
       "          [ 3.2, 14. ,  7. ,  2. ,  0. ],\n",
       "          [ 3.1, 12. ,  7. ,  3. ,  0. ],\n",
       "          [ 6.1, 12. ,  6. ,  3. ,  0. ],\n",
       "          [ 7.1, 12. ,  5. ,  3. ,  0. ],\n",
       "          [ 1.1, 12. ,  5. ,  3. ,  0. ],\n",
       "          [ 4.1, 12. ,  5. ,  3. ,  0. ],\n",
       "          [ 4.2, 12. ,  4. ,  3. ,  0. ],\n",
       "          [ 4.1, 10. ,  4. ,  3. ,  0. ],\n",
       "          [ 1.2, 10. ,  4. ,  3. ,  0. ],\n",
       "          [ 6.1, 10. ,  4. ,  3. ,  0. ],\n",
       "          [ 7.2, 10. ,  3. ,  3. ,  0. ],\n",
       "          [ 1.1, 10. ,  3. ,  3. ,  0. ],\n",
       "          [ 5.1, 10. ,  3. ,  3. ,  0. ],\n",
       "          [ 2.1, 10. ,  3. ,  3. ,  0. ],\n",
       "          [ 2.2, 10. ,  2. ,  3. ,  0. ],\n",
       "          [ 2.1,  8. ,  2. ,  4. ,  0. ],\n",
       "          [ 5.2,  8. ,  1. ,  4. ,  0. ],\n",
       "          [ 5.1,  6. ,  1. ,  4. ,  0. ],\n",
       "          [ 1.2,  6. ,  1. ,  4. ,  0. ],\n",
       "          [ 6.1,  6. ,  1. ,  4. ,  0. ],\n",
       "          [ 6.2,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 6.1,  4. ,  0. ,  4. ,  0. ],\n",
       "          [ 7.2,  4. ,  0. ,  4. ,  0. ],\n",
       "          [ 7.1,  2. ,  0. ,  4. ,  0. ],\n",
       "          [ 1.2,  2. ,  0. ,  4. ,  0. ],\n",
       "          [ 1.1,  0. ,  0. ,  4. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  4. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  5. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  6. ]],\n",
       "  \n",
       "         [[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 2.2, 14. , 10. ,  1. ,  0. ],\n",
       "          [ 4.1, 14. ,  9. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. ,  9. ,  2. ,  0. ],\n",
       "          [ 3.1, 14. ,  9. ,  3. ,  0. ],\n",
       "          [ 4.2, 14. ,  8. ,  3. ,  0. ],\n",
       "          [ 5.1, 14. ,  7. ,  3. ,  0. ],\n",
       "          [ 6.1, 14. ,  6. ,  3. ,  0. ],\n",
       "          [ 2.2, 14. ,  6. ,  3. ,  0. ],\n",
       "          [ 3.2, 14. ,  5. ,  3. ,  0. ],\n",
       "          [ 4.1, 14. ,  5. ,  3. ,  0. ],\n",
       "          [ 5.1, 14. ,  5. ,  3. ,  0. ],\n",
       "          [ 5.2, 14. ,  4. ,  3. ,  0. ],\n",
       "          [ 1.2, 14. ,  3. ,  3. ,  0. ],\n",
       "          [ 7.1, 14. ,  2. ,  3. ,  0. ],\n",
       "          [ 4.1, 14. ,  2. ,  3. ,  0. ],\n",
       "          [ 3.2, 14. ,  2. ,  3. ,  0. ],\n",
       "          [ 3.1, 12. ,  2. ,  4. ,  0. ],\n",
       "          [ 7.2, 12. ,  1. ,  4. ,  0. ],\n",
       "          [ 5.1, 12. ,  1. ,  4. ,  0. ],\n",
       "          [ 1.1, 12. ,  1. ,  4. ,  0. ],\n",
       "          [ 6.2, 12. ,  0. ,  4. ,  0. ],\n",
       "          [ 5.1, 12. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.2, 12. ,  0. ,  4. ,  0. ],\n",
       "          [ 7.2, 12. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.2, 12. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.1, 12. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.2, 10. ,  0. ,  4. ,  0. ],\n",
       "          [ 6.2, 10. ,  0. ,  4. ,  0. ],\n",
       "          [ 7.1, 10. ,  0. ,  4. ,  0. ],\n",
       "          [ 1.2, 10. ,  0. ,  4. ,  0. ],\n",
       "          [ 2.1, 10. ,  0. ,  4. ,  0. ],\n",
       "          [ 7.2, 10. ,  0. ,  4. ,  0. ],\n",
       "          [ 1.1, 10. ,  0. ,  4. ,  0. ],\n",
       "          [ 1.2, 10. ,  0. ,  4. ,  0. ],\n",
       "          [ 6.2, 10. ,  0. ,  4. ,  0. ],\n",
       "          [ 2.2, 10. ,  0. ,  4. ,  0. ],\n",
       "          [ 6.1, 10. ,  0. ,  4. ,  0. ]]]),\n",
       "  array([[1., 0.],\n",
       "         [0., 1.]], dtype=float32)),\n",
       " (array([[[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 2.2, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 2.1, 12. , 11. ,  2. ,  0. ],\n",
       "          [ 3.1, 12. , 10. ,  2. ,  0. ],\n",
       "          [ 4.1, 12. ,  9. ,  2. ,  0. ],\n",
       "          [ 5.1, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 1.1, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 6.1, 12. ,  7. ,  2. ,  0. ],\n",
       "          [ 3.2, 12. ,  6. ,  2. ,  0. ],\n",
       "          [ 3.1, 12. ,  6. ,  2. ,  0. ],\n",
       "          [ 3.2, 10. ,  6. ,  2. ,  0. ],\n",
       "          [ 6.2, 10. ,  5. ,  2. ,  0. ],\n",
       "          [ 6.1,  8. ,  5. ,  2. ,  0. ],\n",
       "          [ 4.2,  8. ,  4. ,  2. ,  0. ],\n",
       "          [ 5.1,  8. ,  4. ,  2. ,  0. ],\n",
       "          [ 5.2,  8. ,  3. ,  2. ,  0. ],\n",
       "          [ 5.1,  6. ,  3. ,  3. ,  0. ],\n",
       "          [ 7.1,  6. ,  2. ,  3. ,  0. ],\n",
       "          [ 4.1,  6. ,  2. ,  3. ,  0. ],\n",
       "          [ 4.2,  6. ,  2. ,  3. ,  0. ],\n",
       "          [ 4.1,  4. ,  2. ,  3. ,  0. ],\n",
       "          [ 1.2,  4. ,  1. ,  3. ,  0. ],\n",
       "          [ 7.2,  4. ,  0. ,  3. ,  0. ],\n",
       "          [ 1.1,  4. ,  0. ,  3. ,  0. ],\n",
       "          [ 1.2,  2. ,  0. ,  3. ,  0. ],\n",
       "          [ 7.1,  2. ,  0. ,  3. ,  0. ],\n",
       "          [ 7.2,  0. ,  0. ,  3. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  4. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  5. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  6. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  7. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  8. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  9. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 10. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 11. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 12. ]],\n",
       "  \n",
       "         [[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 11. ,  2. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  2. ,  0. ],\n",
       "          [ 1.1, 14. , 10. ,  2. ,  0. ],\n",
       "          [ 2.2, 14. ,  9. ,  2. ,  0. ],\n",
       "          [ 3.1, 14. ,  9. ,  2. ,  0. ],\n",
       "          [ 1.2, 14. ,  8. ,  2. ,  0. ],\n",
       "          [ 2.2, 14. ,  8. ,  2. ,  0. ],\n",
       "          [ 3.2, 14. ,  7. ,  2. ,  0. ],\n",
       "          [ 4.1, 14. ,  7. ,  2. ,  0. ],\n",
       "          [ 5.1, 14. ,  6. ,  2. ,  0. ],\n",
       "          [ 4.1, 14. ,  6. ,  3. ,  0. ],\n",
       "          [ 3.2, 14. ,  6. ,  3. ,  0. ],\n",
       "          [ 5.1, 14. ,  6. ,  3. ,  0. ],\n",
       "          [ 6.1, 14. ,  5. ,  3. ,  0. ],\n",
       "          [ 7.1, 14. ,  4. ,  3. ,  0. ],\n",
       "          [ 6.2, 14. ,  3. ,  3. ,  0. ],\n",
       "          [ 5.1, 14. ,  3. ,  3. ,  0. ],\n",
       "          [ 4.1, 14. ,  3. ,  4. ,  0. ],\n",
       "          [ 6.2, 14. ,  3. ,  4. ,  0. ],\n",
       "          [ 3.2, 14. ,  3. ,  4. ,  0. ],\n",
       "          [ 6.2, 14. ,  3. ,  4. ,  0. ],\n",
       "          [ 7.2, 14. ,  2. ,  4. ,  0. ],\n",
       "          [ 6.2, 14. ,  2. ,  4. ,  0. ],\n",
       "          [ 4.2, 14. ,  1. ,  4. ,  0. ],\n",
       "          [ 3.2, 14. ,  1. ,  4. ,  0. ],\n",
       "          [ 5.2, 14. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.2, 14. ,  0. ,  4. ,  0. ],\n",
       "          [ 1.2, 14. ,  0. ,  4. ,  0. ],\n",
       "          [ 2.2, 14. ,  0. ,  4. ,  0. ],\n",
       "          [ 7.1, 14. ,  0. ,  4. ,  0. ],\n",
       "          [ 6.2, 14. ,  0. ,  5. ,  0. ],\n",
       "          [ 7.2, 14. ,  0. ,  5. ,  0. ],\n",
       "          [ 6.1, 14. ,  0. ,  5. ,  0. ],\n",
       "          [ 2.1, 14. ,  0. ,  5. ,  0. ],\n",
       "          [ 1.1, 14. ,  0. ,  5. ,  0. ],\n",
       "          [ 3.1, 14. ,  0. ,  5. ,  0. ],\n",
       "          [ 4.1, 14. ,  0. ,  5. ,  0. ]]]),\n",
       "  array([[1., 0.],\n",
       "         [0., 1.]], dtype=float32)),\n",
       " (array([[[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 10. ,  2. ,  0. ],\n",
       "          [ 3.2, 14. ,  9. ,  2. ,  0. ],\n",
       "          [ 3.2, 14. ,  9. ,  2. ,  0. ],\n",
       "          [ 3.1, 12. ,  9. ,  2. ,  0. ],\n",
       "          [ 5.1, 12. ,  8. ,  2. ,  0. ],\n",
       "          [ 2.1, 12. ,  8. ,  3. ,  0. ],\n",
       "          [ 6.1, 12. ,  7. ,  3. ,  0. ],\n",
       "          [ 5.2, 12. ,  6. ,  3. ,  0. ],\n",
       "          [ 5.2, 12. ,  6. ,  3. ,  0. ],\n",
       "          [ 5.1, 10. ,  6. ,  3. ,  0. ],\n",
       "          [ 7.1, 10. ,  5. ,  3. ,  0. ],\n",
       "          [ 6.1, 10. ,  5. ,  3. ,  0. ],\n",
       "          [ 7.2, 10. ,  4. ,  3. ,  0. ],\n",
       "          [ 7.1,  8. ,  4. ,  3. ,  0. ],\n",
       "          [ 6.2,  8. ,  3. ,  3. ,  0. ],\n",
       "          [ 6.1,  6. ,  3. ,  3. ,  0. ],\n",
       "          [ 1.2,  6. ,  2. ,  3. ,  0. ],\n",
       "          [ 4.2,  6. ,  1. ,  3. ,  0. ],\n",
       "          [ 2.2,  6. ,  0. ,  3. ,  0. ],\n",
       "          [ 4.1,  6. ,  0. ,  3. ,  0. ],\n",
       "          [ 2.1,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.2,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 1.1,  6. ,  0. ,  4. ,  0. ],\n",
       "          [ 1.2,  4. ,  0. ,  4. ,  0. ],\n",
       "          [ 2.1,  4. ,  0. ,  5. ,  0. ],\n",
       "          [ 2.2,  2. ,  0. ,  5. ,  0. ],\n",
       "          [ 4.2,  2. ,  0. ,  5. ,  0. ],\n",
       "          [ 4.1,  0. ,  0. ,  5. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  5. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  5. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  5. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  5. ,  4. ],\n",
       "          [ 0. ,  0. ,  0. ,  5. ,  5. ],\n",
       "          [ 0. ,  0. ,  0. ,  5. ,  6. ],\n",
       "          [ 0. ,  0. ,  0. ,  5. ,  7. ],\n",
       "          [ 0. ,  0. ,  0. ,  5. ,  8. ]],\n",
       "  \n",
       "         [[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 1.2, 12. , 12. ,  1. ,  0. ],\n",
       "          [ 2.1, 12. , 11. ,  1. ,  0. ],\n",
       "          [ 3.1, 12. , 10. ,  1. ,  0. ],\n",
       "          [ 4.1, 12. ,  9. ,  1. ,  0. ],\n",
       "          [ 5.1, 12. ,  8. ,  1. ,  0. ],\n",
       "          [ 6.1, 12. ,  7. ,  1. ,  0. ],\n",
       "          [ 6.2, 10. ,  6. ,  1. ,  0. ],\n",
       "          [ 2.2, 10. ,  5. ,  1. ,  0. ],\n",
       "          [ 5.2, 10. ,  4. ,  1. ,  0. ],\n",
       "          [ 4.1, 10. ,  4. ,  2. ,  0. ],\n",
       "          [ 3.2, 10. ,  3. ,  2. ,  0. ],\n",
       "          [ 7.1, 10. ,  2. ,  2. ,  0. ],\n",
       "          [ 7.2,  8. ,  1. ,  2. ,  0. ],\n",
       "          [ 4.2,  8. ,  0. ,  2. ,  0. ],\n",
       "          [ 4.1,  6. ,  0. ,  3. ,  0. ],\n",
       "          [ 3.2,  6. ,  0. ,  3. ,  0. ],\n",
       "          [ 5.1,  6. ,  0. ,  3. ,  0. ],\n",
       "          [ 5.2,  6. ,  0. ,  3. ,  0. ],\n",
       "          [ 5.1,  4. ,  0. ,  3. ,  0. ],\n",
       "          [ 2.2,  4. ,  0. ,  3. ,  0. ],\n",
       "          [ 2.1,  2. ,  0. ,  3. ,  0. ],\n",
       "          [ 3.1,  2. ,  0. ,  3. ,  0. ],\n",
       "          [ 3.2,  0. ,  0. ,  3. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  4. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  5. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  6. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  7. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  8. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. ,  9. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 10. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 11. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 12. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 13. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 14. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 15. ],\n",
       "          [ 0. ,  0. ,  0. ,  3. , 16. ]]]),\n",
       "  array([[1., 0.],\n",
       "         [0., 1.]], dtype=float32)),\n",
       " (array([[[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  1. ,  0. ],\n",
       "          [ 5.1, 14. ,  9. ,  1. ,  0. ],\n",
       "          [ 6.1, 14. ,  8. ,  1. ,  0. ],\n",
       "          [ 7.1, 14. ,  7. ,  1. ,  0. ],\n",
       "          [ 4.2, 14. ,  6. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. ,  6. ,  2. ,  0. ],\n",
       "          [ 4.1, 14. ,  6. ,  2. ,  0. ],\n",
       "          [ 4.1, 14. ,  6. ,  3. ,  0. ],\n",
       "          [ 4.2, 12. ,  6. ,  3. ,  0. ],\n",
       "          [ 2.2, 12. ,  5. ,  3. ,  0. ],\n",
       "          [ 2.1, 10. ,  5. ,  3. ,  0. ],\n",
       "          [ 1.2, 10. ,  4. ,  3. ,  0. ],\n",
       "          [ 1.1,  8. ,  4. ,  3. ,  0. ],\n",
       "          [ 5.2,  8. ,  3. ,  3. ,  0. ],\n",
       "          [ 5.1,  6. ,  3. ,  3. ,  0. ],\n",
       "          [ 3.1,  6. ,  3. ,  3. ,  0. ],\n",
       "          [ 7.2,  6. ,  2. ,  3. ,  0. ],\n",
       "          [ 7.1,  6. ,  2. ,  3. ,  0. ],\n",
       "          [ 7.2,  4. ,  2. ,  3. ,  0. ],\n",
       "          [ 3.2,  4. ,  1. ,  3. ,  0. ],\n",
       "          [ 3.1,  2. ,  1. ,  4. ,  0. ],\n",
       "          [ 6.2,  2. ,  0. ,  4. ,  0. ],\n",
       "          [ 6.1,  0. ,  0. ,  4. ,  0. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  1. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  2. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  3. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  4. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  5. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  6. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  7. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  8. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. ,  9. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. , 10. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. , 11. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. , 12. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. , 13. ],\n",
       "          [ 0. ,  0. ,  0. ,  4. , 14. ]],\n",
       "  \n",
       "         [[ 1.1, 14. , 13. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 12. ,  1. ,  0. ],\n",
       "          [ 3.1, 14. , 11. ,  1. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  1. ,  0. ],\n",
       "          [ 2.1, 14. , 10. ,  2. ,  0. ],\n",
       "          [ 4.1, 14. , 10. ,  2. ,  0. ],\n",
       "          [ 5.1, 14. ,  9. ,  2. ,  0. ],\n",
       "          [ 4.1, 14. ,  9. ,  3. ,  0. ],\n",
       "          [ 6.1, 14. ,  8. ,  3. ,  0. ],\n",
       "          [ 4.2, 14. ,  7. ,  3. ,  0. ],\n",
       "          [ 2.1, 14. ,  7. ,  3. ,  0. ],\n",
       "          [ 4.2, 14. ,  7. ,  3. ,  0. ],\n",
       "          [ 3.2, 14. ,  6. ,  3. ,  0. ],\n",
       "          [ 6.2, 14. ,  5. ,  3. ,  0. ],\n",
       "          [ 7.1, 14. ,  4. ,  3. ,  0. ],\n",
       "          [ 5.1, 14. ,  4. ,  3. ,  0. ],\n",
       "          [ 4.1, 14. ,  4. ,  4. ,  0. ],\n",
       "          [ 7.1, 14. ,  4. ,  4. ,  0. ],\n",
       "          [ 5.2, 14. ,  3. ,  4. ,  0. ],\n",
       "          [ 7.1, 14. ,  3. ,  4. ,  0. ],\n",
       "          [ 2.2, 14. ,  2. ,  4. ,  0. ],\n",
       "          [ 1.2, 14. ,  1. ,  4. ,  0. ],\n",
       "          [ 1.1, 14. ,  1. ,  4. ,  0. ],\n",
       "          [ 2.2, 14. ,  1. ,  4. ,  0. ],\n",
       "          [ 7.2, 14. ,  0. ,  4. ,  0. ],\n",
       "          [ 7.1, 12. ,  0. ,  4. ,  0. ],\n",
       "          [ 6.1, 12. ,  0. ,  4. ,  0. ],\n",
       "          [ 1.1, 12. ,  0. ,  4. ,  0. ],\n",
       "          [ 2.2, 12. ,  0. ,  4. ,  0. ],\n",
       "          [ 6.1, 12. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.2, 12. ,  0. ,  4. ,  0. ],\n",
       "          [ 2.1, 12. ,  0. ,  4. ,  0. ],\n",
       "          [ 4.1, 12. ,  0. ,  5. ,  0. ],\n",
       "          [ 4.2, 10. ,  0. ,  5. ,  0. ],\n",
       "          [ 3.2, 10. ,  0. ,  5. ,  0. ],\n",
       "          [ 3.1,  8. ,  0. ,  5. ,  0. ],\n",
       "          [ 2.1,  8. ,  0. ,  5. ,  0. ],\n",
       "          [ 6.2,  8. ,  0. ,  5. ,  0. ],\n",
       "          [ 5.2,  8. ,  0. ,  5. ,  0. ],\n",
       "          [ 2.2,  8. ,  0. ,  5. ,  0. ]]]),\n",
       "  array([[1., 0.],\n",
       "         [0., 1.]], dtype=float32))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_data_splits_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_participants_per_split = 19 # 20 but one is removed in each split for testing\n",
    "simulations_per_participant = 1000\n",
    "n_added_simulations_per_participant = 0\n",
    "n_runs = 1\n",
    "n_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def add_simulated_data(X_train, y_train, simulated_train_set):\n",
    "    for n in range(n_added_simulations_per_participant):\n",
    "        for i in range(n_participants_per_split):\n",
    "\n",
    "            X_train_simulated_1 = simulated_train_set[0][(i * simulations_per_participant) + n]\n",
    "            y_train_simulated_1 = simulated_train_set[1][(i * simulations_per_participant) + n]\n",
    "            X_train_simulated_2 = simulated_train_set[0][(simulations_per_participant * n_participants_per_split) \\\n",
    "                                                         + (i * simulations_per_participant) + n]\n",
    "            y_train_simulated_2 = simulated_train_set[1][(simulations_per_participant * n_participants_per_split) \\\n",
    "                                                         + (i * simulations_per_participant) + n]\n",
    "            \n",
    "            X_train_simulated = np.concatenate((X_train_simulated_1[np.newaxis, :, :], \\\n",
    "                                               X_train_simulated_2[np.newaxis, :, :]), axis=0)\n",
    "            y_train_simulated = np.concatenate((y_train_simulated_1[np.newaxis, :], \\\n",
    "                                               y_train_simulated_2[np.newaxis, :]), axis=0)\n",
    "\n",
    "            X_train = np.concatenate((X_train, X_train_simulated), axis=0)\n",
    "            y_train = np.concatenate((y_train, y_train_simulated), axis=0)\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def mean_score_of_run(histories, epochs):\n",
    "    mean_val_losses = []\n",
    "    mean_val_accuracies = []\n",
    "    mean_losses = []\n",
    "    mean_accuracies = []\n",
    "    for i in range(epochs):\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        for l in range(len(histories)):\n",
    "            history = histories[l]\n",
    "            val_losses.append(history.history['val_loss'][i])\n",
    "            val_accuracies.append(history.history['val_accuracy'][i])\n",
    "            losses.append(history.history['loss'][i])\n",
    "            accuracies.append(history.history['accuracy'][i])\n",
    "        mean_val_losses.append(np.mean(val_losses))\n",
    "        mean_val_accuracies.append(np.mean(val_accuracies))\n",
    "        mean_losses.append(np.mean(losses))\n",
    "        mean_accuracies.append(np.mean(accuracies))\n",
    "    return mean_val_losses, mean_val_accuracies, mean_losses, mean_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def mean_score_over_all_runs(mean_run_scores, n_runs):\n",
    "    val_losses = np.asarray(mean_run_scores[0][0])\n",
    "    val_accuracies = np.asarray(mean_run_scores[0][1])\n",
    "    losses = np.asarray(mean_run_scores[0][2])\n",
    "    accuracies = np.asarray(mean_run_scores[0][3])\n",
    "                            \n",
    "    for i in range(1, n_runs):\n",
    "        val_losses += np.asarray(mean_run_scores[i][0])\n",
    "        val_accuracies += np.asarray(mean_run_scores[i][1])\n",
    "        losses += np.asarray(mean_run_scores[i][2])\n",
    "        accuracies += np.asarray(mean_run_scores[i][3])\n",
    "                                 \n",
    "    val_losses /= n_runs\n",
    "    val_accuracies /= n_runs\n",
    "    losses /= n_runs\n",
    "    accuracies /= n_runs\n",
    "    \n",
    "    return val_losses, val_accuracies, losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_and_train(X_train, y_train, X_test, y_test):\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "    cnn_input_shape = X_train[0].shape\n",
    "    \n",
    "    #epochs = n_epochs\n",
    "    cnn_batch_size = 32 #1000\n",
    "    #verbose = 0\n",
    "    \n",
    "    #print(cnn_input_shape)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, (5, 5), input_shape=(76, 40,1), activation='relu'))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train, y_train, epochs=n_epochs, batch_size=cnn_batch_size, verbose=1, \n",
    "                        shuffle=True, validation_data=(X_test, y_test))\n",
    "    \n",
    "    #print(history.history['val_accuracy'])\n",
    "    #print(model.summary())\n",
    "    \n",
    "    histories.append(history)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7349433dec53407b862ab211d51dcfae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Runs', max=1.0, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76, 40)\n",
      "(38, 2)\n",
      "Train on 38 samples, validate on 2 samples\n",
      "Epoch 1/1000\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.6973 - accuracy: 0.4737 - val_loss: 0.6942 - val_accuracy: 0.5000\n",
      "Epoch 2/1000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6942 - val_accuracy: 0.5000\n",
      "Epoch 3/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6955 - accuracy: 0.4737 - val_loss: 0.6942 - val_accuracy: 0.5000\n",
      "Epoch 4/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6912 - accuracy: 0.4737 - val_loss: 0.6941 - val_accuracy: 0.5000\n",
      "Epoch 5/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6869 - accuracy: 0.5263 - val_loss: 0.6941 - val_accuracy: 0.5000\n",
      "Epoch 6/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6987 - accuracy: 0.5263 - val_loss: 0.6941 - val_accuracy: 0.5000\n",
      "Epoch 7/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6932 - accuracy: 0.4737 - val_loss: 0.6941 - val_accuracy: 0.5000\n",
      "Epoch 8/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6994 - accuracy: 0.4211 - val_loss: 0.6941 - val_accuracy: 0.5000\n",
      "Epoch 9/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6976 - accuracy: 0.5526 - val_loss: 0.6941 - val_accuracy: 0.5000\n",
      "Epoch 10/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.7009 - accuracy: 0.4211 - val_loss: 0.6941 - val_accuracy: 0.5000\n",
      "Epoch 11/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6911 - accuracy: 0.5263 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 12/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6925 - accuracy: 0.5263 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 13/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6977 - accuracy: 0.5263 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 14/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6928 - accuracy: 0.5263 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 15/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 16/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6894 - accuracy: 0.5526 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 17/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6918 - accuracy: 0.5526 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 18/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6981 - accuracy: 0.4474 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 19/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6842 - accuracy: 0.6053 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 20/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6977 - accuracy: 0.4474 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 21/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6968 - accuracy: 0.3947 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 22/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6933 - accuracy: 0.4737 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 23/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6919 - accuracy: 0.5526 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 24/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6864 - accuracy: 0.5789 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 25/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6928 - accuracy: 0.5263 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 26/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6922 - accuracy: 0.5526 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 27/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6961 - accuracy: 0.5526 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 28/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6954 - accuracy: 0.5000 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 29/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5789 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 30/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6980 - accuracy: 0.4211 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 31/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6907 - accuracy: 0.5526 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 32/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6945 - accuracy: 0.5263 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 33/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6967 - accuracy: 0.5526 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 34/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6928 - accuracy: 0.5526 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 35/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6932 - accuracy: 0.4737 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 36/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6916 - accuracy: 0.5000 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 37/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6920 - accuracy: 0.5526 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 38/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6896 - accuracy: 0.5789 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 39/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6956 - accuracy: 0.5000 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 40/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6875 - accuracy: 0.6316 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 41/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6986 - accuracy: 0.4474 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 42/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6966 - accuracy: 0.4737 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 43/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6976 - accuracy: 0.4211 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 44/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.7028 - accuracy: 0.4211 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 45/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6935 - accuracy: 0.4737 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 46/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6929 - accuracy: 0.5263 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
      "Epoch 47/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6900 - accuracy: 0.5000 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 48/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6868 - accuracy: 0.6316 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 49/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6937 - accuracy: 0.5000 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 50/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6919 - accuracy: 0.5000 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 51/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6923 - accuracy: 0.5000 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 52/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6922 - accuracy: 0.5526 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 53/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6885 - accuracy: 0.5789 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 54/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6913 - accuracy: 0.5263 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 55/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6930 - accuracy: 0.5000 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 56/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6916 - accuracy: 0.5263 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 57/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.7032 - accuracy: 0.3947 - val_loss: 0.6937 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6972 - accuracy: 0.5526 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 59/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6973 - accuracy: 0.3947 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 60/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6986 - accuracy: 0.4737 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 61/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6903 - accuracy: 0.5526 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 62/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.7045 - accuracy: 0.3684 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 63/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6940 - accuracy: 0.5789 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 64/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.7003 - accuracy: 0.3947 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 65/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6880 - accuracy: 0.5526 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 66/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6959 - accuracy: 0.4211 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 67/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6986 - accuracy: 0.5263 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 68/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6979 - accuracy: 0.5000 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 69/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6883 - accuracy: 0.6053 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 70/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6845 - accuracy: 0.6053 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 71/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6943 - accuracy: 0.5526 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 72/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6936 - accuracy: 0.5263 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 73/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.7012 - accuracy: 0.4211 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 74/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6918 - accuracy: 0.5000 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 75/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6913 - accuracy: 0.5263 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 76/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6915 - accuracy: 0.5789 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 77/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6874 - accuracy: 0.5789 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 78/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.4474 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 79/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6922 - accuracy: 0.5263 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 80/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6930 - accuracy: 0.5526 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 81/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6891 - accuracy: 0.5526 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 82/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6856 - accuracy: 0.5789 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 83/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6938 - accuracy: 0.5263 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 84/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6950 - accuracy: 0.4474 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 85/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6894 - accuracy: 0.5526 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 86/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6871 - accuracy: 0.5526 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 87/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6929 - accuracy: 0.4737 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 88/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6958 - accuracy: 0.5000 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 89/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6896 - accuracy: 0.5526 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 90/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6915 - accuracy: 0.5000 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 91/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6933 - accuracy: 0.5526 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 92/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.6939 - accuracy: 0.5000 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 93/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6919 - accuracy: 0.4737 - val_loss: 0.6936 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6878 - accuracy: 0.5789 - val_loss: 0.6936 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6952 - accuracy: 0.5263 - val_loss: 0.6936 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.7027 - accuracy: 0.3947 - val_loss: 0.6936 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6920 - accuracy: 0.5000 - val_loss: 0.6936 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6900 - accuracy: 0.5526 - val_loss: 0.6935 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6987 - accuracy: 0.3684 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 100/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6934 - accuracy: 0.4737 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 101/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6974 - accuracy: 0.5000 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 102/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6925 - accuracy: 0.5263 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 103/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6910 - accuracy: 0.5789 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 104/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.4211 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 105/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6902 - accuracy: 0.6842 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 106/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.7012 - accuracy: 0.3158 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 107/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6846 - accuracy: 0.6316 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 108/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6957 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 109/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6923 - accuracy: 0.5263 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 110/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6893 - accuracy: 0.6053 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 111/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6840 - accuracy: 0.6579 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
      "Epoch 112/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6907 - accuracy: 0.6053 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
      "Epoch 113/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6880 - accuracy: 0.6316 - val_loss: 0.6929 - val_accuracy: 0.5000\n",
      "Epoch 114/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5000 - val_loss: 0.6929 - val_accuracy: 1.0000\n",
      "Epoch 115/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6838 - accuracy: 0.6316 - val_loss: 0.6928 - val_accuracy: 1.0000\n",
      "Epoch 116/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.5000 - val_loss: 0.6928 - val_accuracy: 1.0000\n",
      "Epoch 117/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6968 - accuracy: 0.3947 - val_loss: 0.6927 - val_accuracy: 1.0000\n",
      "Epoch 118/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6948 - accuracy: 0.4737 - val_loss: 0.6926 - val_accuracy: 1.0000\n",
      "Epoch 119/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6896 - accuracy: 0.5263 - val_loss: 0.6926 - val_accuracy: 0.5000\n",
      "Epoch 120/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6875 - accuracy: 0.5263 - val_loss: 0.6926 - val_accuracy: 0.5000\n",
      "Epoch 121/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6925 - val_accuracy: 0.5000\n",
      "Epoch 122/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6918 - accuracy: 0.5000 - val_loss: 0.6924 - val_accuracy: 0.5000\n",
      "Epoch 123/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6957 - accuracy: 0.5526 - val_loss: 0.6924 - val_accuracy: 0.5000\n",
      "Epoch 124/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6883 - accuracy: 0.6053 - val_loss: 0.6924 - val_accuracy: 0.5000\n",
      "Epoch 125/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6843 - accuracy: 0.5789 - val_loss: 0.6924 - val_accuracy: 0.5000\n",
      "Epoch 126/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6910 - accuracy: 0.5526 - val_loss: 0.6924 - val_accuracy: 0.5000\n",
      "Epoch 127/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6885 - accuracy: 0.6316 - val_loss: 0.6923 - val_accuracy: 0.5000\n",
      "Epoch 128/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6870 - accuracy: 0.6053 - val_loss: 0.6923 - val_accuracy: 0.5000\n",
      "Epoch 129/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6900 - accuracy: 0.5263 - val_loss: 0.6922 - val_accuracy: 0.5000\n",
      "Epoch 130/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6893 - accuracy: 0.6579 - val_loss: 0.6922 - val_accuracy: 0.5000\n",
      "Epoch 131/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6940 - accuracy: 0.4737 - val_loss: 0.6921 - val_accuracy: 0.5000\n",
      "Epoch 132/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6863 - accuracy: 0.5789 - val_loss: 0.6921 - val_accuracy: 0.5000\n",
      "Epoch 133/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5000 - val_loss: 0.6920 - val_accuracy: 0.5000\n",
      "Epoch 134/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6875 - accuracy: 0.6842 - val_loss: 0.6920 - val_accuracy: 0.5000\n",
      "Epoch 135/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6885 - accuracy: 0.6053 - val_loss: 0.6920 - val_accuracy: 0.5000\n",
      "Epoch 136/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6858 - accuracy: 0.6579 - val_loss: 0.6919 - val_accuracy: 0.5000\n",
      "Epoch 137/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6918 - accuracy: 0.4737 - val_loss: 0.6919 - val_accuracy: 0.5000\n",
      "Epoch 138/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6916 - accuracy: 0.5789 - val_loss: 0.6919 - val_accuracy: 0.5000\n",
      "Epoch 139/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6999 - accuracy: 0.4211 - val_loss: 0.6919 - val_accuracy: 0.5000\n",
      "Epoch 140/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6896 - accuracy: 0.5526 - val_loss: 0.6918 - val_accuracy: 0.5000\n",
      "Epoch 141/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6868 - accuracy: 0.5526 - val_loss: 0.6918 - val_accuracy: 0.5000\n",
      "Epoch 142/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6858 - accuracy: 0.5526 - val_loss: 0.6919 - val_accuracy: 0.5000\n",
      "Epoch 143/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6850 - accuracy: 0.6579 - val_loss: 0.6919 - val_accuracy: 0.5000\n",
      "Epoch 144/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6918 - accuracy: 0.6316 - val_loss: 0.6918 - val_accuracy: 0.5000\n",
      "Epoch 145/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6948 - accuracy: 0.5526 - val_loss: 0.6918 - val_accuracy: 0.5000\n",
      "Epoch 146/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6944 - accuracy: 0.4474 - val_loss: 0.6918 - val_accuracy: 0.5000\n",
      "Epoch 147/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6980 - accuracy: 0.4474 - val_loss: 0.6917 - val_accuracy: 0.5000\n",
      "Epoch 148/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6867 - accuracy: 0.5789 - val_loss: 0.6917 - val_accuracy: 0.5000\n",
      "Epoch 149/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6863 - accuracy: 0.5263 - val_loss: 0.6916 - val_accuracy: 0.5000\n",
      "Epoch 150/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6826 - accuracy: 0.6316 - val_loss: 0.6915 - val_accuracy: 0.5000\n",
      "Epoch 151/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6887 - accuracy: 0.5789 - val_loss: 0.6915 - val_accuracy: 0.5000\n",
      "Epoch 152/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6915 - accuracy: 0.5789 - val_loss: 0.6915 - val_accuracy: 0.5000\n",
      "Epoch 153/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6835 - accuracy: 0.6579 - val_loss: 0.6914 - val_accuracy: 0.5000\n",
      "Epoch 154/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6983 - accuracy: 0.3947 - val_loss: 0.6914 - val_accuracy: 0.5000\n",
      "Epoch 155/1000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.6837 - accuracy: 0.7105 - val_loss: 0.6914 - val_accuracy: 0.5000\n",
      "Epoch 156/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6973 - accuracy: 0.5000 - val_loss: 0.6914 - val_accuracy: 0.5000\n",
      "Epoch 157/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6894 - accuracy: 0.4737 - val_loss: 0.6914 - val_accuracy: 0.5000\n",
      "Epoch 158/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6923 - accuracy: 0.5789 - val_loss: 0.6914 - val_accuracy: 0.5000\n",
      "Epoch 159/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6980 - accuracy: 0.4211 - val_loss: 0.6913 - val_accuracy: 0.5000\n",
      "Epoch 160/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6961 - accuracy: 0.4211 - val_loss: 0.6913 - val_accuracy: 0.5000\n",
      "Epoch 161/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6923 - accuracy: 0.5000 - val_loss: 0.6913 - val_accuracy: 0.5000\n",
      "Epoch 162/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6913 - val_accuracy: 0.5000\n",
      "Epoch 163/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6930 - accuracy: 0.6053 - val_loss: 0.6913 - val_accuracy: 0.5000\n",
      "Epoch 164/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6938 - accuracy: 0.5263 - val_loss: 0.6913 - val_accuracy: 0.5000\n",
      "Epoch 165/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6842 - accuracy: 0.6316 - val_loss: 0.6913 - val_accuracy: 0.5000\n",
      "Epoch 166/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6883 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5000\n",
      "Epoch 167/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6916 - accuracy: 0.5789 - val_loss: 0.6912 - val_accuracy: 0.5000\n",
      "Epoch 168/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6940 - accuracy: 0.5263 - val_loss: 0.6912 - val_accuracy: 0.5000\n",
      "Epoch 169/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6937 - accuracy: 0.4211 - val_loss: 0.6912 - val_accuracy: 0.5000\n",
      "Epoch 170/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6933 - accuracy: 0.5526 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 171/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6872 - accuracy: 0.5526 - val_loss: 0.6911 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/1000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.6978 - accuracy: 0.4737 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 173/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6972 - accuracy: 0.4211 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 174/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6916 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 175/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6866 - accuracy: 0.5789 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 176/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 177/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6885 - accuracy: 0.6316 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 178/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6909 - accuracy: 0.5789 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 179/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6823 - accuracy: 0.6053 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 180/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6880 - accuracy: 0.5526 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 181/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6966 - accuracy: 0.4474 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 182/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6937 - accuracy: 0.5526 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 183/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6889 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 184/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6871 - accuracy: 0.5789 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 185/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6941 - accuracy: 0.5526 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 186/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6910 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 187/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6945 - accuracy: 0.5000 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 188/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6921 - accuracy: 0.5526 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 189/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6911 - accuracy: 0.5000 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 190/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6913 - accuracy: 0.5263 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 191/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6845 - accuracy: 0.6316 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 192/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6883 - accuracy: 0.6053 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 193/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6869 - accuracy: 0.6053 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 194/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6834 - accuracy: 0.7105 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 195/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6890 - accuracy: 0.5789 - val_loss: 0.6910 - val_accuracy: 0.5000\n",
      "Epoch 196/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6870 - accuracy: 0.5000 - val_loss: 0.6910 - val_accuracy: 0.5000\n",
      "Epoch 197/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6917 - accuracy: 0.5000 - val_loss: 0.6910 - val_accuracy: 0.5000\n",
      "Epoch 198/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6812 - accuracy: 0.7105 - val_loss: 0.6909 - val_accuracy: 0.5000\n",
      "Epoch 199/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6857 - accuracy: 0.6316 - val_loss: 0.6909 - val_accuracy: 0.5000\n",
      "Epoch 200/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6881 - accuracy: 0.5789 - val_loss: 0.6909 - val_accuracy: 0.5000\n",
      "Epoch 201/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6959 - accuracy: 0.5000 - val_loss: 0.6909 - val_accuracy: 0.5000\n",
      "Epoch 202/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6895 - accuracy: 0.6053 - val_loss: 0.6908 - val_accuracy: 0.5000\n",
      "Epoch 203/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6836 - accuracy: 0.5526 - val_loss: 0.6908 - val_accuracy: 0.5000\n",
      "Epoch 204/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6866 - accuracy: 0.5263 - val_loss: 0.6907 - val_accuracy: 0.5000\n",
      "Epoch 205/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6910 - accuracy: 0.5263 - val_loss: 0.6907 - val_accuracy: 0.5000\n",
      "Epoch 206/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6888 - accuracy: 0.6053 - val_loss: 0.6907 - val_accuracy: 0.5000\n",
      "Epoch 207/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6903 - accuracy: 0.5000 - val_loss: 0.6906 - val_accuracy: 0.5000\n",
      "Epoch 208/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6847 - accuracy: 0.6316 - val_loss: 0.6906 - val_accuracy: 0.5000\n",
      "Epoch 209/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6904 - accuracy: 0.6316 - val_loss: 0.6906 - val_accuracy: 0.5000\n",
      "Epoch 210/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6872 - accuracy: 0.6053 - val_loss: 0.6905 - val_accuracy: 0.5000\n",
      "Epoch 211/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6872 - accuracy: 0.6579 - val_loss: 0.6905 - val_accuracy: 0.5000\n",
      "Epoch 212/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6948 - accuracy: 0.5526 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 213/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6897 - accuracy: 0.5000 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 214/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6957 - accuracy: 0.5263 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 215/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6878 - accuracy: 0.6316 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 216/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6961 - accuracy: 0.4737 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 217/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6967 - accuracy: 0.4474 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 218/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6880 - accuracy: 0.5263 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 219/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6915 - accuracy: 0.6053 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 220/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6833 - accuracy: 0.6316 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 221/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6936 - accuracy: 0.4737 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 222/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6914 - accuracy: 0.5000 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 223/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6859 - accuracy: 0.5263 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 224/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6929 - accuracy: 0.5263 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 225/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6907 - accuracy: 0.5526 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 226/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6876 - accuracy: 0.5789 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 227/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6919 - accuracy: 0.5263 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 228/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6923 - accuracy: 0.5000 - val_loss: 0.6904 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6964 - accuracy: 0.4737 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 230/1000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.6870 - accuracy: 0.7105 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 231/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6858 - accuracy: 0.5000 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 232/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6956 - accuracy: 0.4474 - val_loss: 0.6905 - val_accuracy: 0.5000\n",
      "Epoch 233/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6868 - accuracy: 0.6579 - val_loss: 0.6905 - val_accuracy: 0.5000\n",
      "Epoch 234/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6893 - accuracy: 0.5526 - val_loss: 0.6905 - val_accuracy: 0.5000\n",
      "Epoch 235/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6902 - accuracy: 0.5789 - val_loss: 0.6905 - val_accuracy: 0.5000\n",
      "Epoch 236/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6862 - accuracy: 0.5526 - val_loss: 0.6905 - val_accuracy: 0.5000\n",
      "Epoch 237/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6818 - accuracy: 0.6579 - val_loss: 0.6905 - val_accuracy: 0.5000\n",
      "Epoch 238/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6939 - accuracy: 0.5000 - val_loss: 0.6905 - val_accuracy: 0.5000\n",
      "Epoch 239/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6880 - accuracy: 0.6053 - val_loss: 0.6906 - val_accuracy: 0.5000\n",
      "Epoch 240/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6829 - accuracy: 0.6579 - val_loss: 0.6905 - val_accuracy: 0.5000\n",
      "Epoch 241/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6906 - accuracy: 0.5263 - val_loss: 0.6905 - val_accuracy: 0.5000\n",
      "Epoch 242/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6872 - accuracy: 0.5000 - val_loss: 0.6905 - val_accuracy: 0.5000\n",
      "Epoch 243/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6860 - accuracy: 0.5526 - val_loss: 0.6905 - val_accuracy: 0.5000\n",
      "Epoch 244/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6913 - accuracy: 0.5526 - val_loss: 0.6905 - val_accuracy: 0.5000\n",
      "Epoch 245/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6827 - accuracy: 0.6579 - val_loss: 0.6905 - val_accuracy: 0.5000\n",
      "Epoch 246/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6847 - accuracy: 0.6579 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 247/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6915 - accuracy: 0.5000 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 248/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6749 - accuracy: 0.7632 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 249/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6973 - accuracy: 0.5000 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 250/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6808 - accuracy: 0.6842 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 251/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6906 - accuracy: 0.4737 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 252/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 253/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6854 - accuracy: 0.6053 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 254/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6925 - accuracy: 0.5263 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 255/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6902 - accuracy: 0.5526 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 256/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6793 - accuracy: 0.6053 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 257/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6885 - accuracy: 0.5526 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 258/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6811 - accuracy: 0.7368 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 259/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6802 - accuracy: 0.6579 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 260/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6872 - accuracy: 0.6316 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 261/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6865 - accuracy: 0.5526 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 262/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6792 - accuracy: 0.6842 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 263/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6778 - accuracy: 0.7105 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 264/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6914 - accuracy: 0.5789 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 265/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6816 - accuracy: 0.7105 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 266/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6880 - accuracy: 0.6842 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 267/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6873 - accuracy: 0.6316 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 268/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6837 - accuracy: 0.6316 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 269/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6920 - accuracy: 0.5263 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 270/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6854 - accuracy: 0.6053 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 271/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6874 - accuracy: 0.6053 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 272/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6837 - accuracy: 0.6579 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 273/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6826 - accuracy: 0.6316 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 274/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6847 - accuracy: 0.6053 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 275/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6790 - accuracy: 0.6316 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 276/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6820 - accuracy: 0.5526 - val_loss: 0.6905 - val_accuracy: 0.5000\n",
      "Epoch 277/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6844 - accuracy: 0.6053 - val_loss: 0.6905 - val_accuracy: 0.5000\n",
      "Epoch 278/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6877 - accuracy: 0.6316 - val_loss: 0.6905 - val_accuracy: 0.5000\n",
      "Epoch 279/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6880 - accuracy: 0.5526 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 280/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6892 - accuracy: 0.5526 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 281/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6769 - accuracy: 0.6579 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 282/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6723 - accuracy: 0.7368 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 283/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6896 - accuracy: 0.4737 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 284/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6784 - accuracy: 0.6316 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 285/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6895 - accuracy: 0.5526 - val_loss: 0.6903 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6817 - accuracy: 0.7368 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 287/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6860 - accuracy: 0.5789 - val_loss: 0.6902 - val_accuracy: 0.5000\n",
      "Epoch 288/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.7007 - accuracy: 0.3947 - val_loss: 0.6902 - val_accuracy: 0.5000\n",
      "Epoch 289/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6836 - accuracy: 0.7105 - val_loss: 0.6902 - val_accuracy: 0.5000\n",
      "Epoch 290/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6813 - accuracy: 0.6842 - val_loss: 0.6901 - val_accuracy: 0.5000\n",
      "Epoch 291/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6878 - accuracy: 0.6316 - val_loss: 0.6901 - val_accuracy: 0.5000\n",
      "Epoch 292/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6835 - accuracy: 0.5526 - val_loss: 0.6901 - val_accuracy: 0.5000\n",
      "Epoch 293/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6865 - accuracy: 0.6053 - val_loss: 0.6900 - val_accuracy: 0.5000\n",
      "Epoch 294/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6839 - accuracy: 0.6316 - val_loss: 0.6900 - val_accuracy: 0.5000\n",
      "Epoch 295/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6891 - accuracy: 0.5526 - val_loss: 0.6899 - val_accuracy: 0.5000\n",
      "Epoch 296/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6876 - accuracy: 0.5789 - val_loss: 0.6899 - val_accuracy: 0.5000\n",
      "Epoch 297/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6833 - accuracy: 0.6053 - val_loss: 0.6899 - val_accuracy: 0.5000\n",
      "Epoch 298/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6805 - accuracy: 0.7105 - val_loss: 0.6899 - val_accuracy: 0.5000\n",
      "Epoch 299/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6813 - accuracy: 0.6579 - val_loss: 0.6898 - val_accuracy: 0.5000\n",
      "Epoch 300/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6848 - accuracy: 0.5526 - val_loss: 0.6898 - val_accuracy: 0.5000\n",
      "Epoch 301/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6796 - accuracy: 0.6053 - val_loss: 0.6898 - val_accuracy: 0.5000\n",
      "Epoch 302/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6893 - accuracy: 0.5263 - val_loss: 0.6898 - val_accuracy: 0.5000\n",
      "Epoch 303/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6828 - accuracy: 0.5789 - val_loss: 0.6898 - val_accuracy: 0.5000\n",
      "Epoch 304/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6765 - accuracy: 0.6842 - val_loss: 0.6898 - val_accuracy: 0.5000\n",
      "Epoch 305/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6885 - accuracy: 0.6053 - val_loss: 0.6898 - val_accuracy: 0.5000\n",
      "Epoch 306/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6862 - accuracy: 0.6053 - val_loss: 0.6898 - val_accuracy: 0.5000\n",
      "Epoch 307/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6846 - accuracy: 0.5526 - val_loss: 0.6898 - val_accuracy: 0.5000\n",
      "Epoch 308/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6809 - accuracy: 0.7368 - val_loss: 0.6898 - val_accuracy: 0.5000\n",
      "Epoch 309/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6806 - accuracy: 0.5789 - val_loss: 0.6898 - val_accuracy: 0.5000\n",
      "Epoch 310/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6950 - accuracy: 0.5000 - val_loss: 0.6897 - val_accuracy: 0.5000\n",
      "Epoch 311/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6853 - accuracy: 0.5789 - val_loss: 0.6897 - val_accuracy: 0.5000\n",
      "Epoch 312/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6792 - accuracy: 0.6842 - val_loss: 0.6897 - val_accuracy: 0.5000\n",
      "Epoch 313/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6824 - accuracy: 0.6053 - val_loss: 0.6897 - val_accuracy: 0.5000\n",
      "Epoch 314/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6835 - accuracy: 0.6316 - val_loss: 0.6897 - val_accuracy: 0.5000\n",
      "Epoch 315/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6818 - accuracy: 0.5526 - val_loss: 0.6896 - val_accuracy: 0.5000\n",
      "Epoch 316/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6836 - accuracy: 0.6316 - val_loss: 0.6896 - val_accuracy: 0.5000\n",
      "Epoch 317/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.5526 - val_loss: 0.6896 - val_accuracy: 0.5000\n",
      "Epoch 318/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6827 - accuracy: 0.7105 - val_loss: 0.6895 - val_accuracy: 0.5000\n",
      "Epoch 319/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6842 - accuracy: 0.5789 - val_loss: 0.6895 - val_accuracy: 0.5000\n",
      "Epoch 320/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6933 - accuracy: 0.5526 - val_loss: 0.6895 - val_accuracy: 0.5000\n",
      "Epoch 321/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6864 - accuracy: 0.5526 - val_loss: 0.6895 - val_accuracy: 0.5000\n",
      "Epoch 322/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6853 - accuracy: 0.5526 - val_loss: 0.6895 - val_accuracy: 0.5000\n",
      "Epoch 323/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6854 - accuracy: 0.5789 - val_loss: 0.6895 - val_accuracy: 0.5000\n",
      "Epoch 324/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6834 - accuracy: 0.6053 - val_loss: 0.6895 - val_accuracy: 0.5000\n",
      "Epoch 325/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6817 - accuracy: 0.5789 - val_loss: 0.6895 - val_accuracy: 0.5000\n",
      "Epoch 326/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6902 - accuracy: 0.5263 - val_loss: 0.6895 - val_accuracy: 0.5000\n",
      "Epoch 327/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6803 - accuracy: 0.6053 - val_loss: 0.6894 - val_accuracy: 0.5000\n",
      "Epoch 328/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6851 - accuracy: 0.6316 - val_loss: 0.6894 - val_accuracy: 0.5000\n",
      "Epoch 329/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6892 - accuracy: 0.5789 - val_loss: 0.6894 - val_accuracy: 0.5000\n",
      "Epoch 330/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6782 - accuracy: 0.6579 - val_loss: 0.6894 - val_accuracy: 0.5000\n",
      "Epoch 331/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6823 - accuracy: 0.6053 - val_loss: 0.6894 - val_accuracy: 0.5000\n",
      "Epoch 332/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6780 - accuracy: 0.6579 - val_loss: 0.6894 - val_accuracy: 0.5000\n",
      "Epoch 333/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6873 - accuracy: 0.5263 - val_loss: 0.6894 - val_accuracy: 0.5000\n",
      "Epoch 334/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6894 - accuracy: 0.5789 - val_loss: 0.6894 - val_accuracy: 0.5000\n",
      "Epoch 335/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6901 - accuracy: 0.5526 - val_loss: 0.6894 - val_accuracy: 0.5000\n",
      "Epoch 336/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6810 - accuracy: 0.7105 - val_loss: 0.6894 - val_accuracy: 0.5000\n",
      "Epoch 337/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6777 - accuracy: 0.6053 - val_loss: 0.6893 - val_accuracy: 0.5000\n",
      "Epoch 338/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6721 - accuracy: 0.6053 - val_loss: 0.6893 - val_accuracy: 0.5000\n",
      "Epoch 339/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6777 - accuracy: 0.7105 - val_loss: 0.6893 - val_accuracy: 0.5000\n",
      "Epoch 340/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6868 - accuracy: 0.5789 - val_loss: 0.6892 - val_accuracy: 0.5000\n",
      "Epoch 341/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6748 - accuracy: 0.6842 - val_loss: 0.6892 - val_accuracy: 0.5000\n",
      "Epoch 342/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6713 - accuracy: 0.6579 - val_loss: 0.6892 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 343/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6753 - accuracy: 0.6579 - val_loss: 0.6891 - val_accuracy: 0.5000\n",
      "Epoch 344/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6828 - accuracy: 0.6842 - val_loss: 0.6891 - val_accuracy: 0.5000\n",
      "Epoch 345/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6946 - accuracy: 0.4211 - val_loss: 0.6891 - val_accuracy: 0.5000\n",
      "Epoch 346/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6835 - accuracy: 0.6316 - val_loss: 0.6890 - val_accuracy: 0.5000\n",
      "Epoch 347/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6872 - accuracy: 0.5263 - val_loss: 0.6890 - val_accuracy: 0.5000\n",
      "Epoch 348/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6758 - accuracy: 0.7105 - val_loss: 0.6890 - val_accuracy: 0.5000\n",
      "Epoch 349/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6870 - accuracy: 0.4474 - val_loss: 0.6889 - val_accuracy: 0.5000\n",
      "Epoch 350/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6876 - accuracy: 0.5000 - val_loss: 0.6889 - val_accuracy: 0.5000\n",
      "Epoch 351/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6844 - accuracy: 0.5526 - val_loss: 0.6889 - val_accuracy: 0.5000\n",
      "Epoch 352/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6876 - accuracy: 0.5526 - val_loss: 0.6889 - val_accuracy: 0.5000\n",
      "Epoch 353/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6892 - accuracy: 0.5526 - val_loss: 0.6889 - val_accuracy: 0.5000\n",
      "Epoch 354/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6780 - accuracy: 0.6579 - val_loss: 0.6889 - val_accuracy: 0.5000\n",
      "Epoch 355/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6817 - accuracy: 0.6579 - val_loss: 0.6890 - val_accuracy: 0.5000\n",
      "Epoch 356/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6748 - accuracy: 0.7105 - val_loss: 0.6890 - val_accuracy: 0.5000\n",
      "Epoch 357/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6869 - accuracy: 0.5526 - val_loss: 0.6890 - val_accuracy: 0.5000\n",
      "Epoch 358/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6703 - accuracy: 0.7105 - val_loss: 0.6890 - val_accuracy: 0.5000\n",
      "Epoch 359/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6735 - accuracy: 0.6579 - val_loss: 0.6890 - val_accuracy: 0.5000\n",
      "Epoch 360/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6723 - accuracy: 0.6842 - val_loss: 0.6889 - val_accuracy: 0.5000\n",
      "Epoch 361/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6749 - accuracy: 0.6053 - val_loss: 0.6889 - val_accuracy: 0.5000\n",
      "Epoch 362/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6903 - accuracy: 0.4737 - val_loss: 0.6888 - val_accuracy: 0.5000\n",
      "Epoch 363/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6835 - accuracy: 0.6579 - val_loss: 0.6888 - val_accuracy: 0.5000\n",
      "Epoch 364/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6675 - accuracy: 0.7632 - val_loss: 0.6887 - val_accuracy: 0.5000\n",
      "Epoch 365/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6874 - accuracy: 0.5526 - val_loss: 0.6887 - val_accuracy: 0.5000\n",
      "Epoch 366/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6758 - accuracy: 0.7368 - val_loss: 0.6887 - val_accuracy: 0.5000\n",
      "Epoch 367/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6819 - accuracy: 0.6053 - val_loss: 0.6886 - val_accuracy: 0.5000\n",
      "Epoch 368/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6830 - accuracy: 0.6053 - val_loss: 0.6886 - val_accuracy: 0.5000\n",
      "Epoch 369/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6836 - accuracy: 0.5526 - val_loss: 0.6886 - val_accuracy: 0.5000\n",
      "Epoch 370/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6748 - accuracy: 0.7105 - val_loss: 0.6885 - val_accuracy: 0.5000\n",
      "Epoch 371/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6840 - accuracy: 0.6579 - val_loss: 0.6885 - val_accuracy: 0.5000\n",
      "Epoch 372/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6931 - accuracy: 0.4474 - val_loss: 0.6885 - val_accuracy: 0.5000\n",
      "Epoch 373/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6799 - accuracy: 0.6053 - val_loss: 0.6885 - val_accuracy: 0.5000\n",
      "Epoch 374/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6831 - accuracy: 0.6316 - val_loss: 0.6884 - val_accuracy: 0.5000\n",
      "Epoch 375/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6729 - accuracy: 0.6316 - val_loss: 0.6884 - val_accuracy: 0.5000\n",
      "Epoch 376/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6880 - accuracy: 0.5789 - val_loss: 0.6884 - val_accuracy: 0.5000\n",
      "Epoch 377/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6825 - accuracy: 0.6579 - val_loss: 0.6884 - val_accuracy: 0.5000\n",
      "Epoch 378/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6838 - accuracy: 0.5789 - val_loss: 0.6884 - val_accuracy: 0.5000\n",
      "Epoch 379/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6801 - accuracy: 0.6053 - val_loss: 0.6883 - val_accuracy: 0.5000\n",
      "Epoch 380/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6780 - accuracy: 0.6053 - val_loss: 0.6883 - val_accuracy: 0.5000\n",
      "Epoch 381/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6805 - accuracy: 0.6053 - val_loss: 0.6883 - val_accuracy: 0.5000\n",
      "Epoch 382/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6747 - accuracy: 0.7105 - val_loss: 0.6882 - val_accuracy: 0.5000\n",
      "Epoch 383/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6762 - accuracy: 0.6053 - val_loss: 0.6882 - val_accuracy: 0.5000\n",
      "Epoch 384/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6798 - accuracy: 0.5526 - val_loss: 0.6881 - val_accuracy: 0.5000\n",
      "Epoch 385/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6825 - accuracy: 0.5526 - val_loss: 0.6881 - val_accuracy: 0.5000\n",
      "Epoch 386/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6790 - accuracy: 0.5789 - val_loss: 0.6880 - val_accuracy: 0.5000\n",
      "Epoch 387/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6759 - accuracy: 0.6316 - val_loss: 0.6880 - val_accuracy: 0.5000\n",
      "Epoch 388/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6883 - accuracy: 0.5789 - val_loss: 0.6880 - val_accuracy: 0.5000\n",
      "Epoch 389/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6762 - accuracy: 0.6053 - val_loss: 0.6879 - val_accuracy: 0.5000\n",
      "Epoch 390/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6772 - accuracy: 0.5526 - val_loss: 0.6879 - val_accuracy: 0.5000\n",
      "Epoch 391/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6835 - accuracy: 0.6316 - val_loss: 0.6879 - val_accuracy: 0.5000\n",
      "Epoch 392/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6805 - accuracy: 0.5789 - val_loss: 0.6879 - val_accuracy: 0.5000\n",
      "Epoch 393/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6909 - accuracy: 0.5263 - val_loss: 0.6879 - val_accuracy: 0.5000\n",
      "Epoch 394/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6753 - accuracy: 0.6053 - val_loss: 0.6879 - val_accuracy: 0.5000\n",
      "Epoch 395/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6773 - accuracy: 0.6579 - val_loss: 0.6879 - val_accuracy: 0.5000\n",
      "Epoch 396/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6876 - accuracy: 0.6316 - val_loss: 0.6879 - val_accuracy: 0.5000\n",
      "Epoch 397/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6699 - accuracy: 0.6842 - val_loss: 0.6878 - val_accuracy: 0.5000\n",
      "Epoch 398/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6684 - accuracy: 0.7105 - val_loss: 0.6878 - val_accuracy: 0.5000\n",
      "Epoch 399/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6793 - accuracy: 0.5789 - val_loss: 0.6878 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6675 - accuracy: 0.7632 - val_loss: 0.6878 - val_accuracy: 0.5000\n",
      "Epoch 401/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6750 - accuracy: 0.6579 - val_loss: 0.6878 - val_accuracy: 0.5000\n",
      "Epoch 402/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6804 - accuracy: 0.6316 - val_loss: 0.6877 - val_accuracy: 0.5000\n",
      "Epoch 403/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6666 - accuracy: 0.6842 - val_loss: 0.6877 - val_accuracy: 0.5000\n",
      "Epoch 404/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6663 - accuracy: 0.7105 - val_loss: 0.6877 - val_accuracy: 0.5000\n",
      "Epoch 405/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6816 - accuracy: 0.5789 - val_loss: 0.6877 - val_accuracy: 0.5000\n",
      "Epoch 406/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6865 - accuracy: 0.5526 - val_loss: 0.6877 - val_accuracy: 0.5000\n",
      "Epoch 407/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6720 - accuracy: 0.6842 - val_loss: 0.6877 - val_accuracy: 0.5000\n",
      "Epoch 408/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6788 - accuracy: 0.7368 - val_loss: 0.6877 - val_accuracy: 0.5000\n",
      "Epoch 409/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6810 - accuracy: 0.7105 - val_loss: 0.6877 - val_accuracy: 0.5000\n",
      "Epoch 410/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6897 - accuracy: 0.6053 - val_loss: 0.6877 - val_accuracy: 0.5000\n",
      "Epoch 411/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6636 - accuracy: 0.7105 - val_loss: 0.6876 - val_accuracy: 0.5000\n",
      "Epoch 412/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6811 - accuracy: 0.6579 - val_loss: 0.6876 - val_accuracy: 0.5000\n",
      "Epoch 413/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6655 - accuracy: 0.6316 - val_loss: 0.6876 - val_accuracy: 0.5000\n",
      "Epoch 414/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6703 - accuracy: 0.6053 - val_loss: 0.6875 - val_accuracy: 0.5000\n",
      "Epoch 415/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6835 - accuracy: 0.5789 - val_loss: 0.6875 - val_accuracy: 0.5000\n",
      "Epoch 416/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6816 - accuracy: 0.6579 - val_loss: 0.6875 - val_accuracy: 0.5000\n",
      "Epoch 417/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6759 - accuracy: 0.6053 - val_loss: 0.6876 - val_accuracy: 0.5000\n",
      "Epoch 418/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6702 - accuracy: 0.6842 - val_loss: 0.6876 - val_accuracy: 0.5000\n",
      "Epoch 419/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6717 - accuracy: 0.7632 - val_loss: 0.6876 - val_accuracy: 0.5000\n",
      "Epoch 420/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6787 - accuracy: 0.6316 - val_loss: 0.6876 - val_accuracy: 0.5000\n",
      "Epoch 421/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6750 - accuracy: 0.5263 - val_loss: 0.6875 - val_accuracy: 0.5000\n",
      "Epoch 422/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6729 - accuracy: 0.6579 - val_loss: 0.6874 - val_accuracy: 0.5000\n",
      "Epoch 423/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6527 - accuracy: 0.7105 - val_loss: 0.6874 - val_accuracy: 0.5000\n",
      "Epoch 424/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6823 - accuracy: 0.6053 - val_loss: 0.6873 - val_accuracy: 0.5000\n",
      "Epoch 425/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6603 - accuracy: 0.7895 - val_loss: 0.6872 - val_accuracy: 0.5000\n",
      "Epoch 426/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6731 - accuracy: 0.6842 - val_loss: 0.6871 - val_accuracy: 0.5000\n",
      "Epoch 427/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6876 - accuracy: 0.5263 - val_loss: 0.6870 - val_accuracy: 0.5000\n",
      "Epoch 428/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6852 - accuracy: 0.5526 - val_loss: 0.6870 - val_accuracy: 0.5000\n",
      "Epoch 429/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6677 - accuracy: 0.6842 - val_loss: 0.6869 - val_accuracy: 0.5000\n",
      "Epoch 430/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.6777 - accuracy: 0.6316 - val_loss: 0.6869 - val_accuracy: 0.5000\n",
      "Epoch 431/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6686 - accuracy: 0.6842 - val_loss: 0.6869 - val_accuracy: 0.5000\n",
      "Epoch 432/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6839 - accuracy: 0.5000 - val_loss: 0.6869 - val_accuracy: 0.5000\n",
      "Epoch 433/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6730 - accuracy: 0.6579 - val_loss: 0.6869 - val_accuracy: 0.5000\n",
      "Epoch 434/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6757 - accuracy: 0.5789 - val_loss: 0.6869 - val_accuracy: 0.5000\n",
      "Epoch 435/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6646 - accuracy: 0.7895 - val_loss: 0.6870 - val_accuracy: 0.5000\n",
      "Epoch 436/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6721 - accuracy: 0.7105 - val_loss: 0.6870 - val_accuracy: 0.5000\n",
      "Epoch 437/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6829 - accuracy: 0.6842 - val_loss: 0.6871 - val_accuracy: 0.5000\n",
      "Epoch 438/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6563 - accuracy: 0.7368 - val_loss: 0.6872 - val_accuracy: 0.5000\n",
      "Epoch 439/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6707 - accuracy: 0.6053 - val_loss: 0.6873 - val_accuracy: 0.5000\n",
      "Epoch 440/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6654 - accuracy: 0.7105 - val_loss: 0.6873 - val_accuracy: 0.5000\n",
      "Epoch 441/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6845 - accuracy: 0.5789 - val_loss: 0.6874 - val_accuracy: 0.5000\n",
      "Epoch 442/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6685 - accuracy: 0.7105 - val_loss: 0.6875 - val_accuracy: 0.5000\n",
      "Epoch 443/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6638 - accuracy: 0.7105 - val_loss: 0.6875 - val_accuracy: 0.5000\n",
      "Epoch 444/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6584 - accuracy: 0.6579 - val_loss: 0.6876 - val_accuracy: 0.5000\n",
      "Epoch 445/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6792 - accuracy: 0.5526 - val_loss: 0.6876 - val_accuracy: 0.5000\n",
      "Epoch 446/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6576 - accuracy: 0.6842 - val_loss: 0.6876 - val_accuracy: 0.5000\n",
      "Epoch 447/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6700 - accuracy: 0.6053 - val_loss: 0.6876 - val_accuracy: 0.5000\n",
      "Epoch 448/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6664 - accuracy: 0.6053 - val_loss: 0.6876 - val_accuracy: 0.5000\n",
      "Epoch 449/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6747 - accuracy: 0.5789 - val_loss: 0.6876 - val_accuracy: 0.5000\n",
      "Epoch 450/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6668 - accuracy: 0.5263 - val_loss: 0.6876 - val_accuracy: 0.5000\n",
      "Epoch 451/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6689 - accuracy: 0.5789 - val_loss: 0.6876 - val_accuracy: 0.5000\n",
      "Epoch 452/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6652 - accuracy: 0.6579 - val_loss: 0.6876 - val_accuracy: 0.5000\n",
      "Epoch 453/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6730 - accuracy: 0.5789 - val_loss: 0.6876 - val_accuracy: 0.5000\n",
      "Epoch 454/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6755 - accuracy: 0.5000 - val_loss: 0.6876 - val_accuracy: 0.5000\n",
      "Epoch 455/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6597 - accuracy: 0.6842 - val_loss: 0.6876 - val_accuracy: 0.5000\n",
      "Epoch 456/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6570 - accuracy: 0.7105 - val_loss: 0.6876 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 457/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6713 - accuracy: 0.6053 - val_loss: 0.6876 - val_accuracy: 0.5000\n",
      "Epoch 458/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6590 - accuracy: 0.7105 - val_loss: 0.6876 - val_accuracy: 0.5000\n",
      "Epoch 459/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6797 - accuracy: 0.6579 - val_loss: 0.6875 - val_accuracy: 0.5000\n",
      "Epoch 460/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6640 - accuracy: 0.6316 - val_loss: 0.6875 - val_accuracy: 0.5000\n",
      "Epoch 461/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6748 - accuracy: 0.5789 - val_loss: 0.6874 - val_accuracy: 0.5000\n",
      "Epoch 462/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6835 - accuracy: 0.5000 - val_loss: 0.6874 - val_accuracy: 0.5000\n",
      "Epoch 463/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6725 - accuracy: 0.5789 - val_loss: 0.6874 - val_accuracy: 0.5000\n",
      "Epoch 464/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6735 - accuracy: 0.6053 - val_loss: 0.6873 - val_accuracy: 0.5000\n",
      "Epoch 465/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6710 - accuracy: 0.6316 - val_loss: 0.6872 - val_accuracy: 0.5000\n",
      "Epoch 466/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6881 - accuracy: 0.5000 - val_loss: 0.6872 - val_accuracy: 0.5000\n",
      "Epoch 467/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6513 - accuracy: 0.7895 - val_loss: 0.6872 - val_accuracy: 0.5000\n",
      "Epoch 468/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6662 - accuracy: 0.6842 - val_loss: 0.6873 - val_accuracy: 0.5000\n",
      "Epoch 469/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6545 - accuracy: 0.7105 - val_loss: 0.6873 - val_accuracy: 0.5000\n",
      "Epoch 470/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6645 - accuracy: 0.6316 - val_loss: 0.6872 - val_accuracy: 0.5000\n",
      "Epoch 471/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6717 - accuracy: 0.6579 - val_loss: 0.6872 - val_accuracy: 0.5000\n",
      "Epoch 472/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6602 - accuracy: 0.6842 - val_loss: 0.6872 - val_accuracy: 0.5000\n",
      "Epoch 473/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6862 - accuracy: 0.5263 - val_loss: 0.6872 - val_accuracy: 0.5000\n",
      "Epoch 474/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6487 - accuracy: 0.7105 - val_loss: 0.6871 - val_accuracy: 0.5000\n",
      "Epoch 475/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6604 - accuracy: 0.7105 - val_loss: 0.6870 - val_accuracy: 0.5000\n",
      "Epoch 476/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6653 - accuracy: 0.6053 - val_loss: 0.6870 - val_accuracy: 0.5000\n",
      "Epoch 477/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6773 - accuracy: 0.6053 - val_loss: 0.6869 - val_accuracy: 0.5000\n",
      "Epoch 478/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6779 - accuracy: 0.5526 - val_loss: 0.6869 - val_accuracy: 0.5000\n",
      "Epoch 479/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6565 - accuracy: 0.6579 - val_loss: 0.6868 - val_accuracy: 0.5000\n",
      "Epoch 480/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6707 - accuracy: 0.5526 - val_loss: 0.6867 - val_accuracy: 0.5000\n",
      "Epoch 481/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6632 - accuracy: 0.5000 - val_loss: 0.6866 - val_accuracy: 0.5000\n",
      "Epoch 482/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6814 - accuracy: 0.6316 - val_loss: 0.6866 - val_accuracy: 0.5000\n",
      "Epoch 483/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6603 - accuracy: 0.6316 - val_loss: 0.6865 - val_accuracy: 0.5000\n",
      "Epoch 484/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6561 - accuracy: 0.6842 - val_loss: 0.6865 - val_accuracy: 0.5000\n",
      "Epoch 485/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6561 - accuracy: 0.6579 - val_loss: 0.6864 - val_accuracy: 0.5000\n",
      "Epoch 486/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6645 - accuracy: 0.6316 - val_loss: 0.6864 - val_accuracy: 0.5000\n",
      "Epoch 487/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6656 - accuracy: 0.5263 - val_loss: 0.6863 - val_accuracy: 0.5000\n",
      "Epoch 488/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6696 - accuracy: 0.6316 - val_loss: 0.6863 - val_accuracy: 0.5000\n",
      "Epoch 489/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6627 - accuracy: 0.6579 - val_loss: 0.6863 - val_accuracy: 0.5000\n",
      "Epoch 490/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6703 - accuracy: 0.5789 - val_loss: 0.6862 - val_accuracy: 0.5000\n",
      "Epoch 491/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6782 - accuracy: 0.6053 - val_loss: 0.6861 - val_accuracy: 0.5000\n",
      "Epoch 492/1000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.6701 - accuracy: 0.6316 - val_loss: 0.6859 - val_accuracy: 0.5000\n",
      "Epoch 493/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6623 - accuracy: 0.6579 - val_loss: 0.6858 - val_accuracy: 0.5000\n",
      "Epoch 494/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6529 - accuracy: 0.7368 - val_loss: 0.6856 - val_accuracy: 0.5000\n",
      "Epoch 495/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6506 - accuracy: 0.6842 - val_loss: 0.6855 - val_accuracy: 0.5000\n",
      "Epoch 496/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6528 - accuracy: 0.7105 - val_loss: 0.6854 - val_accuracy: 0.5000\n",
      "Epoch 497/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6520 - accuracy: 0.6842 - val_loss: 0.6853 - val_accuracy: 0.5000\n",
      "Epoch 498/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6627 - accuracy: 0.6842 - val_loss: 0.6853 - val_accuracy: 0.5000\n",
      "Epoch 499/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6532 - accuracy: 0.7368 - val_loss: 0.6853 - val_accuracy: 0.5000\n",
      "Epoch 500/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6647 - accuracy: 0.6842 - val_loss: 0.6853 - val_accuracy: 0.5000\n",
      "Epoch 501/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6441 - accuracy: 0.6579 - val_loss: 0.6854 - val_accuracy: 0.5000\n",
      "Epoch 502/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6465 - accuracy: 0.7368 - val_loss: 0.6854 - val_accuracy: 0.5000\n",
      "Epoch 503/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6648 - accuracy: 0.5526 - val_loss: 0.6853 - val_accuracy: 0.5000\n",
      "Epoch 504/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6329 - accuracy: 0.7368 - val_loss: 0.6852 - val_accuracy: 0.5000\n",
      "Epoch 505/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6756 - accuracy: 0.6316 - val_loss: 0.6851 - val_accuracy: 0.5000\n",
      "Epoch 506/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6556 - accuracy: 0.6053 - val_loss: 0.6850 - val_accuracy: 0.5000\n",
      "Epoch 507/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6471 - accuracy: 0.7105 - val_loss: 0.6849 - val_accuracy: 0.5000\n",
      "Epoch 508/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6516 - accuracy: 0.7105 - val_loss: 0.6849 - val_accuracy: 0.5000\n",
      "Epoch 509/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6643 - accuracy: 0.7368 - val_loss: 0.6848 - val_accuracy: 0.5000\n",
      "Epoch 510/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6604 - accuracy: 0.5789 - val_loss: 0.6848 - val_accuracy: 0.5000\n",
      "Epoch 511/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6629 - accuracy: 0.6579 - val_loss: 0.6847 - val_accuracy: 0.5000\n",
      "Epoch 512/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6571 - accuracy: 0.6316 - val_loss: 0.6847 - val_accuracy: 0.5000\n",
      "Epoch 513/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6336 - accuracy: 0.6579 - val_loss: 0.6847 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 514/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6546 - accuracy: 0.5789 - val_loss: 0.6847 - val_accuracy: 0.5000\n",
      "Epoch 515/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6517 - accuracy: 0.6316 - val_loss: 0.6848 - val_accuracy: 0.5000\n",
      "Epoch 516/1000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.6617 - accuracy: 0.6579 - val_loss: 0.6848 - val_accuracy: 0.5000\n",
      "Epoch 517/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6280 - accuracy: 0.7105 - val_loss: 0.6848 - val_accuracy: 0.5000\n",
      "Epoch 518/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6560 - accuracy: 0.6316 - val_loss: 0.6848 - val_accuracy: 0.5000\n",
      "Epoch 519/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6460 - accuracy: 0.6316 - val_loss: 0.6846 - val_accuracy: 0.5000\n",
      "Epoch 520/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6549 - accuracy: 0.5526 - val_loss: 0.6845 - val_accuracy: 0.5000\n",
      "Epoch 521/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6521 - accuracy: 0.6842 - val_loss: 0.6844 - val_accuracy: 0.5000\n",
      "Epoch 522/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6595 - accuracy: 0.6579 - val_loss: 0.6843 - val_accuracy: 0.5000\n",
      "Epoch 523/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6511 - accuracy: 0.7105 - val_loss: 0.6842 - val_accuracy: 0.5000\n",
      "Epoch 524/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6583 - accuracy: 0.6579 - val_loss: 0.6842 - val_accuracy: 0.5000\n",
      "Epoch 525/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6376 - accuracy: 0.7368 - val_loss: 0.6842 - val_accuracy: 0.5000\n",
      "Epoch 526/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6354 - accuracy: 0.6579 - val_loss: 0.6842 - val_accuracy: 0.5000\n",
      "Epoch 527/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6798 - accuracy: 0.5789 - val_loss: 0.6841 - val_accuracy: 0.5000\n",
      "Epoch 528/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6794 - accuracy: 0.5526 - val_loss: 0.6839 - val_accuracy: 0.5000\n",
      "Epoch 529/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6606 - accuracy: 0.6316 - val_loss: 0.6837 - val_accuracy: 0.5000\n",
      "Epoch 530/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6450 - accuracy: 0.6579 - val_loss: 0.6836 - val_accuracy: 0.5000\n",
      "Epoch 531/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6427 - accuracy: 0.6842 - val_loss: 0.6835 - val_accuracy: 0.5000\n",
      "Epoch 532/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6389 - accuracy: 0.7105 - val_loss: 0.6834 - val_accuracy: 0.5000\n",
      "Epoch 533/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6495 - accuracy: 0.7105 - val_loss: 0.6833 - val_accuracy: 0.5000\n",
      "Epoch 534/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6433 - accuracy: 0.7368 - val_loss: 0.6832 - val_accuracy: 0.5000\n",
      "Epoch 535/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6463 - accuracy: 0.6579 - val_loss: 0.6831 - val_accuracy: 0.5000\n",
      "Epoch 536/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6477 - accuracy: 0.7368 - val_loss: 0.6829 - val_accuracy: 0.5000\n",
      "Epoch 537/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6583 - accuracy: 0.6842 - val_loss: 0.6827 - val_accuracy: 0.5000\n",
      "Epoch 538/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6408 - accuracy: 0.6316 - val_loss: 0.6826 - val_accuracy: 0.5000\n",
      "Epoch 539/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6612 - accuracy: 0.6579 - val_loss: 0.6825 - val_accuracy: 0.5000\n",
      "Epoch 540/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6593 - accuracy: 0.6316 - val_loss: 0.6823 - val_accuracy: 0.5000\n",
      "Epoch 541/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6389 - accuracy: 0.7368 - val_loss: 0.6820 - val_accuracy: 0.5000\n",
      "Epoch 542/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6429 - accuracy: 0.5789 - val_loss: 0.6817 - val_accuracy: 0.5000\n",
      "Epoch 543/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6527 - accuracy: 0.7105 - val_loss: 0.6813 - val_accuracy: 0.5000\n",
      "Epoch 544/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6555 - accuracy: 0.6053 - val_loss: 0.6811 - val_accuracy: 0.5000\n",
      "Epoch 545/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6305 - accuracy: 0.6842 - val_loss: 0.6809 - val_accuracy: 0.5000\n",
      "Epoch 546/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6569 - accuracy: 0.6316 - val_loss: 0.6807 - val_accuracy: 0.5000\n",
      "Epoch 547/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6556 - accuracy: 0.5789 - val_loss: 0.6806 - val_accuracy: 0.5000\n",
      "Epoch 548/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6422 - accuracy: 0.7368 - val_loss: 0.6804 - val_accuracy: 0.5000\n",
      "Epoch 549/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6312 - accuracy: 0.7895 - val_loss: 0.6802 - val_accuracy: 0.5000\n",
      "Epoch 550/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6369 - accuracy: 0.7368 - val_loss: 0.6803 - val_accuracy: 0.5000\n",
      "Epoch 551/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6260 - accuracy: 0.7105 - val_loss: 0.6803 - val_accuracy: 0.5000\n",
      "Epoch 552/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6387 - accuracy: 0.7105 - val_loss: 0.6802 - val_accuracy: 0.5000\n",
      "Epoch 553/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6304 - accuracy: 0.6316 - val_loss: 0.6801 - val_accuracy: 0.5000\n",
      "Epoch 554/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6406 - accuracy: 0.7632 - val_loss: 0.6800 - val_accuracy: 0.5000\n",
      "Epoch 555/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6345 - accuracy: 0.6842 - val_loss: 0.6799 - val_accuracy: 0.5000\n",
      "Epoch 556/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6478 - accuracy: 0.7368 - val_loss: 0.6797 - val_accuracy: 0.5000\n",
      "Epoch 557/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6530 - accuracy: 0.7105 - val_loss: 0.6795 - val_accuracy: 0.5000\n",
      "Epoch 558/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6391 - accuracy: 0.7368 - val_loss: 0.6793 - val_accuracy: 0.5000\n",
      "Epoch 559/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6506 - accuracy: 0.6579 - val_loss: 0.6790 - val_accuracy: 0.5000\n",
      "Epoch 560/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6509 - accuracy: 0.6316 - val_loss: 0.6788 - val_accuracy: 0.5000\n",
      "Epoch 561/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6294 - accuracy: 0.7632 - val_loss: 0.6784 - val_accuracy: 0.5000\n",
      "Epoch 562/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6180 - accuracy: 0.7895 - val_loss: 0.6781 - val_accuracy: 0.5000\n",
      "Epoch 563/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6507 - accuracy: 0.8158 - val_loss: 0.6778 - val_accuracy: 0.5000\n",
      "Epoch 564/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6687 - accuracy: 0.5789 - val_loss: 0.6775 - val_accuracy: 0.5000\n",
      "Epoch 565/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6359 - accuracy: 0.6579 - val_loss: 0.6772 - val_accuracy: 0.5000\n",
      "Epoch 566/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6481 - accuracy: 0.7368 - val_loss: 0.6770 - val_accuracy: 0.5000\n",
      "Epoch 567/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6310 - accuracy: 0.7368 - val_loss: 0.6769 - val_accuracy: 0.5000\n",
      "Epoch 568/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6271 - accuracy: 0.7368 - val_loss: 0.6768 - val_accuracy: 0.5000\n",
      "Epoch 569/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6406 - accuracy: 0.7368 - val_loss: 0.6768 - val_accuracy: 0.5000\n",
      "Epoch 570/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6388 - accuracy: 0.7368 - val_loss: 0.6767 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 571/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6415 - accuracy: 0.6842 - val_loss: 0.6767 - val_accuracy: 0.5000\n",
      "Epoch 572/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6095 - accuracy: 0.7895 - val_loss: 0.6767 - val_accuracy: 0.5000\n",
      "Epoch 573/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6564 - accuracy: 0.7105 - val_loss: 0.6766 - val_accuracy: 0.5000\n",
      "Epoch 574/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6550 - accuracy: 0.6579 - val_loss: 0.6766 - val_accuracy: 0.5000\n",
      "Epoch 575/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6297 - accuracy: 0.7895 - val_loss: 0.6765 - val_accuracy: 0.5000\n",
      "Epoch 576/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6434 - accuracy: 0.7105 - val_loss: 0.6765 - val_accuracy: 0.5000\n",
      "Epoch 577/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6570 - accuracy: 0.7368 - val_loss: 0.6765 - val_accuracy: 0.5000\n",
      "Epoch 578/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6656 - accuracy: 0.5526 - val_loss: 0.6765 - val_accuracy: 0.5000\n",
      "Epoch 579/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6659 - accuracy: 0.7368 - val_loss: 0.6766 - val_accuracy: 0.5000\n",
      "Epoch 580/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6581 - accuracy: 0.6579 - val_loss: 0.6767 - val_accuracy: 0.5000\n",
      "Epoch 581/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6138 - accuracy: 0.6842 - val_loss: 0.6768 - val_accuracy: 0.5000\n",
      "Epoch 582/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6472 - accuracy: 0.7105 - val_loss: 0.6769 - val_accuracy: 0.5000\n",
      "Epoch 583/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6390 - accuracy: 0.6316 - val_loss: 0.6768 - val_accuracy: 0.5000\n",
      "Epoch 584/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6353 - accuracy: 0.6316 - val_loss: 0.6768 - val_accuracy: 0.5000\n",
      "Epoch 585/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6319 - accuracy: 0.7105 - val_loss: 0.6768 - val_accuracy: 0.5000\n",
      "Epoch 586/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6381 - accuracy: 0.6842 - val_loss: 0.6769 - val_accuracy: 0.5000\n",
      "Epoch 587/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6023 - accuracy: 0.6579 - val_loss: 0.6769 - val_accuracy: 0.5000\n",
      "Epoch 588/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6277 - accuracy: 0.7632 - val_loss: 0.6770 - val_accuracy: 0.5000\n",
      "Epoch 589/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6237 - accuracy: 0.6842 - val_loss: 0.6774 - val_accuracy: 0.5000\n",
      "Epoch 590/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6527 - accuracy: 0.7105 - val_loss: 0.6777 - val_accuracy: 0.5000\n",
      "Epoch 591/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6478 - accuracy: 0.6842 - val_loss: 0.6781 - val_accuracy: 0.5000\n",
      "Epoch 592/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6261 - accuracy: 0.7895 - val_loss: 0.6786 - val_accuracy: 0.5000\n",
      "Epoch 593/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6365 - accuracy: 0.7368 - val_loss: 0.6791 - val_accuracy: 0.5000\n",
      "Epoch 594/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6542 - accuracy: 0.7105 - val_loss: 0.6795 - val_accuracy: 0.5000\n",
      "Epoch 595/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6349 - accuracy: 0.7895 - val_loss: 0.6799 - val_accuracy: 0.5000\n",
      "Epoch 596/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6226 - accuracy: 0.7632 - val_loss: 0.6803 - val_accuracy: 0.5000\n",
      "Epoch 597/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6277 - accuracy: 0.7895 - val_loss: 0.6807 - val_accuracy: 0.5000\n",
      "Epoch 598/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6319 - accuracy: 0.7105 - val_loss: 0.6811 - val_accuracy: 0.5000\n",
      "Epoch 599/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6402 - accuracy: 0.7368 - val_loss: 0.6813 - val_accuracy: 0.5000\n",
      "Epoch 600/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6489 - accuracy: 0.6842 - val_loss: 0.6816 - val_accuracy: 0.5000\n",
      "Epoch 601/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6474 - accuracy: 0.6053 - val_loss: 0.6817 - val_accuracy: 0.5000\n",
      "Epoch 602/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6185 - accuracy: 0.7105 - val_loss: 0.6816 - val_accuracy: 0.5000\n",
      "Epoch 603/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6412 - accuracy: 0.7105 - val_loss: 0.6815 - val_accuracy: 0.5000\n",
      "Epoch 604/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5957 - accuracy: 0.7895 - val_loss: 0.6814 - val_accuracy: 0.5000\n",
      "Epoch 605/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6475 - accuracy: 0.6579 - val_loss: 0.6813 - val_accuracy: 0.5000\n",
      "Epoch 606/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6617 - accuracy: 0.6579 - val_loss: 0.6813 - val_accuracy: 0.5000\n",
      "Epoch 607/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6291 - accuracy: 0.6579 - val_loss: 0.6813 - val_accuracy: 0.5000\n",
      "Epoch 608/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5977 - accuracy: 0.7368 - val_loss: 0.6814 - val_accuracy: 0.5000\n",
      "Epoch 609/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6470 - accuracy: 0.7368 - val_loss: 0.6815 - val_accuracy: 0.5000\n",
      "Epoch 610/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5986 - accuracy: 0.8158 - val_loss: 0.6815 - val_accuracy: 0.5000\n",
      "Epoch 611/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6001 - accuracy: 0.7632 - val_loss: 0.6815 - val_accuracy: 0.5000\n",
      "Epoch 612/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6101 - accuracy: 0.7632 - val_loss: 0.6814 - val_accuracy: 0.5000\n",
      "Epoch 613/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5941 - accuracy: 0.7632 - val_loss: 0.6815 - val_accuracy: 0.5000\n",
      "Epoch 614/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6167 - accuracy: 0.8158 - val_loss: 0.6818 - val_accuracy: 0.5000\n",
      "Epoch 615/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6162 - accuracy: 0.8158 - val_loss: 0.6823 - val_accuracy: 0.5000\n",
      "Epoch 616/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6036 - accuracy: 0.8684 - val_loss: 0.6826 - val_accuracy: 0.5000\n",
      "Epoch 617/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6071 - accuracy: 0.7368 - val_loss: 0.6825 - val_accuracy: 0.5000\n",
      "Epoch 618/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5923 - accuracy: 0.7895 - val_loss: 0.6823 - val_accuracy: 0.5000\n",
      "Epoch 619/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6379 - accuracy: 0.6842 - val_loss: 0.6823 - val_accuracy: 0.5000\n",
      "Epoch 620/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6119 - accuracy: 0.6842 - val_loss: 0.6823 - val_accuracy: 0.5000\n",
      "Epoch 621/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6296 - accuracy: 0.7105 - val_loss: 0.6821 - val_accuracy: 0.5000\n",
      "Epoch 622/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6148 - accuracy: 0.6579 - val_loss: 0.6816 - val_accuracy: 0.5000\n",
      "Epoch 623/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6385 - accuracy: 0.6316 - val_loss: 0.6812 - val_accuracy: 0.5000\n",
      "Epoch 624/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6023 - accuracy: 0.7105 - val_loss: 0.6811 - val_accuracy: 0.5000\n",
      "Epoch 625/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5865 - accuracy: 0.8421 - val_loss: 0.6813 - val_accuracy: 0.5000\n",
      "Epoch 626/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6352 - accuracy: 0.6842 - val_loss: 0.6814 - val_accuracy: 0.5000\n",
      "Epoch 627/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6510 - accuracy: 0.6316 - val_loss: 0.6815 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 628/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5895 - accuracy: 0.8421 - val_loss: 0.6816 - val_accuracy: 0.5000\n",
      "Epoch 629/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5973 - accuracy: 0.7632 - val_loss: 0.6815 - val_accuracy: 0.5000\n",
      "Epoch 630/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6309 - accuracy: 0.7368 - val_loss: 0.6815 - val_accuracy: 0.5000\n",
      "Epoch 631/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6235 - accuracy: 0.7368 - val_loss: 0.6815 - val_accuracy: 0.5000\n",
      "Epoch 632/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6275 - accuracy: 0.8158 - val_loss: 0.6815 - val_accuracy: 0.5000\n",
      "Epoch 633/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5947 - accuracy: 0.8158 - val_loss: 0.6814 - val_accuracy: 0.5000\n",
      "Epoch 634/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5979 - accuracy: 0.7105 - val_loss: 0.6811 - val_accuracy: 0.5000\n",
      "Epoch 635/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5916 - accuracy: 0.7632 - val_loss: 0.6810 - val_accuracy: 0.5000\n",
      "Epoch 636/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6008 - accuracy: 0.7632 - val_loss: 0.6812 - val_accuracy: 0.5000\n",
      "Epoch 637/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5872 - accuracy: 0.8158 - val_loss: 0.6817 - val_accuracy: 0.5000\n",
      "Epoch 638/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6274 - accuracy: 0.7105 - val_loss: 0.6822 - val_accuracy: 0.5000\n",
      "Epoch 639/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6439 - accuracy: 0.6579 - val_loss: 0.6826 - val_accuracy: 0.5000\n",
      "Epoch 640/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6156 - accuracy: 0.7368 - val_loss: 0.6829 - val_accuracy: 0.5000\n",
      "Epoch 641/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5848 - accuracy: 0.8158 - val_loss: 0.6832 - val_accuracy: 0.5000\n",
      "Epoch 642/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6033 - accuracy: 0.6579 - val_loss: 0.6837 - val_accuracy: 0.5000\n",
      "Epoch 643/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6191 - accuracy: 0.7368 - val_loss: 0.6841 - val_accuracy: 0.5000\n",
      "Epoch 644/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6188 - accuracy: 0.7368 - val_loss: 0.6845 - val_accuracy: 0.5000\n",
      "Epoch 645/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6096 - accuracy: 0.7632 - val_loss: 0.6849 - val_accuracy: 0.5000\n",
      "Epoch 646/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6080 - accuracy: 0.7632 - val_loss: 0.6852 - val_accuracy: 0.5000\n",
      "Epoch 647/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6169 - accuracy: 0.7105 - val_loss: 0.6852 - val_accuracy: 0.5000\n",
      "Epoch 648/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6221 - accuracy: 0.7368 - val_loss: 0.6854 - val_accuracy: 0.5000\n",
      "Epoch 649/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6371 - accuracy: 0.7105 - val_loss: 0.6856 - val_accuracy: 0.5000\n",
      "Epoch 650/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6638 - accuracy: 0.6053 - val_loss: 0.6848 - val_accuracy: 0.5000\n",
      "Epoch 651/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5676 - accuracy: 0.7895 - val_loss: 0.6836 - val_accuracy: 0.5000\n",
      "Epoch 652/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6282 - accuracy: 0.6842 - val_loss: 0.6823 - val_accuracy: 0.5000\n",
      "Epoch 653/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5678 - accuracy: 0.8684 - val_loss: 0.6813 - val_accuracy: 0.5000\n",
      "Epoch 654/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6077 - accuracy: 0.8158 - val_loss: 0.6804 - val_accuracy: 0.5000\n",
      "Epoch 655/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6536 - accuracy: 0.6579 - val_loss: 0.6801 - val_accuracy: 0.5000\n",
      "Epoch 656/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5900 - accuracy: 0.7368 - val_loss: 0.6802 - val_accuracy: 0.5000\n",
      "Epoch 657/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5814 - accuracy: 0.7368 - val_loss: 0.6803 - val_accuracy: 0.5000\n",
      "Epoch 658/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5964 - accuracy: 0.7895 - val_loss: 0.6801 - val_accuracy: 0.5000\n",
      "Epoch 659/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5689 - accuracy: 0.8158 - val_loss: 0.6796 - val_accuracy: 0.5000\n",
      "Epoch 660/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6302 - accuracy: 0.6053 - val_loss: 0.6790 - val_accuracy: 0.5000\n",
      "Epoch 661/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6084 - accuracy: 0.7895 - val_loss: 0.6784 - val_accuracy: 0.5000\n",
      "Epoch 662/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6454 - accuracy: 0.5789 - val_loss: 0.6778 - val_accuracy: 0.5000\n",
      "Epoch 663/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6079 - accuracy: 0.8158 - val_loss: 0.6775 - val_accuracy: 0.5000\n",
      "Epoch 664/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6326 - accuracy: 0.6579 - val_loss: 0.6771 - val_accuracy: 0.5000\n",
      "Epoch 665/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5876 - accuracy: 0.7632 - val_loss: 0.6771 - val_accuracy: 0.5000\n",
      "Epoch 666/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6048 - accuracy: 0.7368 - val_loss: 0.6773 - val_accuracy: 0.5000\n",
      "Epoch 667/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6093 - accuracy: 0.7632 - val_loss: 0.6777 - val_accuracy: 0.5000\n",
      "Epoch 668/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5659 - accuracy: 0.7632 - val_loss: 0.6781 - val_accuracy: 0.5000\n",
      "Epoch 669/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5821 - accuracy: 0.7895 - val_loss: 0.6780 - val_accuracy: 0.5000\n",
      "Epoch 670/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5724 - accuracy: 0.7632 - val_loss: 0.6778 - val_accuracy: 0.5000\n",
      "Epoch 671/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5619 - accuracy: 0.7632 - val_loss: 0.6773 - val_accuracy: 0.5000\n",
      "Epoch 672/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6204 - accuracy: 0.6579 - val_loss: 0.6768 - val_accuracy: 0.5000\n",
      "Epoch 673/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6104 - accuracy: 0.6842 - val_loss: 0.6763 - val_accuracy: 0.5000\n",
      "Epoch 674/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5978 - accuracy: 0.7368 - val_loss: 0.6758 - val_accuracy: 0.5000\n",
      "Epoch 675/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5708 - accuracy: 0.6842 - val_loss: 0.6758 - val_accuracy: 0.5000\n",
      "Epoch 676/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5897 - accuracy: 0.7105 - val_loss: 0.6764 - val_accuracy: 0.5000\n",
      "Epoch 677/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5710 - accuracy: 0.8158 - val_loss: 0.6771 - val_accuracy: 0.5000\n",
      "Epoch 678/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5792 - accuracy: 0.7368 - val_loss: 0.6777 - val_accuracy: 0.5000\n",
      "Epoch 679/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5867 - accuracy: 0.7368 - val_loss: 0.6783 - val_accuracy: 0.5000\n",
      "Epoch 680/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5739 - accuracy: 0.7895 - val_loss: 0.6792 - val_accuracy: 0.5000\n",
      "Epoch 681/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5638 - accuracy: 0.7632 - val_loss: 0.6803 - val_accuracy: 0.5000\n",
      "Epoch 682/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5944 - accuracy: 0.7105 - val_loss: 0.6816 - val_accuracy: 0.5000\n",
      "Epoch 683/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5740 - accuracy: 0.7368 - val_loss: 0.6833 - val_accuracy: 0.5000\n",
      "Epoch 684/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6323 - accuracy: 0.6842 - val_loss: 0.6847 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 685/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5724 - accuracy: 0.7632 - val_loss: 0.6851 - val_accuracy: 0.5000\n",
      "Epoch 686/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6329 - accuracy: 0.6579 - val_loss: 0.6856 - val_accuracy: 0.5000\n",
      "Epoch 687/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6106 - accuracy: 0.7105 - val_loss: 0.6863 - val_accuracy: 0.5000\n",
      "Epoch 688/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6324 - accuracy: 0.6579 - val_loss: 0.6866 - val_accuracy: 0.5000\n",
      "Epoch 689/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5477 - accuracy: 0.8158 - val_loss: 0.6869 - val_accuracy: 0.5000\n",
      "Epoch 690/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5753 - accuracy: 0.7632 - val_loss: 0.6866 - val_accuracy: 0.5000\n",
      "Epoch 691/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5614 - accuracy: 0.7895 - val_loss: 0.6863 - val_accuracy: 0.5000\n",
      "Epoch 692/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6007 - accuracy: 0.7632 - val_loss: 0.6862 - val_accuracy: 0.5000\n",
      "Epoch 693/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5875 - accuracy: 0.7368 - val_loss: 0.6862 - val_accuracy: 0.5000\n",
      "Epoch 694/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5885 - accuracy: 0.7368 - val_loss: 0.6863 - val_accuracy: 0.5000\n",
      "Epoch 695/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5624 - accuracy: 0.7632 - val_loss: 0.6865 - val_accuracy: 0.5000\n",
      "Epoch 696/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5743 - accuracy: 0.7632 - val_loss: 0.6874 - val_accuracy: 0.5000\n",
      "Epoch 697/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6110 - accuracy: 0.6316 - val_loss: 0.6886 - val_accuracy: 0.5000\n",
      "Epoch 698/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5896 - accuracy: 0.7632 - val_loss: 0.6890 - val_accuracy: 0.5000\n",
      "Epoch 699/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6060 - accuracy: 0.6842 - val_loss: 0.6894 - val_accuracy: 0.5000\n",
      "Epoch 700/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5792 - accuracy: 0.7632 - val_loss: 0.6896 - val_accuracy: 0.5000\n",
      "Epoch 701/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5660 - accuracy: 0.7895 - val_loss: 0.6893 - val_accuracy: 0.5000\n",
      "Epoch 702/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5511 - accuracy: 0.7368 - val_loss: 0.6889 - val_accuracy: 0.5000\n",
      "Epoch 703/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5803 - accuracy: 0.8158 - val_loss: 0.6886 - val_accuracy: 0.5000\n",
      "Epoch 704/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5700 - accuracy: 0.7632 - val_loss: 0.6883 - val_accuracy: 0.5000\n",
      "Epoch 705/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5677 - accuracy: 0.7105 - val_loss: 0.6884 - val_accuracy: 0.5000\n",
      "Epoch 706/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5593 - accuracy: 0.8158 - val_loss: 0.6887 - val_accuracy: 0.5000\n",
      "Epoch 707/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5783 - accuracy: 0.7632 - val_loss: 0.6888 - val_accuracy: 0.5000\n",
      "Epoch 708/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6087 - accuracy: 0.7895 - val_loss: 0.6885 - val_accuracy: 0.5000\n",
      "Epoch 709/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5858 - accuracy: 0.7895 - val_loss: 0.6881 - val_accuracy: 0.5000\n",
      "Epoch 710/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5732 - accuracy: 0.7105 - val_loss: 0.6874 - val_accuracy: 0.5000\n",
      "Epoch 711/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5910 - accuracy: 0.7895 - val_loss: 0.6866 - val_accuracy: 0.5000\n",
      "Epoch 712/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5861 - accuracy: 0.7368 - val_loss: 0.6865 - val_accuracy: 0.5000\n",
      "Epoch 713/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6037 - accuracy: 0.7895 - val_loss: 0.6873 - val_accuracy: 0.5000\n",
      "Epoch 714/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5306 - accuracy: 0.8421 - val_loss: 0.6884 - val_accuracy: 0.5000\n",
      "Epoch 715/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6137 - accuracy: 0.6842 - val_loss: 0.6886 - val_accuracy: 0.5000\n",
      "Epoch 716/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5542 - accuracy: 0.7632 - val_loss: 0.6882 - val_accuracy: 0.5000\n",
      "Epoch 717/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5607 - accuracy: 0.7632 - val_loss: 0.6882 - val_accuracy: 0.5000\n",
      "Epoch 718/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5669 - accuracy: 0.7368 - val_loss: 0.6883 - val_accuracy: 0.5000\n",
      "Epoch 719/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5453 - accuracy: 0.8158 - val_loss: 0.6885 - val_accuracy: 0.5000\n",
      "Epoch 720/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5943 - accuracy: 0.7895 - val_loss: 0.6893 - val_accuracy: 0.5000\n",
      "Epoch 721/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5419 - accuracy: 0.7895 - val_loss: 0.6906 - val_accuracy: 0.5000\n",
      "Epoch 722/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5713 - accuracy: 0.7895 - val_loss: 0.6922 - val_accuracy: 0.5000\n",
      "Epoch 723/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5118 - accuracy: 0.8421 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 724/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5089 - accuracy: 0.8158 - val_loss: 0.6947 - val_accuracy: 0.5000\n",
      "Epoch 725/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5522 - accuracy: 0.7368 - val_loss: 0.6947 - val_accuracy: 0.5000\n",
      "Epoch 726/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5879 - accuracy: 0.7105 - val_loss: 0.6943 - val_accuracy: 0.5000\n",
      "Epoch 727/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5735 - accuracy: 0.7368 - val_loss: 0.6943 - val_accuracy: 0.5000\n",
      "Epoch 728/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5704 - accuracy: 0.7368 - val_loss: 0.6946 - val_accuracy: 0.5000\n",
      "Epoch 729/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5456 - accuracy: 0.7632 - val_loss: 0.6955 - val_accuracy: 0.5000\n",
      "Epoch 730/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.5104 - accuracy: 0.8158 - val_loss: 0.6964 - val_accuracy: 0.5000\n",
      "Epoch 731/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5383 - accuracy: 0.8421 - val_loss: 0.6978 - val_accuracy: 0.5000\n",
      "Epoch 732/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5576 - accuracy: 0.7368 - val_loss: 0.6996 - val_accuracy: 0.5000\n",
      "Epoch 733/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5654 - accuracy: 0.7368 - val_loss: 0.7011 - val_accuracy: 0.5000\n",
      "Epoch 734/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6155 - accuracy: 0.7368 - val_loss: 0.7016 - val_accuracy: 0.5000\n",
      "Epoch 735/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5743 - accuracy: 0.7368 - val_loss: 0.7016 - val_accuracy: 0.5000\n",
      "Epoch 736/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5542 - accuracy: 0.7895 - val_loss: 0.7018 - val_accuracy: 0.5000\n",
      "Epoch 737/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5409 - accuracy: 0.7895 - val_loss: 0.7029 - val_accuracy: 0.5000\n",
      "Epoch 738/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5885 - accuracy: 0.7105 - val_loss: 0.7040 - val_accuracy: 0.5000\n",
      "Epoch 739/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5891 - accuracy: 0.7895 - val_loss: 0.7048 - val_accuracy: 0.5000\n",
      "Epoch 740/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5755 - accuracy: 0.6842 - val_loss: 0.7056 - val_accuracy: 0.5000\n",
      "Epoch 741/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5155 - accuracy: 0.7895 - val_loss: 0.7059 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 742/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4890 - accuracy: 0.8158 - val_loss: 0.7058 - val_accuracy: 0.5000\n",
      "Epoch 743/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5417 - accuracy: 0.7632 - val_loss: 0.7047 - val_accuracy: 0.5000\n",
      "Epoch 744/1000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.4880 - accuracy: 0.7632 - val_loss: 0.7036 - val_accuracy: 0.5000\n",
      "Epoch 745/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5434 - accuracy: 0.7895 - val_loss: 0.7028 - val_accuracy: 0.5000\n",
      "Epoch 746/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5046 - accuracy: 0.7895 - val_loss: 0.7016 - val_accuracy: 0.5000\n",
      "Epoch 747/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5435 - accuracy: 0.7895 - val_loss: 0.7013 - val_accuracy: 0.5000\n",
      "Epoch 748/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5171 - accuracy: 0.7895 - val_loss: 0.7010 - val_accuracy: 0.5000\n",
      "Epoch 749/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5266 - accuracy: 0.7368 - val_loss: 0.7003 - val_accuracy: 0.5000\n",
      "Epoch 750/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5748 - accuracy: 0.7368 - val_loss: 0.7000 - val_accuracy: 0.5000\n",
      "Epoch 751/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5520 - accuracy: 0.7368 - val_loss: 0.6995 - val_accuracy: 0.5000\n",
      "Epoch 752/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5569 - accuracy: 0.7105 - val_loss: 0.6983 - val_accuracy: 0.5000\n",
      "Epoch 753/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5455 - accuracy: 0.7368 - val_loss: 0.6972 - val_accuracy: 0.5000\n",
      "Epoch 754/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5700 - accuracy: 0.7895 - val_loss: 0.6958 - val_accuracy: 0.5000\n",
      "Epoch 755/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5615 - accuracy: 0.7105 - val_loss: 0.6947 - val_accuracy: 0.5000\n",
      "Epoch 756/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5711 - accuracy: 0.7368 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 757/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5314 - accuracy: 0.7895 - val_loss: 0.6913 - val_accuracy: 0.5000\n",
      "Epoch 758/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5739 - accuracy: 0.6579 - val_loss: 0.6896 - val_accuracy: 0.5000\n",
      "Epoch 759/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5211 - accuracy: 0.7368 - val_loss: 0.6889 - val_accuracy: 0.5000\n",
      "Epoch 760/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4843 - accuracy: 0.7105 - val_loss: 0.6904 - val_accuracy: 0.5000\n",
      "Epoch 761/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4916 - accuracy: 0.7895 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 762/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5913 - accuracy: 0.7368 - val_loss: 0.6971 - val_accuracy: 0.5000\n",
      "Epoch 763/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5476 - accuracy: 0.7105 - val_loss: 0.7006 - val_accuracy: 0.5000\n",
      "Epoch 764/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5402 - accuracy: 0.7632 - val_loss: 0.7039 - val_accuracy: 0.5000\n",
      "Epoch 765/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5121 - accuracy: 0.7632 - val_loss: 0.7076 - val_accuracy: 0.5000\n",
      "Epoch 766/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5142 - accuracy: 0.7632 - val_loss: 0.7115 - val_accuracy: 0.5000\n",
      "Epoch 767/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5119 - accuracy: 0.8158 - val_loss: 0.7152 - val_accuracy: 0.5000\n",
      "Epoch 768/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5336 - accuracy: 0.7632 - val_loss: 0.7179 - val_accuracy: 0.5000\n",
      "Epoch 769/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5911 - accuracy: 0.7105 - val_loss: 0.7203 - val_accuracy: 0.5000\n",
      "Epoch 770/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5166 - accuracy: 0.7368 - val_loss: 0.7230 - val_accuracy: 0.5000\n",
      "Epoch 771/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5249 - accuracy: 0.7368 - val_loss: 0.7257 - val_accuracy: 0.5000\n",
      "Epoch 772/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5227 - accuracy: 0.7368 - val_loss: 0.7286 - val_accuracy: 0.5000\n",
      "Epoch 773/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5247 - accuracy: 0.8158 - val_loss: 0.7317 - val_accuracy: 0.5000\n",
      "Epoch 774/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4917 - accuracy: 0.8158 - val_loss: 0.7340 - val_accuracy: 0.5000\n",
      "Epoch 775/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5456 - accuracy: 0.7632 - val_loss: 0.7358 - val_accuracy: 0.5000\n",
      "Epoch 776/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5498 - accuracy: 0.7632 - val_loss: 0.7379 - val_accuracy: 0.5000\n",
      "Epoch 777/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5351 - accuracy: 0.6842 - val_loss: 0.7391 - val_accuracy: 0.5000\n",
      "Epoch 778/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5438 - accuracy: 0.7632 - val_loss: 0.7394 - val_accuracy: 0.5000\n",
      "Epoch 779/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4495 - accuracy: 0.8684 - val_loss: 0.7403 - val_accuracy: 0.5000\n",
      "Epoch 780/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5382 - accuracy: 0.6842 - val_loss: 0.7401 - val_accuracy: 0.5000\n",
      "Epoch 781/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5617 - accuracy: 0.7895 - val_loss: 0.7387 - val_accuracy: 0.5000\n",
      "Epoch 782/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5350 - accuracy: 0.8158 - val_loss: 0.7383 - val_accuracy: 0.5000\n",
      "Epoch 783/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5965 - accuracy: 0.7368 - val_loss: 0.7388 - val_accuracy: 0.5000\n",
      "Epoch 784/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5253 - accuracy: 0.8421 - val_loss: 0.7402 - val_accuracy: 0.5000\n",
      "Epoch 785/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5309 - accuracy: 0.7105 - val_loss: 0.7410 - val_accuracy: 0.5000\n",
      "Epoch 786/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5394 - accuracy: 0.7632 - val_loss: 0.7407 - val_accuracy: 0.5000\n",
      "Epoch 787/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5144 - accuracy: 0.7895 - val_loss: 0.7405 - val_accuracy: 0.5000\n",
      "Epoch 788/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4390 - accuracy: 0.8421 - val_loss: 0.7410 - val_accuracy: 0.5000\n",
      "Epoch 789/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4614 - accuracy: 0.8421 - val_loss: 0.7419 - val_accuracy: 0.5000\n",
      "Epoch 790/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4846 - accuracy: 0.8158 - val_loss: 0.7419 - val_accuracy: 0.5000\n",
      "Epoch 791/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5144 - accuracy: 0.7632 - val_loss: 0.7419 - val_accuracy: 0.5000\n",
      "Epoch 792/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4455 - accuracy: 0.8421 - val_loss: 0.7422 - val_accuracy: 0.5000\n",
      "Epoch 793/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5154 - accuracy: 0.7632 - val_loss: 0.7423 - val_accuracy: 0.5000\n",
      "Epoch 794/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4679 - accuracy: 0.8421 - val_loss: 0.7416 - val_accuracy: 0.5000\n",
      "Epoch 795/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5616 - accuracy: 0.6842 - val_loss: 0.7416 - val_accuracy: 0.5000\n",
      "Epoch 796/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6067 - accuracy: 0.6579 - val_loss: 0.7433 - val_accuracy: 0.5000\n",
      "Epoch 797/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5020 - accuracy: 0.8421 - val_loss: 0.7461 - val_accuracy: 0.5000\n",
      "Epoch 798/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4729 - accuracy: 0.8158 - val_loss: 0.7496 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 799/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4981 - accuracy: 0.7632 - val_loss: 0.7527 - val_accuracy: 0.5000\n",
      "Epoch 800/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5519 - accuracy: 0.7895 - val_loss: 0.7551 - val_accuracy: 0.5000\n",
      "Epoch 801/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4661 - accuracy: 0.8158 - val_loss: 0.7580 - val_accuracy: 0.5000\n",
      "Epoch 802/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6542 - accuracy: 0.6579 - val_loss: 0.7606 - val_accuracy: 0.5000\n",
      "Epoch 803/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4710 - accuracy: 0.8158 - val_loss: 0.7625 - val_accuracy: 0.5000\n",
      "Epoch 804/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5053 - accuracy: 0.7895 - val_loss: 0.7630 - val_accuracy: 0.5000\n",
      "Epoch 805/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5189 - accuracy: 0.7632 - val_loss: 0.7631 - val_accuracy: 0.5000\n",
      "Epoch 806/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5052 - accuracy: 0.7895 - val_loss: 0.7630 - val_accuracy: 0.5000\n",
      "Epoch 807/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5037 - accuracy: 0.7895 - val_loss: 0.7624 - val_accuracy: 0.5000\n",
      "Epoch 808/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4421 - accuracy: 0.8158 - val_loss: 0.7614 - val_accuracy: 0.5000\n",
      "Epoch 809/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5215 - accuracy: 0.7632 - val_loss: 0.7607 - val_accuracy: 0.5000\n",
      "Epoch 810/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5414 - accuracy: 0.7632 - val_loss: 0.7601 - val_accuracy: 0.5000\n",
      "Epoch 811/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5648 - accuracy: 0.7632 - val_loss: 0.7603 - val_accuracy: 0.5000\n",
      "Epoch 812/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5438 - accuracy: 0.8421 - val_loss: 0.7609 - val_accuracy: 0.5000\n",
      "Epoch 813/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5009 - accuracy: 0.7895 - val_loss: 0.7617 - val_accuracy: 0.5000\n",
      "Epoch 814/1000\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.5132 - accuracy: 0.8158 - val_loss: 0.7610 - val_accuracy: 0.5000\n",
      "Epoch 815/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6008 - accuracy: 0.7632 - val_loss: 0.7587 - val_accuracy: 0.5000\n",
      "Epoch 816/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5029 - accuracy: 0.7368 - val_loss: 0.7557 - val_accuracy: 0.5000\n",
      "Epoch 817/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5046 - accuracy: 0.8158 - val_loss: 0.7522 - val_accuracy: 0.5000\n",
      "Epoch 818/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5180 - accuracy: 0.7895 - val_loss: 0.7489 - val_accuracy: 0.5000\n",
      "Epoch 819/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5033 - accuracy: 0.8421 - val_loss: 0.7468 - val_accuracy: 0.5000\n",
      "Epoch 820/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4482 - accuracy: 0.8421 - val_loss: 0.7458 - val_accuracy: 0.5000\n",
      "Epoch 821/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5299 - accuracy: 0.7105 - val_loss: 0.7463 - val_accuracy: 0.5000\n",
      "Epoch 822/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4698 - accuracy: 0.8158 - val_loss: 0.7465 - val_accuracy: 0.5000\n",
      "Epoch 823/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5317 - accuracy: 0.7632 - val_loss: 0.7463 - val_accuracy: 0.5000\n",
      "Epoch 824/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4895 - accuracy: 0.7632 - val_loss: 0.7462 - val_accuracy: 0.5000\n",
      "Epoch 825/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4953 - accuracy: 0.7632 - val_loss: 0.7466 - val_accuracy: 0.5000\n",
      "Epoch 826/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4659 - accuracy: 0.8421 - val_loss: 0.7479 - val_accuracy: 0.5000\n",
      "Epoch 827/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4935 - accuracy: 0.7895 - val_loss: 0.7485 - val_accuracy: 0.5000\n",
      "Epoch 828/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4821 - accuracy: 0.8158 - val_loss: 0.7478 - val_accuracy: 0.5000\n",
      "Epoch 829/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4505 - accuracy: 0.8421 - val_loss: 0.7472 - val_accuracy: 0.5000\n",
      "Epoch 830/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4865 - accuracy: 0.8158 - val_loss: 0.7475 - val_accuracy: 0.5000\n",
      "Epoch 831/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4584 - accuracy: 0.7632 - val_loss: 0.7485 - val_accuracy: 0.5000\n",
      "Epoch 832/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4976 - accuracy: 0.8947 - val_loss: 0.7501 - val_accuracy: 0.5000\n",
      "Epoch 833/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5137 - accuracy: 0.7368 - val_loss: 0.7522 - val_accuracy: 0.5000\n",
      "Epoch 834/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5591 - accuracy: 0.7632 - val_loss: 0.7544 - val_accuracy: 0.5000\n",
      "Epoch 835/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4146 - accuracy: 0.8684 - val_loss: 0.7569 - val_accuracy: 0.5000\n",
      "Epoch 836/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4997 - accuracy: 0.7368 - val_loss: 0.7601 - val_accuracy: 0.5000\n",
      "Epoch 837/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4330 - accuracy: 0.8158 - val_loss: 0.7645 - val_accuracy: 0.5000\n",
      "Epoch 838/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4896 - accuracy: 0.7895 - val_loss: 0.7690 - val_accuracy: 0.5000\n",
      "Epoch 839/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5277 - accuracy: 0.7632 - val_loss: 0.7723 - val_accuracy: 0.5000\n",
      "Epoch 840/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5545 - accuracy: 0.7895 - val_loss: 0.7744 - val_accuracy: 0.5000\n",
      "Epoch 841/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5124 - accuracy: 0.7632 - val_loss: 0.7767 - val_accuracy: 0.5000\n",
      "Epoch 842/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5288 - accuracy: 0.6579 - val_loss: 0.7801 - val_accuracy: 0.5000\n",
      "Epoch 843/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4808 - accuracy: 0.8158 - val_loss: 0.7843 - val_accuracy: 0.5000\n",
      "Epoch 844/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4549 - accuracy: 0.8158 - val_loss: 0.7884 - val_accuracy: 0.5000\n",
      "Epoch 845/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5538 - accuracy: 0.7895 - val_loss: 0.7916 - val_accuracy: 0.5000\n",
      "Epoch 846/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4783 - accuracy: 0.8684 - val_loss: 0.7955 - val_accuracy: 0.5000\n",
      "Epoch 847/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5381 - accuracy: 0.7632 - val_loss: 0.7990 - val_accuracy: 0.5000\n",
      "Epoch 848/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4840 - accuracy: 0.7632 - val_loss: 0.8012 - val_accuracy: 0.5000\n",
      "Epoch 849/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5262 - accuracy: 0.7632 - val_loss: 0.8025 - val_accuracy: 0.5000\n",
      "Epoch 850/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5391 - accuracy: 0.7895 - val_loss: 0.8044 - val_accuracy: 0.5000\n",
      "Epoch 851/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4556 - accuracy: 0.7895 - val_loss: 0.8071 - val_accuracy: 0.5000\n",
      "Epoch 852/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4902 - accuracy: 0.7895 - val_loss: 0.8091 - val_accuracy: 0.5000\n",
      "Epoch 853/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5390 - accuracy: 0.7368 - val_loss: 0.8110 - val_accuracy: 0.5000\n",
      "Epoch 854/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5047 - accuracy: 0.8421 - val_loss: 0.8137 - val_accuracy: 0.5000\n",
      "Epoch 855/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4666 - accuracy: 0.7368 - val_loss: 0.8170 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 856/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5042 - accuracy: 0.7632 - val_loss: 0.8199 - val_accuracy: 0.5000\n",
      "Epoch 857/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5128 - accuracy: 0.7632 - val_loss: 0.8217 - val_accuracy: 0.5000\n",
      "Epoch 858/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4998 - accuracy: 0.7632 - val_loss: 0.8233 - val_accuracy: 0.5000\n",
      "Epoch 859/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5351 - accuracy: 0.7632 - val_loss: 0.8248 - val_accuracy: 0.5000\n",
      "Epoch 860/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4251 - accuracy: 0.8947 - val_loss: 0.8264 - val_accuracy: 0.5000\n",
      "Epoch 861/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5455 - accuracy: 0.7632 - val_loss: 0.8271 - val_accuracy: 0.5000\n",
      "Epoch 862/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5804 - accuracy: 0.7105 - val_loss: 0.8265 - val_accuracy: 0.5000\n",
      "Epoch 863/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4675 - accuracy: 0.7895 - val_loss: 0.8249 - val_accuracy: 0.5000\n",
      "Epoch 864/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4198 - accuracy: 0.8421 - val_loss: 0.8236 - val_accuracy: 0.5000\n",
      "Epoch 865/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5560 - accuracy: 0.7895 - val_loss: 0.8228 - val_accuracy: 0.5000\n",
      "Epoch 866/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5808 - accuracy: 0.6842 - val_loss: 0.8228 - val_accuracy: 0.5000\n",
      "Epoch 867/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5036 - accuracy: 0.7632 - val_loss: 0.8230 - val_accuracy: 0.5000\n",
      "Epoch 868/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4489 - accuracy: 0.8421 - val_loss: 0.8232 - val_accuracy: 0.5000\n",
      "Epoch 869/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5521 - accuracy: 0.7895 - val_loss: 0.8245 - val_accuracy: 0.5000\n",
      "Epoch 870/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4654 - accuracy: 0.8158 - val_loss: 0.8253 - val_accuracy: 0.5000\n",
      "Epoch 871/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.6003 - accuracy: 0.6842 - val_loss: 0.8239 - val_accuracy: 0.5000\n",
      "Epoch 872/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5256 - accuracy: 0.8158 - val_loss: 0.8218 - val_accuracy: 0.5000\n",
      "Epoch 873/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5269 - accuracy: 0.7368 - val_loss: 0.8195 - val_accuracy: 0.5000\n",
      "Epoch 874/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5205 - accuracy: 0.7895 - val_loss: 0.8179 - val_accuracy: 0.5000\n",
      "Epoch 875/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5278 - accuracy: 0.7632 - val_loss: 0.8146 - val_accuracy: 0.5000\n",
      "Epoch 876/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5535 - accuracy: 0.8158 - val_loss: 0.8115 - val_accuracy: 0.5000\n",
      "Epoch 877/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4447 - accuracy: 0.8947 - val_loss: 0.8100 - val_accuracy: 0.5000\n",
      "Epoch 878/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4184 - accuracy: 0.8421 - val_loss: 0.8073 - val_accuracy: 0.5000\n",
      "Epoch 879/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5340 - accuracy: 0.8421 - val_loss: 0.8045 - val_accuracy: 0.5000\n",
      "Epoch 880/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5256 - accuracy: 0.7895 - val_loss: 0.8017 - val_accuracy: 0.5000\n",
      "Epoch 881/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5139 - accuracy: 0.7368 - val_loss: 0.8007 - val_accuracy: 0.5000\n",
      "Epoch 882/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4851 - accuracy: 0.8158 - val_loss: 0.8010 - val_accuracy: 0.5000\n",
      "Epoch 883/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5651 - accuracy: 0.6842 - val_loss: 0.8014 - val_accuracy: 0.5000\n",
      "Epoch 884/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4740 - accuracy: 0.8421 - val_loss: 0.8034 - val_accuracy: 0.5000\n",
      "Epoch 885/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4739 - accuracy: 0.7632 - val_loss: 0.8061 - val_accuracy: 0.5000\n",
      "Epoch 886/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4449 - accuracy: 0.8684 - val_loss: 0.8081 - val_accuracy: 0.5000\n",
      "Epoch 887/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4756 - accuracy: 0.7632 - val_loss: 0.8093 - val_accuracy: 0.5000\n",
      "Epoch 888/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5084 - accuracy: 0.7632 - val_loss: 0.8115 - val_accuracy: 0.5000\n",
      "Epoch 889/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4294 - accuracy: 0.8947 - val_loss: 0.8154 - val_accuracy: 0.5000\n",
      "Epoch 890/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5170 - accuracy: 0.8684 - val_loss: 0.8180 - val_accuracy: 0.5000\n",
      "Epoch 891/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4434 - accuracy: 0.8421 - val_loss: 0.8198 - val_accuracy: 0.5000\n",
      "Epoch 892/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4969 - accuracy: 0.7632 - val_loss: 0.8215 - val_accuracy: 0.5000\n",
      "Epoch 893/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5077 - accuracy: 0.7632 - val_loss: 0.8226 - val_accuracy: 0.5000\n",
      "Epoch 894/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4375 - accuracy: 0.8421 - val_loss: 0.8239 - val_accuracy: 0.5000\n",
      "Epoch 895/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4998 - accuracy: 0.7368 - val_loss: 0.8267 - val_accuracy: 0.5000\n",
      "Epoch 896/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5665 - accuracy: 0.7895 - val_loss: 0.8296 - val_accuracy: 0.5000\n",
      "Epoch 897/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4848 - accuracy: 0.8158 - val_loss: 0.8322 - val_accuracy: 0.5000\n",
      "Epoch 898/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4919 - accuracy: 0.7632 - val_loss: 0.8346 - val_accuracy: 0.5000\n",
      "Epoch 899/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4591 - accuracy: 0.8158 - val_loss: 0.8350 - val_accuracy: 0.5000\n",
      "Epoch 900/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.5523 - accuracy: 0.7105 - val_loss: 0.8348 - val_accuracy: 0.5000\n",
      "Epoch 901/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4219 - accuracy: 0.7895 - val_loss: 0.8343 - val_accuracy: 0.5000\n",
      "Epoch 902/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.5080 - accuracy: 0.7105 - val_loss: 0.8333 - val_accuracy: 0.5000\n",
      "Epoch 903/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4638 - accuracy: 0.8158 - val_loss: 0.8330 - val_accuracy: 0.5000\n",
      "Epoch 904/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.5475 - accuracy: 0.8158 - val_loss: 0.8330 - val_accuracy: 0.5000\n",
      "Epoch 905/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4803 - accuracy: 0.7895 - val_loss: 0.8320 - val_accuracy: 0.5000\n",
      "Epoch 906/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5931 - accuracy: 0.6842 - val_loss: 0.8322 - val_accuracy: 0.5000\n",
      "Epoch 907/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.6014 - accuracy: 0.7105 - val_loss: 0.8333 - val_accuracy: 0.5000\n",
      "Epoch 908/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5258 - accuracy: 0.7895 - val_loss: 0.8330 - val_accuracy: 0.5000\n",
      "Epoch 909/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5449 - accuracy: 0.7368 - val_loss: 0.8331 - val_accuracy: 0.5000\n",
      "Epoch 910/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5141 - accuracy: 0.7895 - val_loss: 0.8337 - val_accuracy: 0.5000\n",
      "Epoch 911/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4267 - accuracy: 0.8684 - val_loss: 0.8344 - val_accuracy: 0.5000\n",
      "Epoch 912/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5016 - accuracy: 0.8158 - val_loss: 0.8352 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 913/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4884 - accuracy: 0.8158 - val_loss: 0.8347 - val_accuracy: 0.5000\n",
      "Epoch 914/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5104 - accuracy: 0.7632 - val_loss: 0.8345 - val_accuracy: 0.5000\n",
      "Epoch 915/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4939 - accuracy: 0.8158 - val_loss: 0.8354 - val_accuracy: 0.5000\n",
      "Epoch 916/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5352 - accuracy: 0.7368 - val_loss: 0.8379 - val_accuracy: 0.5000\n",
      "Epoch 917/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5327 - accuracy: 0.7368 - val_loss: 0.8408 - val_accuracy: 0.5000\n",
      "Epoch 918/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4605 - accuracy: 0.8158 - val_loss: 0.8419 - val_accuracy: 0.5000\n",
      "Epoch 919/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5407 - accuracy: 0.7368 - val_loss: 0.8410 - val_accuracy: 0.5000\n",
      "Epoch 920/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4826 - accuracy: 0.8684 - val_loss: 0.8394 - val_accuracy: 0.5000\n",
      "Epoch 921/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5024 - accuracy: 0.8684 - val_loss: 0.8370 - val_accuracy: 0.5000\n",
      "Epoch 922/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5385 - accuracy: 0.7368 - val_loss: 0.8354 - val_accuracy: 0.5000\n",
      "Epoch 923/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5022 - accuracy: 0.7632 - val_loss: 0.8346 - val_accuracy: 0.5000\n",
      "Epoch 924/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4953 - accuracy: 0.7632 - val_loss: 0.8350 - val_accuracy: 0.5000\n",
      "Epoch 925/1000\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 0.5010 - accuracy: 0.8158 - val_loss: 0.8358 - val_accuracy: 0.5000\n",
      "Epoch 926/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.3915 - accuracy: 0.8421 - val_loss: 0.8371 - val_accuracy: 0.5000\n",
      "Epoch 927/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4437 - accuracy: 0.8158 - val_loss: 0.8384 - val_accuracy: 0.5000\n",
      "Epoch 928/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4695 - accuracy: 0.7895 - val_loss: 0.8391 - val_accuracy: 0.5000\n",
      "Epoch 929/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4531 - accuracy: 0.8158 - val_loss: 0.8415 - val_accuracy: 0.5000\n",
      "Epoch 930/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.5063 - accuracy: 0.7895 - val_loss: 0.8463 - val_accuracy: 0.5000\n",
      "Epoch 931/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.5909 - accuracy: 0.7632 - val_loss: 0.8513 - val_accuracy: 0.5000\n",
      "Epoch 932/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4321 - accuracy: 0.8421 - val_loss: 0.8563 - val_accuracy: 0.5000\n",
      "Epoch 933/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.5355 - accuracy: 0.6842 - val_loss: 0.8615 - val_accuracy: 0.5000\n",
      "Epoch 934/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4681 - accuracy: 0.7368 - val_loss: 0.8670 - val_accuracy: 0.5000\n",
      "Epoch 935/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4212 - accuracy: 0.7895 - val_loss: 0.8712 - val_accuracy: 0.5000\n",
      "Epoch 936/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.5283 - accuracy: 0.8421 - val_loss: 0.8744 - val_accuracy: 0.5000\n",
      "Epoch 937/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4244 - accuracy: 0.8421 - val_loss: 0.8778 - val_accuracy: 0.5000\n",
      "Epoch 938/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5207 - accuracy: 0.7632 - val_loss: 0.8808 - val_accuracy: 0.5000\n",
      "Epoch 939/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4455 - accuracy: 0.7895 - val_loss: 0.8822 - val_accuracy: 0.5000\n",
      "Epoch 940/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.5629 - accuracy: 0.7895 - val_loss: 0.8804 - val_accuracy: 0.5000\n",
      "Epoch 941/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.5218 - accuracy: 0.7105 - val_loss: 0.8775 - val_accuracy: 0.5000\n",
      "Epoch 942/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4878 - accuracy: 0.8421 - val_loss: 0.8745 - val_accuracy: 0.5000\n",
      "Epoch 943/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.5343 - accuracy: 0.7368 - val_loss: 0.8726 - val_accuracy: 0.5000\n",
      "Epoch 944/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4629 - accuracy: 0.8158 - val_loss: 0.8723 - val_accuracy: 0.5000\n",
      "Epoch 945/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4548 - accuracy: 0.8421 - val_loss: 0.8722 - val_accuracy: 0.5000\n",
      "Epoch 946/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4432 - accuracy: 0.8421 - val_loss: 0.8727 - val_accuracy: 0.5000\n",
      "Epoch 947/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4796 - accuracy: 0.7895 - val_loss: 0.8727 - val_accuracy: 0.5000\n",
      "Epoch 948/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4267 - accuracy: 0.8158 - val_loss: 0.8713 - val_accuracy: 0.5000\n",
      "Epoch 949/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.5171 - accuracy: 0.7895 - val_loss: 0.8706 - val_accuracy: 0.5000\n",
      "Epoch 950/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4597 - accuracy: 0.7895 - val_loss: 0.8700 - val_accuracy: 0.5000\n",
      "Epoch 951/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.5258 - accuracy: 0.7895 - val_loss: 0.8675 - val_accuracy: 0.5000\n",
      "Epoch 952/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4714 - accuracy: 0.7632 - val_loss: 0.8646 - val_accuracy: 0.5000\n",
      "Epoch 953/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4537 - accuracy: 0.8421 - val_loss: 0.8617 - val_accuracy: 0.5000\n",
      "Epoch 954/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4532 - accuracy: 0.8158 - val_loss: 0.8597 - val_accuracy: 0.5000\n",
      "Epoch 955/1000\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 0.4813 - accuracy: 0.7105 - val_loss: 0.8594 - val_accuracy: 0.5000\n",
      "Epoch 956/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4657 - accuracy: 0.6842 - val_loss: 0.8597 - val_accuracy: 0.5000\n",
      "Epoch 957/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4634 - accuracy: 0.8421 - val_loss: 0.8607 - val_accuracy: 0.5000\n",
      "Epoch 958/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.5362 - accuracy: 0.7632 - val_loss: 0.8621 - val_accuracy: 0.5000\n",
      "Epoch 959/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4914 - accuracy: 0.8158 - val_loss: 0.8624 - val_accuracy: 0.5000\n",
      "Epoch 960/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4708 - accuracy: 0.7368 - val_loss: 0.8615 - val_accuracy: 0.5000\n",
      "Epoch 961/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5389 - accuracy: 0.7632 - val_loss: 0.8581 - val_accuracy: 0.5000\n",
      "Epoch 962/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4327 - accuracy: 0.8421 - val_loss: 0.8532 - val_accuracy: 0.5000\n",
      "Epoch 963/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5008 - accuracy: 0.7105 - val_loss: 0.8476 - val_accuracy: 0.5000\n",
      "Epoch 964/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4840 - accuracy: 0.7632 - val_loss: 0.8432 - val_accuracy: 0.5000\n",
      "Epoch 965/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4618 - accuracy: 0.7895 - val_loss: 0.8399 - val_accuracy: 0.5000\n",
      "Epoch 966/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4478 - accuracy: 0.8421 - val_loss: 0.8365 - val_accuracy: 0.5000\n",
      "Epoch 967/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4372 - accuracy: 0.8684 - val_loss: 0.8338 - val_accuracy: 0.5000\n",
      "Epoch 968/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4948 - accuracy: 0.7632 - val_loss: 0.8320 - val_accuracy: 0.5000\n",
      "Epoch 969/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5045 - accuracy: 0.7632 - val_loss: 0.8316 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 970/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4624 - accuracy: 0.8421 - val_loss: 0.8327 - val_accuracy: 0.5000\n",
      "Epoch 971/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4285 - accuracy: 0.8421 - val_loss: 0.8346 - val_accuracy: 0.5000\n",
      "Epoch 972/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4625 - accuracy: 0.7632 - val_loss: 0.8356 - val_accuracy: 0.5000\n",
      "Epoch 973/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.5102 - accuracy: 0.7895 - val_loss: 0.8377 - val_accuracy: 0.5000\n",
      "Epoch 974/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.5191 - accuracy: 0.7632 - val_loss: 0.8402 - val_accuracy: 0.5000\n",
      "Epoch 975/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4711 - accuracy: 0.8158 - val_loss: 0.8428 - val_accuracy: 0.5000\n",
      "Epoch 976/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.5246 - accuracy: 0.7105 - val_loss: 0.8465 - val_accuracy: 0.5000\n",
      "Epoch 977/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4729 - accuracy: 0.7895 - val_loss: 0.8494 - val_accuracy: 0.5000\n",
      "Epoch 978/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4949 - accuracy: 0.8158 - val_loss: 0.8527 - val_accuracy: 0.5000\n",
      "Epoch 979/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.4554 - accuracy: 0.7632 - val_loss: 0.8555 - val_accuracy: 0.5000\n",
      "Epoch 980/1000\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.5287 - accuracy: 0.7368 - val_loss: 0.8575 - val_accuracy: 0.5000\n",
      "Epoch 981/1000\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 0.5557 - accuracy: 0.7632 - val_loss: 0.8614 - val_accuracy: 0.5000\n",
      "Epoch 982/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5083 - accuracy: 0.8421 - val_loss: 0.8663 - val_accuracy: 0.5000\n",
      "Epoch 983/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5536 - accuracy: 0.7632 - val_loss: 0.8710 - val_accuracy: 0.5000\n",
      "Epoch 984/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5248 - accuracy: 0.7368 - val_loss: 0.8748 - val_accuracy: 0.5000\n",
      "Epoch 985/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5170 - accuracy: 0.7105 - val_loss: 0.8790 - val_accuracy: 0.5000\n",
      "Epoch 986/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4592 - accuracy: 0.7895 - val_loss: 0.8833 - val_accuracy: 0.5000\n",
      "Epoch 987/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4150 - accuracy: 0.8421 - val_loss: 0.8878 - val_accuracy: 0.5000\n",
      "Epoch 988/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4648 - accuracy: 0.8158 - val_loss: 0.8930 - val_accuracy: 0.5000\n",
      "Epoch 989/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5697 - accuracy: 0.7632 - val_loss: 0.8978 - val_accuracy: 0.5000\n",
      "Epoch 990/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4649 - accuracy: 0.8158 - val_loss: 0.9028 - val_accuracy: 0.5000\n",
      "Epoch 991/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5296 - accuracy: 0.8158 - val_loss: 0.9089 - val_accuracy: 0.5000\n",
      "Epoch 992/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5176 - accuracy: 0.7632 - val_loss: 0.9145 - val_accuracy: 0.5000\n",
      "Epoch 993/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4790 - accuracy: 0.7632 - val_loss: 0.9191 - val_accuracy: 0.5000\n",
      "Epoch 994/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4961 - accuracy: 0.7895 - val_loss: 0.9245 - val_accuracy: 0.5000\n",
      "Epoch 995/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4714 - accuracy: 0.7895 - val_loss: 0.9300 - val_accuracy: 0.5000\n",
      "Epoch 996/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4340 - accuracy: 0.8684 - val_loss: 0.9352 - val_accuracy: 0.5000\n",
      "Epoch 997/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5613 - accuracy: 0.7895 - val_loss: 0.9396 - val_accuracy: 0.5000\n",
      "Epoch 998/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4936 - accuracy: 0.8158 - val_loss: 0.9437 - val_accuracy: 0.5000\n",
      "Epoch 999/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.5375 - accuracy: 0.7632 - val_loss: 0.9476 - val_accuracy: 0.5000\n",
      "Epoch 1000/1000\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4866 - accuracy: 0.8421 - val_loss: 0.9503 - val_accuracy: 0.5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "mean_run_scores = []\n",
    "for i in trange(n_runs, desc='Runs'): \n",
    "    histories = []\n",
    "    \n",
    "    data = list((zip(real_data_splits_train_images, \\\n",
    "                                    real_data_splits_test_images, simulated_data_splits_train_images)))[0]\n",
    "    train_set, test_set, simulated_train_set = data[0], data[1], data[2]\n",
    "    \n",
    "    #for train_set, test_set, simulated_train_set in tqdm(zip(real_data_splits_train_images, \\\n",
    "    #                                real_data_splits_test_images, simulated_data_splits_train_images), total=20, desc='Folds'):\n",
    "        \n",
    "    X_train = train_set[0]\n",
    "    y_train = train_set[1]\n",
    "    X_test = test_set[0]\n",
    "    y_test = test_set[1]\n",
    "        \n",
    "        #print(X_test[0])\n",
    "        \n",
    "        #plt.imshow(X_test[1], origin='lower') \n",
    "        \n",
    "        # Adding simulated data. \n",
    "    X_train, y_train = add_simulated_data(X_train, y_train, simulated_train_set)   \n",
    "\n",
    "    print(X_train[0].shape)\n",
    "    print(y_train.shape) \n",
    "    \n",
    "\n",
    "    # Shuffling training data\n",
    "    temp_train = list(zip(X_train, y_train.tolist()))\n",
    "    random.shuffle(temp_train)\n",
    "    X_train, y_train = zip(*temp_train)\n",
    "\n",
    "    model = create_and_train(np.asarray(X_train), np.asarray(y_train), np.asarray(X_test), y_test)\n",
    "        \n",
    "    mean_run_score = mean_score_of_run(histories=histories, epochs=n_epochs)\n",
    "    mean_run_scores.append(mean_run_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 72, 36, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 70, 34, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 35, 17, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 35, 17, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 33, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 31, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 7, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 7, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 5, 64)         36928     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 12, 3, 64)         36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               49280     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 197,026\n",
      "Trainable params: 197,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_54 (Conv2D)           (None, 72, 36, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 70, 34, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 35, 17, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 35, 17, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_56 (Conv2D)           (None, 33, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 31, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 16, 7, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 16, 7, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 7168)              0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 128)               917632    \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 991,522\n",
      "Trainable params: 991,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_model = Sequential()\n",
    "\n",
    "test_model.add(Conv2D(32, (5, 5), input_shape=(76, 40,1), activation='relu'))\n",
    "test_model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "test_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "test_model.add(Dropout(0.5))\n",
    "\n",
    "test_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "test_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "test_model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "test_model.add(Dropout(0.5))\n",
    "\n",
    "#test_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "#test_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "#test_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#test_model.add(Dropout(0.5))\n",
    "\n",
    "test_model.add(Flatten())\n",
    "test_model.add(Dense(128, activation='relu'))\n",
    "test_model.add(Dense(64, activation='relu'))\n",
    "test_model.add(Dense(2, activation='softmax'))\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "game = real_data_splits_train[0][0][0]\n",
    "image = create_image(game, components=[True, True, True, True, True])#[True, False, False, False, False]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 40)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c330066248>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.xlabel('Step', fontsize=20)\n",
    "plt.ylabel('Dynamic card codes')\n",
    "plt.imshow(image, origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "activation_model = keras.models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "img = image.reshape(image.shape[0], image.shape[1], 1)\n",
    "img_tensor = np.expand_dims(img, axis=0)\n",
    "img_tensor /= 255.\n",
    "\n",
    "activations = activation_model.predict(img_tensor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAHuCAYAAACf/rmqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAZhklEQVR4nO3de5BU5ZnH8d/DzHAHYRCQBQxeMEq8oKHwlmSzkosxbjS1Scpk45INWSrZxNJNrI17KTdr1VYlW9kkVuVWrGaXTSVeojG6KXNxJ7rZFIqiIIgoCAFEbiLiKCjMDM/+0QdDzHlnunv6nJ6Z5/uporr7ffvt89Zx5uc75+lzjrm7AMQ1rNkTANBchAAQHCEABEcIAMERAkBwhAAQXFNCwMwuNrOnzewZM7uuSXPYbGZrzGyVma0oaZvfM7PdZvbEUW3tZnafmW3IHieWvP0vmdlz2X5YZWaXFLX9bHszzex+M1tnZmvN7Oqsvcz9kJpDafvCzEaa2cNm9ng2h3/O2k8ws+XZfrjNzIYXNYfXuXup/yS1SNoo6URJwyU9LmlOE+axWdKxJW/zHZLOkfTEUW3/Kum67Pl1kr5S8va/JOnaEvfBNEnnZM/HSVovaU7J+yE1h9L2hSSTNDZ73iZpuaTzJN0u6Yqs/buSPlP0XJqxEpgv6Rl33+TuhyTdKumyJsyjdO7+a0l739B8maSl2fOlki4vefulcvcd7v5Y9vxlSeskTVe5+yE1h9J4xSvZy7bsn0u6SNIdWXuh++GIZoTAdEnPHvV6m0r+D5BxSb80s0fNbHETtn/EVHffIVV+OCVNacIcPmdmq7M/Fwpbhr+Rmc2SdLYq/xdsyn54wxykEveFmbWY2SpJuyXdp8oKeZ+7d2dvKeV3oxkhYDltzfju8oXufo6k90n6rJm9owlzGAi+I+kkSXMl7ZD0b2Vs1MzGSrpT0jXu3lnGNquYQ6n7wt173H2upBmqrJBPy3tbkXOQmhMC2yTNPOr1DEnby56Eu2/PHndLukuV/wjNsMvMpklS9ri7zI27+67sh/GwpH9XCfvBzNpU+eX7gbv/OGsudT/kzaEZ+yLb7j5JD6hyTGCCmbVmXaX8bjQjBB6RNDs7Cjpc0hWS7ilzAmY2xszGHXku6T2Snuh9VGHukbQwe75Q0t1lbvzIL17mgyp4P5iZSbpZ0jp3/9pRXaXth9QcytwXZjbZzCZkz0dJepcqxybul/Sh7G3l/DyUcSQ058joJaockd0o6R+asP0TValKPC5pbVlzkHSLKsvMLlVWRIskTZLUIWlD9the8va/L2mNpNWq/CJOK3gfvE2VJe5qSauyf5eUvB9ScyhtX0g6U9LKbFtPSLr+qJ/NhyU9I+lHkkYU/XNp2YYBBMU3BoHgCAEgOEIACI4QAIIjBIDgmhYCTf6qLnMYQHNo9vajz6GZK4Gm73QxhyOaPYdmb18KPId+hcBAuC4AgP6p+8tCZtaiyrf+3q3Kt88ekfRRd38yNWZ4yygf1XaMJOlQzwENbxn9ep8fPFTXPGrVM2nM68+7X9uv1pG/e93ywv7cMRPf0pX8vJ0vT8htH/5Ser/aSwdef96lg2rTiOR7XzdmVLpv/6t9j+9F1XMoSLO3H2EOr2m/DvnBvJP31JrXWKXXrwsgSWZ25LoAyRAY1XaMLphxZW5f96bN/ZhK9fa9//xk34TvP5jb/md3ps9l+fKv/jS3febP0iEw8qcPJ/uSzjwz3ffQ6to/D6Es945kX3/+HBgo1wUA0A/9WQlUdV2A7IjnYkka2TquH5sDUIT+rASqui6Auy9x93nuPu/oYwAABob+hEDTrwsAoP/q/nPA3bvN7HOSfqHKFYS/5+5rex1z8JC6Nz/b21sKlzr4J0l7f3pKbvudeRd9ypx23Obc9u6du2qZVp+sJ32gccv1F+S2H3/DsvTnzTs9t91XpK+jMe7/jk32rdwyM7f95I+vTI5pmdSe297zQlOvhRpOf44JyN3vlXRvg+YCoAk4dwAIjhAAgiMEgOAIASC4fh0YrMvhntxmG5H/nWk/eDD5UT3vPCe3fet70t+//umffzXZ9+m/mpfsS2lkFaB11vHp7TyyJtl3/CO1bytVBdj6ozPS23l7eg4na0/Nc6AKMDCwEgCCIwSA4AgBIDhCAAiOEACCIwSA4EotEfa0j1Hnxefl9g3rzj9BZucH0pcdmzk1v8R0y+zbk2OuetOFyb42rUj2NdLGr+bvg5Oufaih23nl5ycm+6aPfSm3/fi3pcuAvWk55aTc9p71G+v6PJSHlQAQHCEABEcIAMERAkBwhAAQXN03H6nHeGv3c21BTWNapx2X7Hvqi7Ny20++prFH2evh55+V7LMHH8/vmJ8+eWfP3LHJviuv/llu+1UTNiXHvP+yv8ht7xqfPvmq9VePJvvq0TqDK9SXZdnOW/TSoV25Nx9hJQAERwgAwRECQHCEABAcIQAERwgAwQ34EmEollvBkSS1nHpysq9n3YYiZoMhZLl3qNP3UiIE8IcIASA4QgAIjhAAgiMEgOBKvbzY4QljdOCic3P7Rt+1vMypDEy9VGqoAKAorASA4AgBIDhCAAiOEACCIwSA4AgBILhSS4TD9u2vuRT42qXzk32vHtuS277vlPTnffB9D9a0/cHg3s1zctsn/Ne45BhKsjiClQAQHCEABEcIAMERAkBwhAAQHCEABFdqidBGjlDLSfn1u6c/1Z7bPnZLOqc653Tltk9Y3ZYcs/Kz6duDDVbHb92T29697cmaP6uekqyULssOxZLsYLTmY+k+VgJAcIQAEBwhAARHCADBEQJAcKVWB9TdI72wL7fr5M+vz223tuHJjzuu61BDpjXYdSfaW+akz6RqZDVGSldkhmI1ZjA6sGVZso+VABAcIQAERwgAwRECQHCEABAcIQAE12eJ0My+J+lSSbvd/fSsrV3SbZJmSdos6SPu/mJfn+Xd3erZtbumCTplwPolyrESJdlw/NVkVzUrgf+UdPEb2q6T1OHusyV1ZK8BDEJ9hoC7/1rS3jc0XyZpafZ8qaTLGzwvACWp95jAVHffIUnZ45TGTQlAmQr/2rCZLZa0WJJGanTRmwNQo3pXArvMbJokZY/Jo33uvsTd57n7vDaNqHNzAIpS70rgHkkLJX05e7y7YTNCw9RaiZGoxkTU50rAzG6R9KCkN5vZNjNbpMov/7vNbIOkd2evAQxCfa4E3P2jia4FDZ4LgCbgG4NAcIQAEBwhAARHCADBEQJAcIQAEBwhAARHCADBEQJAcIQAEBwhAARHCADBEQJAcIQAEBwhAARHCADBEQJAcIQAEBwhAARHCADBEQJAcIQAEBwhAARHCADBEQJAcIQAEBwhAARHCADBEQJAcIQAEBwhAARHCADBEQJAcIQAEBwhAARHCADBEQJAcIQAEBwhAARHCADBEQJAcIQAEBwhAARHCADBEQJAcIQAEBwhAARHCADBEQJAcIQAEBwhAARHCADBEQJAcIQAEBwhAARHCADBEQJAcH2GgJnNNLP7zWydma01s6uz9nYzu8/MNmSPE4ufLoBGq2Yl0C3pC+5+mqTzJH3WzOZIuk5Sh7vPltSRvQYwyPQZAu6+w90fy56/LGmdpOmSLpO0NHvbUkmXFzVJAMWp6ZiAmc2SdLak5ZKmuvsOqRIUkqYkxiw2sxVmtqJLB/s3WwANV3UImNlYSXdKusbdO6sd5+5L3H2eu89r04h65gigQFWFgJm1qRIAP3D3H2fNu8xsWtY/TdLuYqYIoEitfb3BzEzSzZLWufvXjuq6R9JCSV/OHu/u87Pa2tR63PQ6p4oy+CuvJPt69r1U4kxQlj5DQNKFkq6UtMbMVmVtf6/KL//tZrZI0lZJHy5migCK1GcIuPtvJFmie0FjpwOgbHxjEAiOEACCIwSA4AgBILhqqgMN411d6t72XJmbBNAHVgJAcIQAEBwhAARHCADBEQJAcKVWBxCQpb5xLrXMOSW3vWft00XNBjlYCQDBEQJAcIQAEBwhAARHCADBEQJAcJQIUSh761uSfT0rnqh5jD+6tt9zwu9jJQAERwgAwRECQHCEABAcIQAEV2p1YPQcae4Py9wianXv5jnpzgcnJLsmru/ObR/1k4eTY16+4rzc9nG3PpQcQ+Wg8VgJAMERAkBwhAAQHCEABEcIAMERAkBw5u6lbWz82Ok+f+5nStseate2dU+yb6DfPSpVPqR0KC33DnX63twLPrISAIIjBIDgCAEgOEIACI4QAIIr9/Ji+1+VLXu81E2iNvmnAVW0TJyY7Os6c1Zu+7D/XZn+vMmTc9t7nn++l1mkpaoAnHTUO1YCQHCEABAcIQAERwgAwRECQHCEABAcdyBC9SalrzHY0nkot33XX1+QHDPl28ty21tPnJUc071pc7IvpbcyICcdsRIAwiMEgOAIASA4QgAIjhAAgiMEgOAoEaJqPc/8NtnXcvIJue1TH0x/Xqp8mCodSuWVDyOdechKAAiOEACCIwSA4AgBIDhCAAiuz+qAmY2U9GtJI7L33+Hu/2RmJ0i6VVK7pMckXenu+WeRYMhLVQ5SVQMpXTmo56QjKV054KSj3lWzEjgo6SJ3P0vSXEkXm9l5kr4i6evuPlvSi5IWFTdNAEXpMwS84pXsZVv2zyVdJOmOrH2ppMsLmSGAQlV1TMDMWsxslaTdku6TtFHSPnc/coXqbZKmFzNFAEWqKgTcvcfd50qaIWm+pNPy3pY31swWm9kKM1vRpYP1zxRAIWqqDrj7PkkPSDpP0gQzO3JgcYak7YkxS9x9nrvPa9OI/swVQAH6DAEzm2xmE7LnoyS9S9I6SfdL+lD2toWS7i5qkgCKY+65q/jfvcHsTFUO/LWoEhq3u/sNZnaiflciXCnp4+7e63p/vLX7ubagIRPH4JcqHx4eNzo5Ztf5xyT7yrpmYcpAPulouXeo0/daXl+f3xNw99WSzs5p36TK8QEAgxjfGASCIwSA4AgBIDhCAAiOy4uhaRp50pHU2MuVlXXSUV/jysBKAAiOEACCIwSA4AgBIDhCAAiOEACCo0SIAaeeOx1Jjb1mYVl3OpKaf81CVgJAcIQAEBwhAARHCADBEQJAcFQHMKjUUzkYyCcdSekqQFknHbESAIIjBIDgCAEgOEIACI4QAIIjBIDgKBFiyGjkNQsjnXTESgAIjhAAgiMEgOAIASA4QgAIjhAAgjN3L21j463dz7UFpW0P6EuqfHh43OjkmF3nH5PbXs+Zh1L9Zx/mSZUOH3pyiTr3b7e8PlYCQHCEABAcIQAERwgAwRECQHBUB4AcvZ10lKocpKoGUrnXLMyz3DvU6XupDgD4Q4QAEBwhAARHCADBEQJAcIQAEBzXGARylHW7M6m8axamsBIAgiMEgOAIASA4QgAIjhAAgqM6ANSokXc6ktKVg7JOOmIlAARHCADBEQJAcIQAEBwhAARHCADBVV0iNLMWSSskPeful5rZCZJuldQu6TFJV7r7oWKmCQx89Zx0JKXLh4086ci2DU9+Vi0rgaslrTvq9Vckfd3dZ0t6UdKiGj4LwABRVQiY2QxJ75d0U/baJF0k6Y7sLUslXV7EBAEUq9qVwDck/a2kw9nrSZL2uXt39nqbpOkNnhuAEvQZAmZ2qaTd7v7o0c05b829gYGZLTazFWa2oksH65wmgKJUc2DwQkkfMLNLJI2UNF6VlcEEM2vNVgMzJG3PG+zuSyQtkSo3H2nIrAE0TE13IDKzd0q6NqsO/EjSne5+q5l9V9Jqd/92b+O5AxHwh1KVg9SdjqT03Y5SVYOi7kD0RUmfN7NnVDlGcHM/PgtAk9R0KrG7PyDpgez5JknzGz8lAGXiG4NAcIQAEBwhAARHCADBcY1BoMkaec3C1ElH3bc/lPwsVgJAcIQAEBwhAARHCADBEQJAcIQAEBwlQmCAqueahanS4W/39yQ/i5UAEBwhAARHCADBEQJAcIQAEBzVAWAQqvWkI+s5nNsusRIAwiMEgOAIASA4QgAIjhAAgiMEgOAoEQJDSKp06H4oOYaVABAcIQAERwgAwRECQHCEABAcIQAERwgAwRECQHCEABAcIQAERwgAwRECQHCEABAcIQAERwgAwRECQHCEABAcIQAEx+XF8HtsxIhk37BZM5N9nWdMym0fuacrOWb4Y8/kz2H8uOQY1Md2tiX7WAkAwRECQHCEABAcIQAERwgAwRECQHCUCIew3sp9Le0Tc9u7d+5Kjul5Or+kJ0ljeulLfl6qo7Oz5s9C79zTpVpWAkBwhAAQHCEABEcIAMERAkBwhAAQXFUlQjPbLOllVao63e4+z8zaJd0maZakzZI+4u4vFjNNNLrc171jZ7/nhKGhlpXAn7j7XHefl72+TlKHu8+W1JG9BjDI9OfPgcskLc2eL5V0ef+nA6Bs1YaAS/qlmT1qZouztqnuvkOSsscpeQPNbLGZrTCzFV062P8ZA2ioar82fKG7bzezKZLuM7Onqt2Auy+RtESSxlu71zFHAAWqaiXg7tuzx92S7pI0X9IuM5smSdnj7qImCaA4fa4EzGyMpGHu/nL2/D2SbpB0j6SFkr6cPd5d5ESHktSR/tRRfokj/ShONX8OTJV0l5kdef8P3f3nZvaIpNvNbJGkrZI+XNw0ARSlzxBw902Szsppf0HSgiImBaA8fGMQCI4QAIIjBIDgCAEgOK4xeJRU6a6e22+N2XogOcYfXpPb3lupr2Xy5GRfz/PPJ/saafu1FyT7Pv3J/85t79hzanLM6Nb8695NH7WvtomhT2s+lu5jJQAERwgAwRECQHCEABAcIQAENySrA/VciktKn6TT6DvvtJxyUv521m9Mjml0BWDr9flH+m/6xDeTY27cnl8JkaQbf3JpbnvXlPSdb8Y+NTy3ffey/ckxqM+BLcuSfawEgOAIASA4QgAIjhAAgiMEgOAIASA4cy/vAsDjh03y80a8L7cvdZJO6gQdqZeTdBIn6PQldZJOWSfo1Kuect81T16R2z75b7qTY3o2bKptYhgwlnuHOn2v5fWxEgCCIwSA4AgBIDhCAAiOEACCK7c6YO1+rhV/q4LUCTpS7yfpNN38M5Jds7+1Ptm3fNebcts50o8jqA4ASCIEgOAIASA4QgAIjhAAgiMEgOCG5DUGG10GrOfOO8tfOiE55o8n5Jf79va8nBzzy0+/PdnX/ptVue09yRH16XrXW5N9w/cdzG0ftn5rcoyNH9fvOaE6trMt2cdKAAiOEACCIwSA4AgBIDhCAAiOEACCG5Ilwl7VcabeMQefTo5J3X6rN8/fcCi3/fBrryXHDFN+GbBMbf/zaLIvdS5qr2XKzs7+TAc1cE/fDo6VABAcIQAERwgAwRECQHCEABDcgKkOtJw2O7e965vpI+bTRr+U2546QUeq7ySdYYkTdCTpBD2Y7Es5XPMIoDisBIDgCAEgOEIACI4QAIIjBIDgCAEguFJLhNNOP6C/u2d1bl+P1uS2f+q+RcnP27J3Rm576gQdaWCcpDNszJjc9sMHDqQHlXi7OMTCSgAIjhAAgiMEgOAIASA4QgAIzryKo85mNkHSTZJOV+VKUp+U9LSk2yTNkrRZ0kfc/cXePme8tfu5tqB/Mx5ghtqR/nrutiRJHXtOzW0f3Zq+rNX0Ufuqnxj65fsf69DOtXstr6/alcCNkn7u7qdKOkvSOknXSepw99mSOrLXAAaZPkPAzMZLeoekmyXJ3Q+5+z5Jl0lamr1tqaTLi5okgOJUsxI4UdLzkv7DzFaa2U1mNkbSVHffIUnZ45QC5wmgINWEQKukcyR9x93PlrRfNSz9zWyxma0wsxVdyr9zLYDmqSYEtkna5u7Ls9d3qBIKu8xsmiRlj7vzBrv7Enef5+7z2jSiEXMG0EB9hoC775T0rJm9OWtaIOlJSfdIWpi1LZR0dyEzBFCoak8gukrSD8xsuKRNkv5SlQC53cwWSdoq6cPFTLGxbER6NdLSPjG3vXvnruSYw/v393tORdl6fbrcd9MnvpnbfuP2Sckxvd1tqWtKfilw7FPDk2N2Lxu4+26oObBlWbKvqhBw91WS5uV0Da2iPxAQ3xgEgiMEgOAIASA4QgAIbsDcgageqSP9qaP8Uu9H+rt37Oz3nIpSz5H+a548JTnmX977odz2ng2bkmPqudsSBgh/NdnFSgAIjhAAgiMEgOAIASA4QgAIjhAAghvUJUI/mH99gt5KfXb2W9Kft3Jtv+dUjVS5L1Xqk+or97VvWJ8c05PsQTSsBIDgCAEgOEIACI4QAIIjBIDgCAEguEFdImw55aTcdt/6XHLM4QaXARtZ7kuV+iTKfSgOKwEgOEIACI4QAIIjBIDgCAEguFKrA9bWqtbJx+X2Ja/95578vJ71Gxsxrdft/eT5ue3f+MdvJcc08kg/R/nRDKwEgOAIASA4QgAIjhAAgiMEgOAIASC4UkuE3tWdvP5f6mSgRpcBb312WbLvyo3Tctsp92EoYyUABEcIAMERAkBwhAAQHCEABFfuCURmGjZyZG5fI6sAv9i+Ktl3yRmXJvt6XtjRsDkAgwUrASA4QgAIjhAAgiMEgOAIASA4QgAIrtwTiNx1+LXXahrzwqL86/5J0t63H8ptf+8f9faJe2vaPjDUsRIAgiMEgOAIASA4QgAIjhAAgiu1OtCbLzyzNrf9qtvS1YHZn3g0t7115ozkmO5nt9U2MWCIYyUABEcIAMERAkBwhAAQHCEABEcIAMGZu5e3MbPnJW3JXh4raU9pG8/HHAbGHJq9/QhzeJO7T87rKDUEfm/DZivcfV5TNs4cBtQcmr396HPgzwEgOEIACK6ZIbCkids+gjlUNHsOzd6+FHgOTTsmAGBg4M8BIDhCAAiOEACCIwSA4AgBILj/BwwEwMtKTAWXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x592.941 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x592.941 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x592.941 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x633.6 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x686.769 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x658.286 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAIhCAYAAABOqA9WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAASOUlEQVR4nO3dbYxc5XnG8etidzHY4ICBALVRDS2yktI2oG0aQKINBEIbBPnQSiARkRRppb6k0LSi0FRF7adKrUIqpaKygEAVCkp4aVDUpiASSqMCiTEQXkwIMgYcSIwhafBSbK9994Mnkm3t1sucM+fscv1/kuWdl537Wdn++5mZMzOuKgHIdVDfCwDQLyIAhCMCQDgiAIQjAkA4IgCEWxQRsH2+7e/Zft721T3Mv8n2FttP9TD7BNvftL3B9tO2r+h4/iG2v237icH8v+5y/l7rGLP9mO2v9TB7k+0nbT9ue10P84+wfYftZwd/D05v9fYX+nECtsckPSfpXEmbJX1H0iVV9UyHazhL0jZJ/1xVp3Q1dzD7eEnHV9V624dLelTSx7v6+W1b0rKq2mZ7QtK3JF1RVQ93MX+vdXxG0qSk5VV1QcezN0marKqtXc7da/4tkv6rqm6wfbCkpVX1k7ZufzHsBD4o6fmq2lhVOyTdLumiLhdQVQ9KeqPLmXvNfrWq1g++flPSBkkrO5xfVbVtcHJi8KvT/zlsr5L0MUk3dDl3IbC9XNJZkm6UpKra0WYApMURgZWSXt7r9GZ1+I9gIbG9WtKpkh7peO6Y7cclbZF0X1V1Ol/S5yVdJWl3x3N/piTda/tR21Mdzz5J0muSvji4O3SD7WVtDlgMEfAs5y3s+zAjYPswSXdKurKqftrl7KraVVUfkLRK0gdtd3aXyPYFkrZU1aNdzZzFmVV1mqTfkvSHg7uHXRmXdJqk66vqVEnTklp9XGwxRGCzpBP2Or1K0is9raUXg/vid0q6taru6msdg23oA5LO73DsmZIuHNwvv13S2ba/1OF8VdUrg9+3SLpbe+6idmWzpM177b7u0J4otGYxROA7kk62feLgQZGLJd3T85o6M3hg7kZJG6rqcz3MP8b2EYOvD5X0EUnPdjW/qq6pqlVVtVp7/uy/UVWXdjXf9rLBA7IabMPPk9TZs0RV9UNJL9teMzjrHEmtPig83uaNjUJVzdj+I0n/IWlM0k1V9XSXa7B9m6TflHS07c2Srq2qGzsaf6akT0h6cnC/XJL+oqr+raP5x0u6ZfAszUGSvlxVnT9N16NjJd29p8Ual/QvVfX1jtfwaUm3Dv4T3CjpU23e+IJ/ihDAaC2GuwMARogIAOGIABCOCADhiAAQblFFoIdDNpm/QOYn/+yjnr+oIiCp1z8I5vc6P/lnH+n8xRYBAC3r9GChsWXLanzFiqG/f9f0tMaWNXgB1WwvRXon87dNa+yw4ecfv/zHjeZv+/FOHXbkxNDff/TYTKP5r72+S8ccNTb09z+19Zihv7fxn72kg1+ZHvp7d2q7JrSk0fwmms5/W9PaUdtn/RfQ6WHD4ytWaOVnruxy5D5293yQ9F99tLfX/kiSPrl8S6/z19z0+73OX/2XD/U6v0+P1P1zXsbdASAcEQDCEQEgHBEAwhEBIBwRAMIRASAcEQDCEQEgHBEAwjWKQN8fFAqguaEjMHgL6n/Unk9leb+kS2y/v62FAehGk51A7x8UCqC5JhHgg0KBd4EmEZjXB4XanrK9zva6XdPDv54bwGg0icC8Pii0qtZW1WRVTTZ9UwgA7WsSgegPCgXeLYZ+r52F8EGhAJpr9IZbg0/G7erTcQGMAEcMAuGIABCOCADhiAAQjggA4YgAEI4IAOGIABCOCADhiAAQrtOPJn/PkuPqjFWXdjZvf2+feHRvsyXpoF27e52/8aL+Plpbkg7f1O//OQft7O7v+kLz3Feu01tbXp71o8nZCQDhiAAQjggA4YgAEI4IAOGIABCOCADhiAAQjggA4YgAEI4IAOGIABCOCADhiAAQjggA4YgAEI4IAOGIABCOCADhiAAQjggA4YgAEI4IAOGIABCOCADhiAAQjggA4YgAEI4IAOGIABCOCADhxrscVjt2aOaFF7scuY/xHmcvBL/4n32vAH3ZWNNzXsZOAAhHBIBwRAAIRwSAcEQACEcEgHBEAAhHBIBwRAAIRwSAcEQACEcEgHBDR8D2Cba/aXuD7adtX9HmwgB0o8mrCGck/WlVrbd9uKRHbd9XVc+0tDYAHRh6J1BVr1bV+sHXb0raIGllWwsD0I1WHhOwvVrSqZIeaeP2AHSn8ZuK2D5M0p2Srqyqn85y+ZSkKUk6REubjgPQskY7AdsT2hOAW6vqrtmuU1Vrq2qyqiYntKTJOAAj0OTZAUu6UdKGqvpce0sC0KUmO4EzJX1C0tm2Hx/8+u2W1gWgI0M/JlBV35LkFtcCoAccMQiEIwJAOCIAhCMCQDgiAIQjAkA4IgCEIwJAOCIAhCMCQDgiAIQjAkA4IgCEIwJAOCIAhCMCQDgiAIQjAkA4IgCEIwJAOCIAhCMCQDgiAIQjAkA4IgCEIwJAOCIAhCMCQDgiAIQjAkA4IgCEIwJAOCIAhCMCQDgiAIQjAkA4IgCEIwJAOCIAhCMCQDgiAIQjAkC48b4XgO544uBe5+/+tff1Ot///USv8xcqdgJAOCIAhCMCQDgiAIQjAkA4IgCEIwJAOCIAhCMCQDgiAIQjAkA4IgCEaxwB22O2H7P9tTYWBKBbbewErpC0oYXbAdCDRhGwvUrSxyTd0M5yAHSt6U7g85KukrS7hbUA6MHQEbB9gaQtVfXoAa43ZXud7XU7tX3YcQBGpMlO4ExJF9reJOl2SWfb/tL+V6qqtVU1WVWTE1rSYByAURg6AlV1TVWtqqrVki6W9I2qurS1lQHoBMcJAOFaeaPRqnpA0gNt3BaAbrETAMIRASAcEQDCEQEgHBEAwhEBIBwRAMIRASAcEQDCEQEgXCuHDc/X7iOX6c2PfqjLkQvKoVt39jp/6y/3+yrO3RO9jtfEr57e6/xjrn+o1/lzYScAhCMCQDgiAIQjAkA4IgCEIwJAOCIAhCMCQDgiAIQjAkA4IgCEIwJAOCIAhCMCQDgiAIQjAkA4IgCEIwJAOCIAhCMCQDgiAIQjAkA4IgCEIwJAOCIAhCMCQDgiAIQjAkA4IgCEIwJAOCIAhHNVdTZsuVfUr/uczuZhYRk/aXWv83/04eN7nd+nZ//1Or312sue7TJ2AkA4IgCEIwJAOCIAhCMCQDgiAIQjAkA4IgCEIwJAOCIAhCMCQLhGEbB9hO07bD9re4Pt09taGIBujDf8/n+Q9PWq+h3bB0ta2sKaAHRo6AjYXi7pLEmflKSq2iFpRzvLAtCVJncHTpL0mqQv2n7M9g22l7W0LgAdaRKBcUmnSbq+qk6VNC3p6v2vZHvK9jrb63Zqe4NxAEahSQQ2S9pcVY8MTt+hPVHYR1WtrarJqpqc0JIG4wCMwtARqKofSnrZ9prBWedIeqaVVQHoTNNnBz4t6dbBMwMbJX2q+ZIAdKlRBKrqcUmTLa0FQA84YhAIRwSAcEQACEcEgHBEAAhHBIBwRAAIRwSAcEQACEcEgHBEAAjX9AVEwLzNbNzU6/xjd+/udf6bHziut9kHzdTcl3W4DgALEBEAwhEBIBwRAMIRASAcEQDCEQEgHBEAwhEBIBwRAMIRASAcEQDCEQEgHBEAwhEBIBwRAMIRASAcEQDCEQEgHBEAwhEBIBwRAMIRASAcEQDCEQEgHBEAwhEBIBwRAMIRASAcEQDC8dHkiDGz6aVe5x+2bbq32WPbts95GTsBIBwRAMIRASAcEQDCEQEgHBEAwhEBIBwRAMIRASAcEQDCEQEgHBEAwjWKgO0/sf207ads32b7kLYWBqAbQ0fA9kpJfyxpsqpOkTQm6eK2FgagG03vDoxLOtT2uKSlkl5pviQAXRo6AlX1A0l/L+klSa9K+p+qurethQHoRpO7A0dKukjSiZJ+TtIy25fOcr0p2+tsr9upud/YAEA/mtwd+IikF6rqtaraKekuSWfsf6WqWltVk1U1OaElDcYBGIUmEXhJ0odsL7VtSedI2tDOsgB0pcljAo9IukPSeklPDm5rbUvrAtCRRm80WlXXSrq2pbUA6AFHDALhiAAQjggA4YgAEI4IAOGIABCOCADhiAAQjggA4YgAEK7RYcMA5m/X1td7m121a87L2AkA4YgAEI4IAOGIABCOCADhiAAQjggA4YgAEI4IAOGIABCOCADhiAAQjggA4YgAEI4IAOGIABCOCADhiAAQjggA4YgAEI4IAOGIABCOCADhiAAQjggA4YgAEI4IAOGIABCOCADhiAAQjggA4YgAEI4IAOGIABCOCADhiAAQjggA4YgAEI4IAOGIABCOCADhiAAQjggA4YgAEO6AEbB9k+0ttp/a67wVtu+z/f3B70eOdpkARmU+O4GbJZ2/33lXS7q/qk6WdP/gNIBF6IARqKoHJb2x39kXSbpl8PUtkj7e8roAdGTYxwSOrapXJWnw+3vbWxKALo2PeoDtKUlTknSIlo56HIB3aNidwI9sHy9Jg9+3zHXFqlpbVZNVNTmhJUOOAzAqw0bgHkmXDb6+TNJX21kOgK7N5ynC2yQ9JGmN7c22L5f0t5LOtf19SecOTgNYhA74mEBVXTLHRee0vBYAPeCIQSAcEQDCEQEgHBEAwhEBIBwRAMIRASAcEQDCEQEgHBEAwo38pcRYOHZ9+LRe549N7+x1vr79ZL/zFyh2AkA4IgCEIwJAOCIAhCMCQDgiAIQjAkA4IgCEIwJAOCIAhCMCQDgiAIQjAkA4IgCEIwJAOCIAhCMCQDgiAIQjAkA4IgCEIwJAOCIAhCMCQDgiAIQjAkA4IgCEIwJAOCIAhCMCQDgiAIQjAkC48S6HeWJC48et7HLkPmr5st5mS9Jzl6/odf7zl/xTr/NPvGeq1/nLzzyj1/kHbe9v9syXH57zMnYCQDgiAIQjAkA4IgCEIwJAOCIAhCMCQDgiAIQjAkA4IgCEIwJAOCIAhDtgBGzfZHuL7af2Ou/vbD9r+7u277Z9xGiXCWBU5rMTuFnS+fudd5+kU6rqVyQ9J+maltcFoCMHjEBVPSjpjf3Ou7eqZgYnH5a0agRrA9CBNh4T+D1J/97C7QDoQaM3FbH9WUkzkm79f64zJWlKkg4ZO7zJOAAjMHQEbF8m6QJJ51RVzXW9qloraa0kvefgY+e8HoB+DBUB2+dL+nNJv1FVb7W7JABdms9ThLdJekjSGtubbV8u6QuSDpd0n+3Hbff75nUAhnbAnUBVXTLL2TeOYC0AesARg0A4IgCEIwJAOCIAhCMCQDgiAIQjAkA4IgCEIwJAOCIAhGv0UuJ36u2VE9rwN8d2OXIffv3g3mZL0lFrXu91/i994Q96nf++r/b78+96+nu9zu/TCzU952XsBIBwRAAIRwSAcEQACEcEgHBEAAhHBIBwRAAIRwSAcEQACEcEgHBEAAhHBIBwRAAIRwSAcEQACEcEgHBEAAhHBIBwRAAIRwSAcEQACEcEgHBEAAhHBIBwRAAIRwSAcEQACEcEgHBEAAhHBIBw410Om/iJtOruTkfu4+0j3NtsSfrd89b3Ov+2XZO9zn+xjup1/pKzTu91fp9mvvLwnJexEwDCEQEgHBEAwhEBIBwRAMIRASAcEQDCEQEgHBEAwhEBIBwRAMIRASDcASNg+ybbW2w/Nctlf2a7bB89muUBGLX57ARulnT+/mfaPkHSuZJeanlNADp0wAhU1YOS3pjlouskXSWp2l4UgO4M9ZiA7Qsl/aCqnpjHdadsr7O9bmb79DDjAIzQO36HD9tLJX1W0nnzuX5VrZW0VpIOO3IVuwZggRlmJ/ALkk6U9ITtTZJWSVpv+7g2FwagG+94J1BVT0p6789OD0IwWVVbW1wXgI7M5ynC2yQ9JGmN7c22Lx/9sgB05YA7gaq65ACXr25tNQA6xxGDQDgiAIQjAkA4IgCEIwJAOCIAhCMCQDgiAIQjAkA4IgCEe8cvIGpi7H9ndPh3t3Q5ch+n37mpt9mSdP395/Y6/+Qr5v6Mery7bay538uDnQAQjggA4YgAEI4IAOGIABCOCADhiAAQjggA4YgAEI4IAOGIABCOCADhiAAQjggA4YgAEI4IAOGIABCOCADhiAAQjggA4YgAEI4IAOGIABCOCADhiAAQjggA4YgAEI4IAOGIABCOCADhiAAQzlXV3TD7NUkvNriJoyVtbWk5zF9c85N/9jbm/3xVHTPbBZ1GoCnb66pqkvl585N/9lHP5+4AEI4IAOEWWwTWMj92fvLPPtL5i+oxAQDtW2w7AQAtIwJAOCIAhCMCQDgiAIT7P+ILkHVR78HWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x658.286 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x806.4 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(12):\n",
    "    first_layer_activation = activations[i]\n",
    "    plt.matshow(first_layer_activation[0, :, :, 4], cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mean_val_losses, mean_val_accuracies, mean_losses, mean_accuracies = mean_score_over_all_runs(mean_run_scores, n_runs)\n",
    "mean_val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.title('glareObs_noAdapt: Best validation accuracy %s%% (at epoch %s)' \\\n",
    "          %(round(mean_val_accuracies.max() * 100, 2), np.argmax(mean_val_accuracies) + 1))\n",
    "plt.plot(mean_accuracies)\n",
    "plt.plot(mean_val_accuracies)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train(RD_SD%sx)' %n_added_simulations_per_participant, 'validation(RD)'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.title('glareObs_noAdapt: Lowest validation loss %s (at epoch %s)' \\\n",
    "          %(round(mean_val_losses.min(), 4), np.argmin(mean_val_losses) + 1))\n",
    "plt.plot(mean_losses)\n",
    "plt.plot(mean_val_losses)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train(RD_SD%sx)' %n_added_simulations_per_participant, 'validation(RD)'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
