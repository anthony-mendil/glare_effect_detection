\chapter{Convolutional neural networks}
As convolutional layers are complex and have many variations for differnet use cases, only concepts and functionaslities that are relevant for this work will be covered. This also menas that 3 dimensional cnns will not be explained, but in generel cnns function the same way whether they have 1, 2 or 3 dimensions. The main differnce lies in how the filter, also known as kernel or festure detector, moves acrross the data. This is later explained in more detail. (\todo{ich glaube input data ist keine regel sondern nur meist so..zum beispiel kann man auch 1d convoltion bei 1d, 2d, oder 3d daten machen}). \\
 First of all the core concepts of cnns will be explained and afterwards the models used in this work will be described.\\

On of the core concepts of convoluntional neural networks are the concolutions. \\




Unituitievly, a 2d cnn must not neccerailty have 2 dimensional data as input.

notes bei models:\\
- stride = wie viele schritte auf einmal, default 1 deshalb vermute ich bei mir 1\\
- maxpooling: pool size = 2*2, und bei keras wird stride dann defauklt auch zu 2*2 (=pool size)\\
- input size = 85*40 glaube ich\\
- filter weights are randomly initialized, so that they dont all initially learn the same feauters. To briefly explain the behaviour of filters a seperation bettween two cases can be made: The first case is that not all features of high quality have been learned yet, high quality meaning that learning them would lower the cost function. In that case it is highly unlikely that each filter would begin to resemble other filters, as that would most certainly result in an increase of the cost function and therefore no gradient descent algorithm would head in that direction. The other case is that all features of high quality have already been leanred by a subset of the avaibable filters. In this case the cost function would not be increased if the remaining filters learn similar features than the other filters. \\
- in einem der schritte wird eine zeile usgelassen, aber ich trainiere nicht nochmal alles neu (oder?)\\

- 1d and 2d convolution
- reduction of dimension
- relu 
- number od filter, random initializion, etc
- kernel and kernel size
- loss function and optimizer, stochastic gradeint decent
- Dropout layers
- Pooling, max pooling 
- flatten
- dense relu, softmax
- droput layer, pooling ändert nichts an der anzahl der fetsure maps
- \todo{bessere quelle finden, buch oder so}
- stride (ganz kurz, beschriebt verhalte wenn etwas über bleibt, zero padding oder weglassen)
- back propagation
- wie schaffen die es bspw. 64 feature maps auf 32 zu reduzieren? also wie funktionieren die filter in dem fall? 

  


