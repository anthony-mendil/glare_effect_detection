@article{Rabiner89-ATO,
  author =    {Lawrence R. Rabiner},
  title =     {A Tutorial on Hidden {Markov} Models and Selected Applications in Speech Recognition},
  journal =   {Proceedings of the IEEE},
  volume =    {77}, 
  number =    {2},
  pages =     {257--286}, 
  year =      {1989}
}


@masterthesis{Markus,
	author = {M. Ihrig},
	title = {Analyse und Vergleich von Benutzerverhalten und Gehirnaktivität bei verschiedenen Versionen eines Memory-Spiels zum Test adaptiver Hilfestellungen},
	school = {Universität Bremen, Germany},
	year = {2019},
}

@masterthesis{Mir,
	author = {R. Miranda},
	title = {Recognising \& dynamically adapting to visual and memory-based hci obstacles based on behavioural data},
	school = {Universität Bremen, Germany},
	year = {2018},
}

@inproceedings{salous_putze_2018esann,
	author={Salous, Mazen and Putze, Felix},
	title={Behaviour-Based Working Memory Capacity Classification Using Recurrent Neural Networks},
	booktitle={{ESANN} 2018 -- 26th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
	isbn = {978-2-87587-047-6},
	pages = {159--164},
	numpages = {6},
	url={https://www.csl.uni-bremen.de/cms/images/documents/publications/salous_putze_2018esann.pdf},
	address = {Brugge, Belgium},
	year={2018},
	abstract={A user's working memory capacity is a crucial factor for successful Human Computer Interaction (HCI). While reliable tests for working memory capacity are available, they are time-consuming, stressful, and not well-integrated into HCI applications. This paper presents a classifier based on Long Short Term Memory networks to exploit sparse temporal dependencies in behavioural data, collected in a complex, memory-intense interaction task, to classify working memory capacity. A cognitive user simulation is introduced to generate additional training data episodes that follow the behaviour of existing real data. We show that the classifier outperforms a linear baseline especially for short segments of data.},
}

@article{actr,
	author = {Anderson, John and Bothell, Daniel and Byrne, Michael and Douglass, Scott and Lebiere, Christian and Qin, Yulin},
	year = {2004},
	month = {11},
	pages = {1036-60},
	title = {An Integrated Theory of the Mind.},
	volume = {111},
	journal = {Psychological review},
	doi = {10.1037/0033-295X.111.4.1036}
}

@inproceedings{memory,
	author = {Salous, Mazen and Putze, Felix},
	year = {2018},
	month = {04},
	pages = {},
	title = {Behaviour-Based Working Memory Capacity Classification Using Recurrent Neural Networks}
}

@inproceedings{blind,
	author = {Salous, Mazen and Putze, Felix and Ihrig, Markus and Schultz, Tanja},
	year = {2019},
	month = {10},
	pages = {},
	title = {Visual and Memory-based HCI Obstacles: Behaviour-based Detection and User Interface Adaptations Analysis},
	doi = {10.1109/SMC.2019.8914233}
}

@article{rgb_not,
	author = {Paschos, George},
	year = {2001},
	month = {07},
	pages = {932 - 937},
	title = {Perceptually uniform color spaces for color texture analysis: An empirical evaluation},
	volume = {10},
	journal = {Image Processing, IEEE Transactions on},
	doi = {10.1109/83.923289}
}

@masterthesis{fischer,
	author = {J. Fischer},
	title = {Untersuchung der Farbabstandsformeln des CIELAB Farbraums auf ihre Eignung, Farbrauschen quantitativ und physiologisch richtig zu beschreiben},
	school = {Fachhochschule Köln, Germany},
	year = {2002},
}

@INPROCEEDINGS{imbalance,  
	author={S. {Wang} and W. {Liu} and J. {Wu} and L. {Cao} and Q. {Meng} and P. J. {Kennedy}},  booktitle={2016 International Joint Conference on Neural Networks (IJCNN)},   
	title={Training deep neural networks on imbalanced data sets},   
	year={2016},  
	volume={},  
	number={},  
	pages={4368-4374},
}

@article{receptive,
	author    = {Wenjie Luo and
	Yujia Li and
	Raquel Urtasun and
	Richard S. Zemel},
	title     = {Understanding the Effective Receptive Field in Deep Convolutional
	Neural Networks},
	journal   = {CoRR},
	volume    = {abs/1701.04128},
	year      = {2017},
	url       = {http://arxiv.org/abs/1701.04128},
	archivePrefix = {arXiv},
	eprint    = {1701.04128},
	timestamp = {Mon, 13 Aug 2018 16:47:12 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/LuoLUZ17.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{cnn,
	added-at = {2018-08-01T08:16:18.000+0200},
	author = {Chollet, François},
	biburl = {https://www.bibsonomy.org/bibtex/231f94815ebbd65d3a31e4a69e818573e/jaeschke},
	interhash = {cfbfd3f93853a469e5e6978f61a74a0a},
	intrahash = {31f94815ebbd65d3a31e4a69e818573e},
	isbn = {9781617294433},
	keywords = {ai deeplearning ml},
	month = nov,
	publisher = {Manning},
	timestamp = {2018-08-01T08:16:18.000+0200},
	title = {Deep Learning with Python },
	year = 2017
}

@article{hinton,
	author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
	title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
	journal = {Journal of Machine Learning Research},
	year    = {2014},
	volume  = {15},
	number  = {56},
	pages   = {1929-1958},
	url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@inproceedings{evolution,
	author = {Young, Steven R. and Rose, Derek C. and Karnowski, Thomas P. and Lim, Seung-Hwan and Patton, Robert M.},
	title = {Optimizing Deep Learning Hyper-Parameters through an Evolutionary Algorithm},
	year = {2015},
	isbn = {9781450340069},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2834892.2834896},
	doi = {10.1145/2834892.2834896},
	abstract = {There has been a recent surge of success in utilizing Deep Learning (DL) in imaging and speech applications for its relatively automatic feature generation and, in particular for convolutional neural networks (CNNs), high accuracy classification abilities. While these models learn their parameters through data-driven methods, model selection (as architecture construction) through hyper-parameter choices remains a tedious and highly intuition driven task. To address this, Multi-node Evolutionary Neural Networks for Deep Learning (MENNDL) is proposed as a method for automating network selection on computational clusters through hyper-parameter optimization performed via genetic algorithms.},
	booktitle = {Proceedings of the Workshop on Machine Learning in High-Performance Computing Environments},
	articleno = {4},
	numpages = {5},
	keywords = {evolutionary algorithm, deep learning, convolutional neural networks, hyper-parameter optimization},
	location = {Austin, Texas},
	series = {MLHPC '15}
}

@INPROCEEDINGS{hyper,  
	author={L. {Deng} and G. {Hinton} and B. {Kingsbury}},  
	booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing},   
	title={New types of deep neural network learning for speech recognition and related applications: an overview},   
	year={2013},  
	volume={},  
	number={},  
	pages={8599-8603},
}

@article{convIm,
	title   = "Understanding the receptive field of deep convolutional networks",
	author  = "Adaloglou, Nikolas",
	journal = "https://theaisummer.com/",
	year    = "2020",
	url     = "https://theaisummer.com/receptive-field/"
}

@MISC{keras,
	author = {{Chollet}, Fran{\c{c}}ois and {others}},
	title = "{Keras: The Python Deep Learning library}",
	keywords = {Software},
	year = 2018,
	month = jun,
	eid = {ascl:1806.022},
	pages = {ascl:1806.022},
	archivePrefix = {ascl},
	eprint = {1806.022},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2018ascl.soft06022C},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@misc{1dcnn,
	author = {{Taguri}, Yosi and {others}},
	title = {Keras Conv1D: Working with 1D Convolutional Neural Networks in Keras},
	howpublished = {https://missinglink.ai/guides/keras/keras-conv1d-working-1d-convolutional-neural-networks-keras/},
	note = {Last accessed: 2020-09-23}
}

@misc{2dcnn,
	author = {M. Amunategui},
	title = {Convolutional Neural Networks And Unconventional Data - Predicting The Stock Market Using Images},
	howpublished = {http://amunategui.github.io/unconventional-convolutional-networks/},
	note = {Last accessed: 2020-09-24}
}

@article{oneOut,
	title = "Performance evaluation of classification algorithms by k-fold and leave-one-out cross validation",
	journal = "Pattern Recognition",
	volume = "48",
	number = "9",
	pages = "2839 - 2846",
	year = "2015",
	issn = "0031-3203",
	doi = "https://doi.org/10.1016/j.patcog.2015.03.009",
	url = "http://www.sciencedirect.com/science/article/pii/S0031320315000989",
	author = "Tzu-Tsung Wong",
	keywords = "Classification, Independence, -Fold cross validation, Leave-one-out cross validation, Sampling distribution",
	abstract = "Classification is an essential task for predicting the class values of new instances. Both k-fold and leave-one-out cross validation are very popular for evaluating the performance of classification algorithms. Many data mining literatures introduce the operations for these two kinds of cross validation and the statistical methods that can be used to analyze the resulting accuracies of algorithms, while those contents are generally not all consistent. Analysts can therefore be confused in performing a cross validation procedure. In this paper, the independence assumptions in cross validation are introduced, and the circumstances that satisfy the assumptions are also addressed. The independence assumptions are then used to derive the sampling distributions of the point estimators for k-fold and leave-one-out cross validation. The cross validation procedure to have such sampling distributions is discussed to provide new insights in evaluating the performance of classification algorithms."
}

@misc{repetition,
	author = {Brownlee, Jason},
	title = {Estimate the Number of Experiment Repeats for Stochastic Machine Learning Algorithms},
	howpublished = {https://machinelearningmastery.com/estimate-number-experiment-repeats-stochastic-machine-learning-algorithms/},
	note = {Last accessed: 2020-09-24}
}

@ARTICLE{ebs,  
	author={R. {Polikar}},  
	journal={IEEE Circuits and Systems Magazine},   
	title={Ensemble based systems in decision making},   
	year={2006},  
	volume={6},  
	number={3},  
	pages={21-45},
}

@article{smartphone,
	title   = "Anzahl der Smartphone-Nutzer in Deutschland in den Jahren 2009 bis 2019",
	author  = "F. Tenzer",
	year    = "2020",
	url     = "https://de.statista.com/statistik/daten/studie/198959/umfrage/anzahl-der-smartphonenutzer-in-deutschland-seit-2010/"
}

@misc{Cie,
	title = {CIE2000 Calculator},
	howpublished = {http://colormine.org/delta-e-calculator/cie2000},
	note = {Last accessed: 2020-09-26}
}
