\section{Ensemble-based systems}
\label{voting}
The accuracies shown in section \ref{training_results_and_evaluation} \nameref{training_results_and_evaluation} can not be used as measurement for the real life performance of the models. Since the accuracy is the mean of 20 repetitions for each split, there is no single model that produces that accuracy. A naive approach could be to simply select one of the models. However, selecting a single model for the classification comes with a problem that should not be ignored: Due to the fact that convnets are stochastic machine learning algorithms, multiple models, even if they have similar training performances, may have different generalization performances. In such cases, combining the predictions of several classifiers by letting them vote for the final classification may reduce the risk of an unfortunate selection of a poorly performing classifier. Systems that incorporate this method are called ensemble-based systems. They do not necessarily beat the performance of the best classifier in the ensemble, but they certainly reduce the overall risk of selecting a poorly performing classifier \cite[p.~22]{ebs}. 
%The accuracies shown in section \ref{training_results_and_evaluation} \nameref{training_results_and_evaluation} are not representative of how the models would perform in real life scenarios. Since the accuracy is the mean accuracy of 20 repetitions for each split there is no single model that produces that accuracy. And due to the fact that stochastic models can produce different outcomes each time they are trained, using only one of the models for the prediction does not produce reliable results \todo{quelle}.
%For analysing how the models would perform in real life scenarios a ensemble based system is deployed \todo{dei ersten s√§tze hier anpassen}. It consists of multiple models classifying the input data and then voting for the final classification. 

Therefore, an ensemble-based system for each of the best convnets and ratios of simulated to real data for each number of steps is created. As shown in table \ref{tab:best_acc} the 1D convnet creates the best accuracy for 5 and 10 rounds while the 2D convnet creates the best accuracies for 15 and 20 rounds. Hence, in total 4 ensemble-based systems are created, one for each of the 4 numbers of rounds. As before, the systems are tested using leave-one-out cross validation and each of the 20 repetitions of a split is only tested with the two real games (one no obstacle and one glare) that were not included in the training of that split. However, instead of directly calculating the accuracy of each repetition of that split, the output labels are saved for all repetitions and used to perform a voting for the final label. Meaning that if for instance 15 out of 20 repetitions classified the game as a glare effect game, while the other 5 classified it as a no obstacle game, the final classification will be that of a glare effect game. This is done for the two test games specific to each split. Using the final labels created by the voting, the accuracy of that split is calculated. In the example above, if assumed that both final labels would be correct, the accuracy of that split would be 100\%. This procedure is repeated for each of the 20 splits and the accuracies of the splits are averaged. 

As none of the models have been saved after the training, the 400 models for each of the 4 ensemble-based systems, are retrained and saved. In total 1600 models are trained and saved for the ensemble-based systems. However, they are trained for less epochs than in section \ref{training_results_and_evaluation} \nameref{training_results_and_evaluation}, as the models would otherwise overfit. For each of the 4 configurations the number of epochs is chosen so that the accuracy of the models in the last epoch, and therefore the accuracy of the saved model, is in the area of the maximum. The number of epochs was intentionally chosen only in a rough area of the maximum of the test accuracy and not exactly at the maximum. As mentioned in section \ref{models} \nameref{models}, there is no validation set and therefore using the exact number of epochs as optimal for the test data would mean that the models would be optimized according to the test data and the results would be less representative of the real world performance of the systems. Table \ref{tab:voting_systems} shows amongst other things after which epoch the training of the different models was stopped and what the average accuracies were after the according epoch, with and without voting. The slight difference of the average accuracies without voting shown in the table and the best accuracies shown in table \ref{tab:chosen_configs} are a result of two factors: Firstly, the number of training epochs was only chosen roughly in the area of the maximum accuracy and secondly, due to the nature of stochastic models, even though the extend is reduced by averaging over multiple repetitions, the accuracy in a specific epoch can still vary in every training \cite{repetition}. It should be emphasized that the average accuracies in table \ref{tab:voting_systems} are not that of the best epochs but those produced by the final model, meaning after the last training epoch.


\begin{table}[H]
	\centering
	
	\resizebox{\columnwidth}{!}{
		\begin{tabular}{|c||c|c|c|c|c|}
			\hline
			& training epochs & avg. acc. & acc. of EBS & Overruled correct classifications & Overruled false classifications  \\
			\hline
			\hline
			5 r. & 650 & 73.13\%  &72.5\%  &38 & 33 \\
			\hline
			10 r. &2400 & 80.63\% &82.5\% & 23 & 38  \\
			\hline
			15 r. & 90 & 80.63\% &80.0\% & 9 & 4 \\
			\hline
			20 r. & 60 & 85.0\% &85.0\%& 0 & 0  \\
			\hline
		\end{tabular}
	}
	\caption[Results of voting based systems compared to average of the models.]{Number of epochs the models were trained for, their average accuracy with and without voting, as well as the number of overruled correct and false classifications. EBS stands for ensemble-based system.}
	\label{tab:voting_systems}
\end{table}

When comparing the average accuracies without voting with the accuracies of the ensemble-based systems, it can bee seen that the accuracy of the ensemble-based systems is either better, worse or equal to the average accuracy without voting. Key factors that influence whether the ensemble-based system performs better than the average accuracies of the models are the numbers of correct and false classifications that are overruled by the majority in the voting. If more correct classifications are overruled than wrong ones, the accuracy of the ensemble-based system will be worse than the average accuracy without voting. The reason is that once a correct classification is overruled by the majority, it has no influence on the calculated accuracy any more, because the accuracy of the ensemble-based system is determined by the final labels. A good example of such a case occurred in the ensemble-based system for 5 rounds, where in one of the splits, 11 out of the 20 models voted incorrect, resulting in 9 correct predictions being ignored. Likewise, the accuracy with voting is higher than without if more false predictions are overruled than correct ones. Table \ref{tab:voting_systems} also shows how many correct and false predictions were overruled by their opposition which can directly be connected to whether the accuracy with or without voting is higher. The only voting based system that outperforms the average accuracy is that based on 1D convnets for 10 rounds, as there are more overruled false than correct predictions. Furthermore, the systems for 5 and 15 rounds perform worse than the average accuracy. Lastly, for 20 rounds, the results with and without voting are equally good.  

%As shown by the accuracies that are all higher than 50\%, the majority of the 400 models in each system make the correct prediction. However, their votes are not evenly distributed across all splits. A ensemble based systems highly profits from low fluctuation in the voting distribution between the models. If the voting in the majority of splits has similar results, such that for instance around 15 models vote correct and 5 vote wrong, the accuracy of those splits will all be 100\% and the average accuracy of the voting will be higher than the average over all models without voting. But if in for instance in the first split 20 models vote correct and 0 incorrect, while in the second split 9 vote correct and 11 incorrect, the overall performance of the ensemble based system suffers. Instead it would be better if in the example above, in the first split 15 vote correct and 5 incorrect and in the second split 14 vote correct and 6 incorrect, resulting in the accuracy of both split being 100\%. In total in both explained distributions, 29 models vote correct and 11 incorrect, but the performance of the ensemble based system that has less fluctuation in the distribution performs massively better. A way to achieve less fluctuation in the voting distribution can be to optimize the hyper parameters and the topology of the models.

It is important to mention that the created systems are not meant for production use, even if the fact that they are not optimized is overlooked. The training of models for production use, unlike before, should not be divided into 20 splits from which each split excludes real data for testing, but instead all of the real data should be included in the training. The validation of these systems would be done on new data collected from new participants. 

Contrary to using a single model for the classification, these ensemble-based systems reduce the overall risk of making a particularly poor selection of a model \cite[p.~22]{ebs}. Furthermore, the accuracies of these systems are more representative of their performance in real world scenarios than the accuracies in section \ref{training_results_and_evaluation} \nameref{training_results_and_evaluation}. In the context of detecting interaction obstacles, models that use less rounds are of higher relevance, as they make their prediction earlier. If only one of the 4 ensemble-based systems was used, the one for 10 rounds with an accuracy of 82.5\% should be the preferred choice. Making the prediction after 5 rounds with an accuracy of 72.5\% seems less ideal. Same goes for the two ensemble-based systems for 15 and 20 rounds. The system trained on 15 rounds is less accurate than the system for 10 rounds and waiting 20 rounds before making a prediction is too long. By then the user might have already left the game, due to a bad experience caused by the interaction obstacle. However, it is also possible to use all 4 systems. Each 5 rounds could be predicted whether an interaction obstacle is involved. This could be especially useful as the glare effect is a transient visual obstacle, unlike for instance colour blindness. Therefore, it  might only be an issue for a couple turns and not for the duration of the whole game. If predicted that the sun is no issue any more, the eventual adaptation could be removed again. Continuing the adaptation even though there is no need for it any more can be harmful for the user experience and acceptance \cite[p.~1]{blind}.

%To explain this an examplary voting of 20 models will be used, from which 11 classified the game as no obstacle game and 9 as glare effcet game. On the one hand, assuming that the majority of models classifies the game correctly, the result of the 9 wrong classification is ignored. The final predition of the voting will be correct, resulting in a accury of 100\% for that classification in that split. When calculating the average over all splits this will increase the average accuracy. On the other hand, the opposite is equally possible. If assumed that the majority of models in the example classiefies the game wrong, the result of the 9 correct classifications is ignored. The final prediction of the voting will be wrong, resulting in an accuracy of 0\% for that classification in that split. When calculating the average over all splits this will decrease the average accuracy. This example shows that a voting based system can either perform better or worse than the average accuracy of all models. A key factor that influences the outcome is the way the correct votes are distributed acroos the splits. A voting based systems highly profits from low fluctuation in the voting distribution between the models. If the voting in the majority of splits has similar results, such that for instance around 15 models vote correct and 5 vote wrong, the accuracy of those splits will all be 100\% and the average accuracy of the voting will be higher than the average over all models without voting. But if in for instance in the first split 20 models vote correct and 0 incorrect, while in the second split 9 vote correct and 11 incorrect, the overall performance of the voting based system suffers. Instead it would be better if in the example above, in the first split 15 vote correct and 5 incorrect and in the second split 14 vote correct and 6 incorrect, resulting in the accuracy of both split being 100\%. In total in both explained distributions, 29 models vote correct and 11 incorrect, but the performance of the voting based system that has less fluctuation in the distribution performs massively better. In conclusion is can be said, that if more correct classification are overvoted by wrong ones that wrong by correct ones, the accuracy of the voting based system is worse than the average accuracy without voting.

%\begin{table}[H]
%	\centering
%	\caption{add caption.}%\label{tab1}
%	%\resizebox{\columnwidth}{!}{%
%	\begin{tabular}{|l||c|c|c|c|}
%		\hline
%		& 5 r. & 10 r. & 15 r. & 20 r.  \\
%		\hline
%		\hline
%		1D CNN & 74.88\% (sd9x) & 81.12\% (sd0x) & &  \\
%		\hline
%		2D CNN &  & & 80.0\%  & 85.0\%  \\
%		\hline
%	\end{tabular}
%	%}
%\end{table}


%During training and creation topology and hyper paarameters are best practice confog and not optimized for each model. This is just a offline test, and before production use optimization should be done. Furthermnore for production the inetad of the 20 split and 400 models, only 20 models are trained on all data. it could be also tested to increase the number of repitions as this might increase the performance of the voting. 

%It would be possible to use these four systems during a game. Depending on the number of step a differet system is used. 



%All models used to calculate the best mean entries in the two tables for the results before the changes to the simulator are retrained and saved, as . This is only done if they are the best results regarding the number of steps. This menas that for the 1d cnns the models for 5 rounds and sd9x and for 10 rounds and sd0x are saved. For the 2d cnn the models for 15 rounds and sd9x and for 20 rounds for sd4x are saved. This results in 400 $\cdot$ 4 models. These 1600 models are used in the voting based system explained in chapter \todo{ref}. 