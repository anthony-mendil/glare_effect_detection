\chapter{Voting based system}
The accuracies shown in section \todo{ref} are not representitive of how the models would perform in real life scenarios. Since the accuracy is the mean accuracy of 20 repititions for each split there is no single modeel that produces that accuracy. And due to the fact that stochastic models can produce different outcomes each time they are trained, using only one of the models for the prediction does not produce relaible results \todo{quelle}. For analysing how the modles would perform in real life scenarios a voting based system is deployed \todo{quelle die sagt dass man sowas benutzt}. It consists of multple models classifying the input data and then voting for the final classification. A voting based sytsem for each of the best cnns and ratios of simulated to real data for each number of step is created. As mentioned in section \todo{ref} the 1d cnn creates the best accuracy for 5 and 10 rounds while the 2d cnn creates the best accuracies for 15 and 20 rounds. Hence, in total 4 voting based systems are created, one for each of the 4 number of rounds. \todo{besser ref zu einer tabl machen wo die drin stehen} Each of the 20 repititions of a split is tested as before, only with the two real games (one no obstacle and one glare) that were not included in the training. However instead of direclty calculating the accuracy of each repitition of that split, the output labels are saved for all repitions and used to perform a voting for the final label. Meaning that if for instance 15 out of 20 repitions classyfied the game as a glare effect game, while the other 5 classified it as a no obstacle game, the final classification will be that of a glare effect game. This is done for the two test games specific to each split. Using the final labels created by the voting, the accuracy of that split is calculated. In the example above, if assumed that both final labels would be correct, the accuracy of that split would be 100\%. This procedure is repeated for each of the 20 splits and the accuracies of the splits are averaged. 

As none of the models have been saved after the training, the 400 models for each of the 4 voting based systems, are retrained and saved. In total 1600 models are trained and saved for the voting based systems. However, they are trained for less epochs than in section \todo{ref}, as the models would otherwise overfit. For each of the 4 configurations the number of epochs in chosen so that the accuracy of the models in the last epoch, and therefore the accuracy of the saved model, is in the area of the local maxima. The number of epochs was intentionally chosen only in a rough area of the local maximum of the test accuracy and not exactly at the maximum. As mentioned in section \todo{ref} there is no validation set and therefore using the exact number of epochs as optimal for the test data would mean that the models would be optimized according to the test data and the results would be less representative of the real world performance of the systems. The \todo{ref} shows amongst other things the average accuracy after the last training epoch for each of the 4 configurations with and without voting. The slight difference of the average accuracies without voting shown in the table and the accuracies shown in table \todo{ref zu oben wo optimum ist} are caused by the fact that the models needed to be retrained in order to save them. As the accuracies in table \todo{ref zu oben wo optimum ist} were those from the best epoch and due to the nature of stochastic models their accuracy does not neccessarily reach that identical value after exactly the epoch after which the training for the models was stopped in. 

\begin{table}[H]
	\centering
	\caption{add caption.}%\label{tab1}
	\resizebox{\columnwidth}{!}{%
		\begin{tabular}{|c||c|c|c|c|}
			\hline
			& avg. acc. & acc. of VBS & Overruled correct classifications & Overruled false classifications  \\
			\hline
			\hline
			5 r. & 73.13\%  &72.5\%  &38 & 33 \\
			\hline
			10 r. & 80.13\% &80.0\% & 32 & 25  \\
			\hline
			15 r. & 80.63\% &80.0\% & 9 & 4 \\
			\hline
			20 r. & 85.0\% &85.0\%& 0 & 0  \\
			\hline
		\end{tabular}
	}
\end{table}

When comparing the average accuracies without voting with the accuracies of the voting based systems, it can bee seen that the accuracy of the voting based systems is eithher sligly lower or identical. In some cases such a system may have a better accuracy that without voting, but this is not always the case. Two key factors that this is influenced by are the numbers of correct and false classifications that are overruled by the majority in the voting. If more correct classification are overruled by wrong ones than wrong by correct ones, the accuracy of the voting based system will be worse than the average accuracy without voting. The reason is that once a correct classification is overruled by the majority, it has no influence on the calculated accuracy any more, because the accuracy of the voting based system is determined by the final labels. An good example of such a case occured in the voting based system for 5 rounds, where in one of the splits, 11 out of the 20 models voted incorrect, resulting in 9 correct predicitoins being ignored. Table \todo{ref} also shows how many correct and false predictions were overruled by their opposition. In every system except the one for 20 rounds, more correct predictions are overruled than false ones, resulting in a lower accuracy of those voting based systems compared to the accuracy calculated without voting. 

As shown by the accuracies that are all higher than 50\%, the majority of the 400 models in each system make the correct prediction. However, their votes are not evenly distributed across all splits. A voting based systems highly profits from low fluctuation in the voting distribution between the models. If the voting in the majority of splits has similar results, such that for instance around 15 models vote correct and 5 vote wrong, the accuracy of those splits will all be 100\% and the average accuracy of the voting will be higher than the average over all models without voting. But if in for instance in the first split 20 models vote correct and 0 incorrect, while in the second split 9 vote correct and 11 incorrect, the overall performance of the voting based system suffers. Instead it would be better if in the example above, in the first split 15 vote correct and 5 incorrect and in the second split 14 vote correct and 6 incorrect, resulting in the accuracy of both split being 100\%. In total in both explained distributions, 29 models vote correct and 11 incorrect, but the performance of the voting based system that has less fluctuation in the distribution performs massively better. A way to achieve less fluctuation in the voting distribution can be to optimize the hyper parameters and the topology of the models.

It is important to mention that these voting based systems are not meant for production use, even if the fact that they are not optimized is overlooked. The training of models for production use, unlike before, should not be divided into 20 splits from which each split excludes real data for testing, but instead all of the real data should be included in the training. The validation of these systems would be done with be done by collecting data from new participants. 

In the context of detecting interaction obstacles, models that use less rounds are of higher relevance, as they make their prediction earlier. If only one of the 4 voting based systems was used, the one for 10 rounds with an accuracy of 80\% should be the preferred choice. Making the prediction after 5 rounds with an accuracy of 72.5\% seem less ideal. Same goes for the two voting based systems for 15 and 20 rounds. The system trained on 15 rounds is not more accurate than the system for 10 rounds and waiting 20 rounds before making a prediction is too long. By then the user might have already left the game, due to a bad experience caused by the interaction obstacle. However, it is also possible to use all 4 systems. Each 5 rounds could be predicted whether an interaction obstacle is involved. This could be especially usefull as the glare effect might only be an issue for a couple turn until the player reaches shade or enters a building. If predicted that the sun is no issue anymore, the eventual adaptiation could be removed again. 

%To explain this an examplary voting of 20 models will be used, from which 11 classified the game as no obstacle game and 9 as glare effcet game. On the one hand, assuming that the majority of models classifies the game correctly, the result of the 9 wrong classification is ignored. The final predition of the voting will be correct, resulting in a accury of 100\% for that classification in that split. When calculating the average over all splits this will increase the average accuracy. On the other hand, the opposite is equally possible. If assumed that the majority of models in the example classiefies the game wrong, the result of the 9 correct classifications is ignored. The final prediction of the voting will be wrong, resulting in an accuracy of 0\% for that classification in that split. When calculating the average over all splits this will decrease the average accuracy. This example shows that a voting based system can either perform better or worse than the average accuracy of all models. A key factor that influences the outcome is the way the correct votes are distributed acroos the splits. A voting based systems highly profits from low fluctuation in the voting distribution between the models. If the voting in the majority of splits has similar results, such that for instance around 15 models vote correct and 5 vote wrong, the accuracy of those splits will all be 100\% and the average accuracy of the voting will be higher than the average over all models without voting. But if in for instance in the first split 20 models vote correct and 0 incorrect, while in the second split 9 vote correct and 11 incorrect, the overall performance of the voting based system suffers. Instead it would be better if in the example above, in the first split 15 vote correct and 5 incorrect and in the second split 14 vote correct and 6 incorrect, resulting in the accuracy of both split being 100\%. In total in both explained distributions, 29 models vote correct and 11 incorrect, but the performance of the voting based system that has less fluctuation in the distribution performs massively better. In conclusion is can be said, that if more correct classification are overvoted by wrong ones that wrong by correct ones, the accuracy of the voting based system is worse than the average accuracy without voting.

%\begin{table}[H]
%	\centering
%	\caption{add caption.}%\label{tab1}
%	%\resizebox{\columnwidth}{!}{%
%	\begin{tabular}{|l||c|c|c|c|}
%		\hline
%		& 5 r. & 10 r. & 15 r. & 20 r.  \\
%		\hline
%		\hline
%		1D CNN & 74.88\% (sd9x) & 81.12\% (sd0x) & &  \\
%		\hline
%		2D CNN &  & & 80.0\%  & 85.0\%  \\
%		\hline
%	\end{tabular}
%	%}
%\end{table}


%During training and creation topology and hyper paarameters are best practice confog and not optimized for each model. This is just a offline test, and before production use optimization should be done. Furthermnore for production the inetad of the 20 split and 400 models, only 20 models are trained on all data. it could be also tested to increase the number of repitions as this might increase the performance of the voting. 

%It would be possible to use these four systems during a game. Depending on the number of step a differet system is used. 



%All models used to calculate the best mean entries in the two tables for the results before the changes to the simulator are retrained and saved, as . This is only done if they are the best results regarding the number of steps. This menas that for the 1d cnns the models for 5 rounds and sd9x and for 10 rounds and sd0x are saved. For the 2d cnn the models for 15 rounds and sd9x and for 20 rounds for sd4x are saved. This results in 400 $\cdot$ 4 models. These 1600 models are used in the voting based system explained in chapter \todo{ref}. 